"""
Page Scraping - Google Maps
Extraction des artisans depuis Google Maps
"""
import streamlit as st
import sys
import json
import time
from pathlib import Path
from datetime import datetime
import pandas as pd
import requests
from io import BytesIO
try:
    from openpyxl import Workbook
    from openpyxl.styles import Font, Alignment, PatternFill
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False
# ‚úÖ Plus besoin de threading/ThreadPoolExecutor - GitHub Actions uniquement

# Configuration de la page
st.set_page_config(page_title="Scraping Google Maps", page_icon="üîç", layout="wide")

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from whatsapp_database.queries import ajouter_artisan, get_statistiques
from whatsapp_database.models import init_database

# ‚úÖ Initialiser la base de donn√©es au d√©marrage de la page (ajoute les nouvelles colonnes si n√©cessaire)
try:
    init_database()
except Exception as e:
    # Ne pas bloquer si erreur, mais logger
    import logging
    logging.warning(f"Erreur initialisation BDD: {e}")

# ‚úÖ Import des fonctions de tracking (avec fallback si elles n'existent pas)
try:
    from whatsapp_database.queries import is_already_scraped, get_scraping_history, mark_scraping_done
except ImportError:
    # Si les fonctions n'existent pas encore, cr√©er des stubs
    def is_already_scraped(metier: str, departement: str, ville: str) -> bool:
        return False
    
    def get_scraping_history(metier: str = None, departement: str = None):
        return []
    
    def mark_scraping_done(metier: str, departement: str, ville: str, results_count: int = 0):
        pass
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Charger les villes par d√©partement
try:
    with open(Path(__file__).parent.parent.parent / "data" / "villes_par_departement.json", 'r', encoding='utf-8') as f:
        villes_par_dept = json.load(f)
except:
    villes_par_dept = {}

# ‚úÖ Fonction pour r√©cup√©rer les communes depuis data.gouv.fr
def get_communes_from_api(departement: str, min_population: int = 0, max_population: int = 50000):
    """R√©cup√®re les communes d'un d√©partement depuis l'API data.gouv.fr avec coordonn√©es GPS"""
    try:
        url = f"https://geo.api.gouv.fr/departements/{departement}/communes"
        params = {
            "fields": "nom,code,codesPostaux,population,centre",
            "format": "json"
        }
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            communes = response.json()
            # Filtrer par population
            filtered = []
            for c in communes:
                pop = c.get('population', 0)
                if min_population <= pop <= max_population:
                    centre = c.get('centre', {})
                    filtered.append({
                        'nom': c['nom'],
                        'code': c['code'],
                        'code_postal': c.get('codesPostaux', [c.get('code', '')])[0] if c.get('codesPostaux') else c.get('code', ''),
                        'population': pop,
                        'latitude': centre.get('coordinates', [None, None])[1] if centre else None,
                        'longitude': centre.get('coordinates', [None, None])[0] if centre else None
                    })
            # Trier par population (croissant)
            filtered.sort(key=lambda x: x['population'])
            return filtered
    except Exception as e:
        logger.error(f"Erreur API communes: {e}")
    return []

st.title("üîç Scraping Google Maps - Artisans")

# Configuration du scraping
st.subheader("‚öôÔ∏è Configuration")

col_config1, col_config2 = st.columns(2)

with col_config1:
    # ‚úÖ Multi-select pour les m√©tiers
    metiers_options = ["plombier", "√©lectricien", "chauffagiste", "menuisier", "peintre", "ma√ßon", "couvreur", "carreleur"]
    metiers = st.multiselect(
        "Type(s) d'artisan(s)",
        options=metiers_options,
        default=["plombier"],
        help="S√©lectionnez un ou plusieurs types d'artisans √† rechercher"
    )
    if not metiers:
        st.warning("‚ö†Ô∏è Veuillez s√©lectionner au moins un m√©tier")
        metier = "plombier"
    else:
        metier = metiers[0]

with col_config2:
    # Liste des d√©partements fran√ßais
    departements_liste = [
        "01", "02", "03", "04", "05", "06", "07", "08", "09", "10",
        "11", "12", "13", "14", "15", "16", "17", "18", "19", "21",
        "22", "23", "24", "25", "26", "27", "28", "29", "2A", "2B",
        "30", "31", "32", "33", "34", "35", "36", "37", "38", "39",
        "40", "41", "42", "43", "44", "45", "46", "47", "48", "49",
        "50", "51", "52", "53", "54", "55", "56", "57", "58", "59",
        "60", "61", "62", "63", "64", "65", "66", "67", "68", "69",
        "70", "71", "72", "73", "74", "75", "76", "77", "78", "79",
        "80", "81", "82", "83", "84", "85", "86", "87", "88", "89",
        "90", "91", "92", "93", "94", "95"
    ]
    # ‚úÖ Multi-select pour les d√©partements
    departements = st.multiselect(
        "D√©partement(s)",
        options=departements_liste,
        default=["77"],
        help="S√©lectionnez un ou plusieurs d√©partements √† scraper"
    )
    if not departements:
        st.warning("‚ö†Ô∏è Veuillez s√©lectionner au moins un d√©partement")
        departement = "77"
    else:
        departement = departements[0]

col_config3, col_config4 = st.columns(2)

with col_config3:
    max_results = st.slider(
        "Nombre max de r√©sultats",
        min_value=10,
        max_value=200,
        value=50,
        step=10,
        help="Nombre maximum d'√©tablissements √† scraper par ville"
    )

with col_config4:
    headless = st.checkbox(
        "Mode headless (navigateur invisible)",
        value=True,
        help="Mode headless activ√© par d√©faut (plus rapide). D√©cochez pour voir le navigateur."
    )

# ‚úÖ Charger la configuration GitHub si elle existe
github_config_file = Path(__file__).parent.parent.parent / "config" / "github_config.json"
github_token_default = ""
github_repo_default = ""

try:
    if github_config_file.exists():
        with open(github_config_file, 'r', encoding='utf-8') as f:
            github_config = json.load(f)
            github_token_default = github_config.get('github_token', '')
            github_repo_default = github_config.get('github_repo', '')
except:
    pass

# ‚úÖ GitHub Actions est maintenant la SEULE option disponible
# Forcer GitHub Actions √† True
use_github_actions = True
st.session_state.use_github_actions = True

# ‚úÖ Utiliser les valeurs du fichier de config automatiquement (pas de champs visibles)
github_token = github_token_default
github_repo = github_repo_default

if not github_token or not github_repo:
    st.error("‚ö†Ô∏è Configuration GitHub manquante. V√©rifiez que config/github_config.json existe avec token et repo.")

# ‚úÖ Section : Gestion des workflows GitHub Actions (VISIBLE EN HAUT, D√àS LE D√âMARRAGE)
# ‚úÖ TOUJOURS AFFICH√âE - m√™me si pas de token (pour montrer qu'il faut configurer)
st.markdown("---")
st.subheader("‚öôÔ∏è Gestion des Workflows GitHub Actions")

# ‚úÖ FIX CRITIQUE : D√©finir les fonctions AVANT leur utilisation
def list_github_workflows(token, repo):
    """Liste tous les workflows GitHub Actions en cours"""
    try:
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        
        workflows = []
        
        # ‚úÖ FIX : Faire deux appels s√©par√©s car l'API ne supporte pas "in_progress,queued" dans un seul param√®tre
        # R√©cup√©rer les runs "in_progress"
        runs_url_in_progress = f"https://api.github.com/repos/{repo}/actions/runs?status=in_progress&per_page=100"
        response = requests.get(runs_url_in_progress, headers=headers)
        if response.status_code == 200:
            runs_data = response.json()
            for run in runs_data.get('workflow_runs', []):
                workflows.append({
                    'id': run.get('id'),
                    'run_number': run.get('run_number'),
                    'status': run.get('status'),
                    'conclusion': run.get('conclusion'),
                    'created_at': run.get('created_at'),
                    'updated_at': run.get('updated_at'),
                    'head_branch': run.get('head_branch'),
                    'workflow_id': run.get('workflow_id'),
                    'html_url': run.get('html_url')
                })
        
        # R√©cup√©rer les runs "queued"
        runs_url_queued = f"https://api.github.com/repos/{repo}/actions/runs?status=queued&per_page=100"
        response = requests.get(runs_url_queued, headers=headers)
        if response.status_code == 200:
            runs_data = response.json()
            for run in runs_data.get('workflow_runs', []):
                # √âviter les doublons
                if not any(w['id'] == run.get('id') for w in workflows):
                    workflows.append({
                        'id': run.get('id'),
                        'run_number': run.get('run_number'),
                        'status': run.get('status'),
                        'conclusion': run.get('conclusion'),
                        'created_at': run.get('created_at'),
                        'updated_at': run.get('updated_at'),
                        'head_branch': run.get('head_branch'),
                        'workflow_id': run.get('workflow_id'),
                        'html_url': run.get('html_url')
                    })
        
        # Trier par date de cr√©ation (plus r√©cent en premier)
        workflows.sort(key=lambda x: x.get('created_at', ''), reverse=True)
        
        return workflows
    except Exception as e:
        logger.error(f"Erreur liste workflows: {e}")
        return []

def cancel_github_workflow(token, repo, run_id):
    """Annule un workflow GitHub Actions sp√©cifique"""
    try:
        cancel_url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id}/cancel"
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        
        response = requests.post(cancel_url, headers=headers)
        return response.status_code == 202
    except Exception as e:
        logger.error(f"Erreur annulation workflow {run_id}: {e}")
        return False

def cancel_all_github_workflows(token, repo):
    """Annule tous les workflows GitHub Actions en cours (in_progress et queued)"""
    try:
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        
        cancelled_count = 0
        total_runs = 0
        
        # R√©cup√©rer les runs "in_progress"
        url_in_progress = f"https://api.github.com/repos/{repo}/actions/runs?status=in_progress&per_page=100"
        response = requests.get(url_in_progress, headers=headers)
        if response.status_code == 200:
            runs_data = response.json()
            for run in runs_data.get('workflow_runs', []):
                total_runs += 1
                if cancel_github_workflow(token, repo, run.get('id')):
                    cancelled_count += 1
        
        # R√©cup√©rer les runs "queued"
        url_queued = f"https://api.github.com/repos/{repo}/actions/runs?status=queued&per_page=100"
        response = requests.get(url_queued, headers=headers)
        if response.status_code == 200:
            runs_data = response.json()
            for run in runs_data.get('workflow_runs', []):
                total_runs += 1
                if cancel_github_workflow(token, repo, run.get('id')):
                    cancelled_count += 1
        
        if total_runs == 0:
            return True, "Aucun workflow en cours √† annuler"
        elif cancelled_count == total_runs:
            return True, f"‚úÖ {cancelled_count} workflow(s) annul√©(s) avec succ√®s"
        else:
            return False, f"‚ö†Ô∏è {cancelled_count}/{total_runs} workflow(s) annul√©(s)"
    except Exception as e:
        logger.error(f"Erreur annulation workflows: {e}")
        return False, f"Erreur: {str(e)}"

def download_github_artifact(token, repo, run_id):
    """T√©l√©charge l'artifact depuis GitHub Actions"""
    try:
        # R√©cup√©rer la liste des artifacts
        url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id}/artifacts"
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            artifacts = response.json().get('artifacts', [])
            for artifact in artifacts:
                if artifact.get('name') == 'scraping-results':
                    # T√©l√©charger l'artifact
                    download_url = artifact.get('archive_download_url')
                    if download_url:
                        download_response = requests.get(download_url, headers=headers)
                        if download_response.status_code == 200:
                            # Sauvegarder le zip
                            import zipfile
                            import io
                            data_dir = Path(__file__).parent.parent.parent / "data"
                            zip_path = data_dir / "github_artifact.zip"
                            with open(zip_path, 'wb') as f:
                                f.write(download_response.content)
                            
                            # Extraire le JSON
                            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                                zip_ref.extractall(data_dir)
                            
                            # Lire les fichiers
                            results_file = data_dir / "scraping_results_github_actions.json"
                            status_file = data_dir / "github_actions_status.json"
                            
                            result_data = None
                            status_data = None
                            
                            if results_file.exists():
                                with open(results_file, 'r', encoding='utf-8') as f:
                                    result_data = json.load(f)
                            
                            if status_file.exists():
                                with open(status_file, 'r', encoding='utf-8') as f:
                                    status_data = json.load(f)
                            
                            # Nettoyer
                            zip_path.unlink()
                            
                            # ‚úÖ Retourner dans le format attendu (compatibilit√©)
                            if result_data and isinstance(result_data, dict) and 'results' in result_data:
                                return result_data  # Format: {'results': [...], 'total_results': ...}
                            elif result_data and isinstance(result_data, list):
                                return {'results': result_data, 'total_results': len(result_data)}
                            else:
                                return {'results': [], 'total_results': 0, 'status': status_data}
            return None
        return None
    except Exception as e:
        logger.error(f"Erreur t√©l√©chargement artifact: {e}")
        return None

if github_token and github_repo:
    # ‚úÖ Bouton Rafra√Æchir TOUJOURS visible EN HAUT pour forcer la mise √† jour
    col_refresh_top1, col_refresh_top2 = st.columns([1, 1])
    with col_refresh_top1:
        if st.button("üîÑ Rafra√Æchir les workflows", key="refresh_workflows_top", help="Afficher tous les workflows et leurs statistiques"):
            # Forcer la mise √† jour en r√©initialisant le cache de session
            if 'workflows_last_refresh' in st.session_state:
                del st.session_state.workflows_last_refresh
            # Forcer le rerun imm√©diatement
            try:
                st.rerun()
            except AttributeError:
                try:
                    st.experimental_rerun()
                except:
                    pass

    # Lister les workflows en cours
    try:
        workflows_en_cours = list_github_workflows(github_token, github_repo)
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration workflows: {e}")
        workflows_en_cours = []
    
    # ‚úÖ Affichage simplifi√© - pas de d√©tails individuels qui apparaissent/disparaissent
    if workflows_en_cours:
        st.info(f"üü¢ **{len(workflows_en_cours)} workflow(s) en cours** - Utilisez le bouton 'Rafra√Æchir les workflows' ci-dessus pour voir les d√©tails")
    
    # ‚úÖ Bouton supprim√© - d√©j√† pr√©sent dans la page Base de donn√©es
    
    # ‚úÖ Afficher les statistiques pour chaque workflow (m√™me termin√©s)
    # R√©cup√©rer TOUS les workflows (pas seulement in_progress/queued)
    try:
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {github_token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        # ‚úÖ R√©cup√©rer les 4 derniers workflows (tous statuts)
        all_workflows_url = f"https://api.github.com/repos/{github_repo}/actions/runs?per_page=4"
        response = requests.get(all_workflows_url, headers=headers)
        all_workflows = []
        if response.status_code == 200:
            runs_data = response.json()
            for run in runs_data.get('workflow_runs', []):
                all_workflows.append({
                    'id': run.get('id'),
                    'run_number': run.get('run_number'),
                    'status': run.get('status'),
                    'conclusion': run.get('conclusion'),
                    'created_at': run.get('created_at'),
                    'html_url': run.get('html_url')
                })
            all_workflows.sort(key=lambda x: x.get('created_at', ''), reverse=True)
    except:
        all_workflows = workflows_en_cours
    
    if all_workflows:
        st.markdown("### üìä Statistiques par workflow")
        from whatsapp_database.queries import get_artisans
        
        # ‚úÖ Afficher en grille 2 colonnes (4 workflows max)
        workflows_to_show = all_workflows[:4]  # Limiter aux 4 derniers
        
        # Cr√©er des paires de workflows pour affichage en 2 colonnes
        for i in range(0, len(workflows_to_show), 2):
            cols = st.columns(2)
            for j, col in enumerate(cols):
                if i + j < len(workflows_to_show):
                    workflow = workflows_to_show[i + j]
                    status_emoji = "üü¢" if workflow['status'] == 'in_progress' else "üü°" if workflow['status'] == 'queued' else "üîµ"
                    conclusion_emoji = "‚úÖ" if workflow.get('conclusion') == 'success' else "‚ùå" if workflow.get('conclusion') == 'failure' else "‚èπÔ∏è" if workflow.get('conclusion') == 'cancelled' else ""
                    
                    with col:
                        with st.expander(f"{status_emoji} {conclusion_emoji} Workflow #{workflow['run_number']} - {workflow['status']} ({workflow['created_at'][:19].replace('T', ' ')})"):
                            # R√©cup√©rer les artisans scrap√©s depuis le d√©but de ce workflow
                            workflow_start = workflow['created_at']
                            
                            try:
                                # R√©cup√©rer tous les artisans
                                all_artisans = get_artisans(limit=10000)
                                
                                # ‚úÖ Normaliser le format de workflow_start (GitHub API format: "2025-11-28T16:56:17Z")
                                # Convertir en format comparable (sans Z, avec espace au lieu de T)
                                workflow_start_normalized = workflow_start.replace('T', ' ').replace('Z', '').split('.')[0]
                                
                                # Filtrer ceux cr√©√©s apr√®s le d√©but du workflow
                                workflow_artisans = []
                                for a in all_artisans:
                                    created_at = a.get('created_at')
                                    if created_at:
                                        # Normaliser created_at (peut √™tre ISO ou SQLite format)
                                        created_at_normalized = str(created_at).replace('T', ' ').replace('Z', '').split('.')[0]
                                        # Comparaison de cha√Ænes ISO normalis√©es (format: "YYYY-MM-DD HH:MM:SS")
                                        if created_at_normalized >= workflow_start_normalized:
                                            workflow_artisans.append(a)
                                
                                # ‚úÖ Debug: Afficher le nombre total d'artisans et ceux filtr√©s
                                if len(all_artisans) > 0 and len(workflow_artisans) == 0:
                                    # Si on a des artisans mais aucun ne correspond, c'est peut-√™tre un probl√®me de format
                                    # Afficher les 3 derniers artisans pour debug
                                    st.caption(f"üîç Debug: {len(all_artisans)} artisans totaux, workflow_start: {workflow_start_normalized}")
                                
                                if workflow_artisans:
                                    total = len(workflow_artisans)
                                    avec_tel = len([a for a in workflow_artisans if a.get('telephone')])
                                    avec_site = len([a for a in workflow_artisans if a.get('site_web')])
                                    sans_site = total - avec_site
                                    
                                    # ‚úÖ Stats simples sans colonnes imbriqu√©es
                                    st.markdown(f"**üìä Scrap√©s:** {total} | **üìû Avec t√©l√©phone:** {avec_tel} | **üåê Avec site web:** {avec_site} | **‚≠ê SANS site:** {sans_site}")
                                else:
                                    st.info("‚è≥ Aucun r√©sultat encore pour ce workflow")
                            except Exception as e:
                                st.error(f"Erreur calcul stats: {e}")
else:
    st.warning("‚ö†Ô∏è Configuration GitHub manquante. La gestion des workflows n√©cessite un token et un repository configur√©s.")

st.markdown("---")

# ‚úÖ Initialiser les variables GitHub Actions dans session_state AVANT de les utiliser
if 'github_workflow_id' not in st.session_state:
    st.session_state.github_workflow_id = None
if 'github_workflow_status' not in st.session_state:
    st.session_state.github_workflow_status = None
if 'github_workflow_conclusion' not in st.session_state:
    st.session_state.github_workflow_conclusion = None
# Initialiser aussi scraping_running si n√©cessaire (pour la v√©rification ci-dessous)
if 'scraping_running' not in st.session_state:
    st.session_state.scraping_running = False

# ‚úÖ CRITIQUE : V√©rifier si on a un workflow GitHub Actions actif au d√©marrage
# Si oui, maintenir scraping_running = True pour garder le dashboard visible
# Note: get_github_workflow_status est d√©fini plus tard, donc on ne peut pas l'appeler ici
# Cette v√©rification sera faite dans la section du dashboard GitHub Actions

# ‚úÖ Options avanc√©es et communes
with st.expander("‚öôÔ∏è Options avanc√©es"):
    col_adv1, col_adv2 = st.columns(2)
    with col_adv1:
        # ‚úÖ API activ√©e par d√©faut avec filtre <100k habitants
        use_api_communes = st.checkbox(
            "Utiliser API data.gouv.fr pour les communes",
            value=True,  # ‚úÖ Activ√© par d√©faut
            help="‚úÖ RECOMMAND√â : R√©cup√®re automatiquement toutes les communes depuis l'API officielle avec coordonn√©es GPS. Si d√©sactiv√©, utilise le fichier data/villes_par_departement.json (liste limit√©e)"
        )
        if use_api_communes:
            # ‚úÖ Valeurs par d√©faut : 0 √† 100k habitants
            min_pop = st.number_input("Population minimum", min_value=0, value=0, step=100, help="Villes avec au moins cette population")
            max_pop = st.number_input("Population maximum", min_value=0, value=100000, step=1000, help="Villes avec au maximum cette population (d√©faut: 100k)")
            
            # ‚úÖ Bouton pour afficher les communes trouv√©es (uniquement pour l'affichage, pas n√©cessaire pour le scraping)
            if st.button("üìã Afficher les communes trouv√©es", help="Affiche la liste des communes qui seront scrap√©es. Le scraping fonctionne m√™me sans cliquer sur ce bouton."):
                st.session_state.show_communes = True
    with col_adv2:
        enable_resume = st.checkbox(
            "Activer resume/checkpoint",
            value=True,
            disabled=use_github_actions,  # D√©sactiver pour GitHub Actions
            help="Permet de reprendre le scraping o√π il s'est arr√™t√© (non disponible avec GitHub Actions)"
        )
        num_threads = st.slider(
            "Nombre de threads",
            min_value=1,
            max_value=20,
            value=10,  # ‚úÖ D√©faut √† 10
            help="Nombre de navigateurs en parall√®le (attention: plus de threads = plus rapide mais plus de ressources)"
        )

# ‚úÖ Afficher les communes si demand√©
if st.session_state.get('show_communes', False) and use_api_communes and departements:
    # Section communes (dans un expander pour simplifier)
    with st.expander("üìç Voir les communes trouv√©es via API data.gouv.fr", expanded=False):
        communes_trouvees = {}
        with st.spinner("üîÑ R√©cup√©ration des communes depuis l'API..."):
            for dept in departements:
                communes = get_communes_from_api(dept, min_pop if use_api_communes else 0, max_pop if use_api_communes else 50000)
                communes_trouvees[dept] = communes
        
        # Afficher un tableau avec toutes les communes
        all_communes = []
        for dept, communes in communes_trouvees.items():
            for commune in communes:
                all_communes.append({
                    'D√©partement': dept,
                    'Commune': commune['nom'],
                    'Code postal': commune['code_postal'],
                    'Population': commune['population'] if commune['population'] > 0 else None
                })
        
        if all_communes:
            st.info(f"üìä Total: {len(all_communes)} communes trouv√©es")
            
            # ‚úÖ Mise en page c√¥te √† c√¥te : tableau et carte
            col_table, col_map = st.columns([1, 1])
            
            with col_table:
                st.markdown("#### üìã Liste des communes")
                df_communes = pd.DataFrame(all_communes)
                # ‚úÖ Convertir la colonne 'Population' en num√©rique pour le tri
                if 'Population' in df_communes.columns:
                    df_communes['Population'] = pd.to_numeric(df_communes['Population'], errors='coerce').fillna(0).astype(int)
                    # Cr√©er une colonne format√©e pour l'affichage avec espaces (ex: 56 659)
                    # On garde la colonne num√©rique pour le tri, et on cr√©e une colonne format√©e pour l'affichage
                    df_communes['Population (format√©e)'] = df_communes['Population'].apply(
                        lambda x: f"{x:,}".replace(',', ' ') if x > 0 else "N/A"
                    )
                    # Pour le tri num√©rique, on garde la colonne Population comme nombre
                    # Pour l'affichage, on utilise la version format√©e
                    df_communes_display = df_communes[['D√©partement', 'Commune', 'Code postal', 'Population']].copy()
                    # On remplace la colonne Population par la version format√©e pour l'affichage
                    df_communes_display['Population'] = df_communes['Population (format√©e)']
                else:
                    df_communes_display = df_communes
                
                # CSS pour autofit toutes les colonnes et √©viter la colonne vide
                st.markdown("""
                <style>
                /* Cibler le conteneur du DataFrame */
                div[data-testid="stDataFrame"] {
                    width: 100% !important;
                }
                div[data-testid="stDataFrame"] > div {
                    width: 100% !important;
                    overflow-x: visible !important;
                }
                /* Tableau avec ajustement automatique - largeur 100% mais colonnes auto */
                div[data-testid="stDataFrame"] table {
                    width: 100% !important;
                    table-layout: auto !important;
                    border-collapse: collapse !important;
                }
                /* Colonnes avec ajustement automatique selon le contenu */
                div[data-testid="stDataFrame"] th,
                div[data-testid="stDataFrame"] td {
                    white-space: nowrap !important;
                    padding: 8px 12px !important;
                    width: auto !important;
                    max-width: none !important;
                }
                /* Masquer compl√®tement la derni√®re colonne si elle est vide */
                div[data-testid="stDataFrame"] table thead tr th:last-child:empty,
                div[data-testid="stDataFrame"] table tbody tr td:last-child:empty {
                    display: none !important;
                    width: 0 !important;
                    padding: 0 !important;
                    border: none !important;
                }
                </style>
                """, unsafe_allow_html=True)
                
                # Utiliser dataframe avec formatage de la population (espaces pour s√©parer les milliers)
                st.dataframe(df_communes_display, height=400)
                
                # JavaScript pour masquer la colonne vide apr√®s le rendu
                st.markdown("""
                <script>
                function hideEmptyColumn() {
                    const tables = document.querySelectorAll('div[data-testid="stDataFrame"] table');
                    tables.forEach(function(table) {
                        const rows = table.querySelectorAll('tr');
                        if (rows.length > 0) {
                            // V√©rifier si la derni√®re colonne est vide dans toutes les lignes
                            let allEmpty = true;
                            rows.forEach(function(row) {
                                const lastCell = row.querySelector('th:last-child, td:last-child');
                                if (lastCell && lastCell.textContent && lastCell.textContent.trim() !== '') {
                                    allEmpty = false;
                                }
                            });
                            
                            // Si toutes les derni√®res colonnes sont vides, les masquer
                            if (allEmpty) {
                                rows.forEach(function(row) {
                                    const lastCell = row.querySelector('th:last-child, td:last-child');
                                    if (lastCell) {
                                        lastCell.style.display = 'none';
                                        lastCell.style.width = '0';
                                        lastCell.style.padding = '0';
                                        lastCell.style.border = 'none';
                                    }
                                });
                            }
                        }
                    });
                }
                // Ex√©cuter apr√®s le chargement
                setTimeout(hideEmptyColumn, 100);
                setTimeout(hideEmptyColumn, 500);
                setTimeout(hideEmptyColumn, 1000);
                </script>
                """, unsafe_allow_html=True)
            
            with col_map:
                st.markdown("#### üó∫Ô∏è Carte interactive")
                
                # ‚úÖ Carte interactive avec folium
                try:
                    import folium
                    from streamlit_folium import folium_static
                    
                    # Filtrer les communes avec coordonn√©es GPS directement depuis communes_trouvees
                    communes_avec_gps = []
                    for dept, communes_list in communes_trouvees.items():
                        for comm in communes_list:
                            if comm.get('latitude') and comm.get('longitude'):
                                communes_avec_gps.append({
                                    'nom': comm['nom'],
                                    'departement': dept,
                                    'code_postal': comm.get('code_postal', ''),
                                    'population': comm.get('population', 0),
                                    'latitude': comm['latitude'],
                                    'longitude': comm['longitude']
                                })
                    
                    if communes_avec_gps:
                        # Calculer le centre de la carte (moyenne des coordonn√©es)
                        avg_lat = sum(c['latitude'] for c in communes_avec_gps) / len(communes_avec_gps)
                        avg_lon = sum(c['longitude'] for c in communes_avec_gps) / len(communes_avec_gps)
                        
                        # Cr√©er une carte centr√©e sur les communes avec un meilleur style
                        m = folium.Map(
                            location=[avg_lat, avg_lon], 
                            zoom_start=8,
                            tiles='OpenStreetMap'  # Style plus propre
                        )
                        
                        # ‚úÖ AFFICHER TOUTES LES COMMUNES (pas de limite de 200)
                        populations = [c['population'] for c in communes_avec_gps if c['population'] > 0]
                        
                        if populations:
                            min_pop_displayed = min(populations)
                            max_pop_displayed = max(populations)
                            pop_range_displayed = max_pop_displayed - min_pop_displayed if max_pop_displayed > min_pop_displayed else 1
                        else:
                            min_pop_displayed = 0
                            max_pop_displayed = 1
                            pop_range_displayed = 1
                        
                        # ‚úÖ Am√©liorer le design : utiliser des clusters pour les grandes quantit√©s
                        from folium.plugins import MarkerCluster
                        marker_cluster = MarkerCluster().add_to(m)
                        
                        for commune in communes_avec_gps:
                            pop = commune['population']
                            pop_str = f"{pop:,}" if pop > 0 else "N/A"
                            popup_text = f"""
                            <div style='font-family: Arial, sans-serif;'>
                                <h4 style='margin: 0 0 10px 0; color: #2c3e50;'>{commune['nom']}</h4>
                                <p style='margin: 5px 0;'><strong>D√©partement:</strong> {commune['departement']}</p>
                                <p style='margin: 5px 0;'><strong>Code postal:</strong> {commune['code_postal']}</p>
                                <p style='margin: 5px 0;'><strong>Population:</strong> {pop_str}</p>
                            </div>
                            """
                            
                            # ‚úÖ Taille du marqueur proportionnelle √† la population (plus petite pour un meilleur design)
                            if pop > 0 and pop_range_displayed > 0:
                                # Normaliser entre 3 et 10 pixels de radius
                                normalized = (pop - min_pop_displayed) / pop_range_displayed
                                radius = 3 + (normalized * 7)  # Entre 3 et 10 pixels
                            else:
                                radius = 3
                            
                            # Couleur selon la population (seuils fixes pour la couleur)
                            if pop > 10000:
                                icon_color = '#e74c3c'  # Rouge
                            elif pop > 5000:
                                icon_color = '#f39c12'  # Orange
                            elif pop > 2000:
                                icon_color = '#3498db'  # Bleu
                            else:
                                icon_color = '#27ae60'  # Vert
                            
                            folium.CircleMarker(
                                location=[commune['latitude'], commune['longitude']],
                                radius=radius,
                                popup=folium.Popup(popup_text, max_width=250),
                                tooltip=f"{commune['nom']} ({pop:,} hab.)" if pop > 0 else commune['nom'],
                                color='white',
                                weight=1.5,
                                fillColor=icon_color,
                                fillOpacity=0.7,
                            ).add_to(marker_cluster)
                        
                        # Afficher la carte
                        folium_static(m, width=700, height=400)
                        
                        st.success(f"üó∫Ô∏è {len(communes_avec_gps)} communes affich√©es avec coordonn√©es GPS")
                    else:
                        st.warning("‚ö†Ô∏è Aucune commune avec coordonn√©es GPS trouv√©e")
                except ImportError:
                    st.info("üí° Pour afficher une carte interactive, installez: `pip install folium streamlit-folium`")
                except Exception as e:
                    st.error(f"Erreur lors de la cr√©ation de la carte: {e}")
        else:
            st.warning("Aucune commune trouv√©e avec les crit√®res s√©lectionn√©s")
        
        if st.button("‚ùå Fermer l'affichage des communes"):
            st.session_state.show_communes = False
            try:
                st.rerun()
            except:
                try:
                    st.experimental_rerun()
                except:
                    pass

st.markdown("---")

# √âtat du scraping
if 'scraper' not in st.session_state:
    st.session_state.scraper = None
if 'scraping_running' not in st.session_state:
    st.session_state.scraping_running = False
if 'scraped_results' not in st.session_state:
    st.session_state.scraped_results = []
if 'scraping_thread' not in st.session_state:
    st.session_state.scraping_thread = None
if 'scraping_started' not in st.session_state:
    st.session_state.scraping_started = False
if 'saved_count' not in st.session_state:
    st.session_state.saved_count = 0
if 'logs_buffer' not in st.session_state:
    st.session_state.logs_buffer = []
# Note: github_workflow_id et github_workflow_status sont initialis√©s plus t√¥t dans le code

# ‚úÖ CRITIQUE : V√©rifier si on a un workflow GitHub Actions actif au d√©marrage
# Si oui, maintenir scraping_running = True pour garder le dashboard visible
# Cette v√©rification se fera plus tard, apr√®s le chargement de la config GitHub

# ‚úÖ Fonctions pour GitHub Actions
def trigger_github_workflow(token, repo, metiers, departements, max_results, num_threads, use_api_communes, min_pop, max_pop):
    """D√©clenche le workflow GitHub Actions"""
    try:
        # ‚úÖ D'abord, r√©cup√©rer la liste des workflows pour trouver le bon ID
        workflows_url = f"https://api.github.com/repos/{repo}/actions/workflows"
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        
        # R√©cup√©rer les workflows
        workflows_response = requests.get(workflows_url, headers=headers)
        if workflows_response.status_code != 200:
            return False, f"Erreur r√©cup√©ration workflows: {workflows_response.status_code} - {workflows_response.text}"
        
        workflows_data = workflows_response.json()
        workflow_id = None
        
        # Chercher le workflow "Google Maps Scraping" ou "scraping.yml"
        for workflow in workflows_data.get('workflows', []):
            if workflow.get('name') == 'Google Maps Scraping' or workflow.get('path', '').endswith('scraping.yml'):
                workflow_id = workflow.get('id')
                break
        
        if not workflow_id:
            # Essayer avec le nom du fichier directement
            url = f"https://api.github.com/repos/{repo}/actions/workflows/scraping.yml/dispatches"
        else:
            # Utiliser l'ID du workflow (plus fiable)
            url = f"https://api.github.com/repos/{repo}/actions/workflows/{workflow_id}/dispatches"
        
        data = {
            "ref": "main",  # Essayer "main" d'abord
            "inputs": {
                "metiers": json.dumps(metiers),
                "departements": json.dumps(departements),
                "max_results": str(max_results),
                "num_threads": str(num_threads),
                "use_api_communes": str(use_api_communes).lower(),
                "min_pop": str(min_pop),
                "max_pop": str(max_pop)
            }
        }
        
        response = requests.post(url, headers=headers, json=data)
        
        # Si 404 avec "main", essayer "master"
        if response.status_code == 404 and data["ref"] == "main":
            data["ref"] = "master"
            response = requests.post(url, headers=headers, json=data)
        
        if response.status_code == 204:
            # ‚úÖ R√©cup√©rer le run_id du workflow qui vient d'√™tre lanc√©
            # Attendre un peu pour que GitHub cr√©e le run
            import time
            time.sleep(2)
            
            # R√©cup√©rer le dernier run du workflow
            runs_url = f"https://api.github.com/repos/{repo}/actions/workflows/scraping.yml/runs?per_page=1"
            runs_response = requests.get(runs_url, headers=headers)
            if runs_response.status_code == 200:
                runs_data = runs_response.json().get('workflow_runs', [])
                if runs_data:
                    run_id = runs_data[0].get('id')
                    return True, f"Workflow d√©clench√© avec succ√®s (Run ID: {run_id})", run_id
            
            return True, "Workflow d√©clench√© avec succ√®s"
        else:
            error_msg = response.text
            if response.status_code == 404:
                error_msg += f"\nüí° V√©rifiez que le workflow existe dans .github/workflows/scraping.yml et qu'il est commit√© sur GitHub"
            return False, f"Erreur: {response.status_code} - {error_msg}", None
    except Exception as e:
        return False, f"Erreur: {str(e)}", None

def get_github_workflow_status(token, repo, workflow_id=None):
    """R√©cup√®re le statut du workflow GitHub Actions"""
    try:
        if workflow_id:
            url = f"https://api.github.com/repos/{repo}/actions/runs/{workflow_id}"
        else:
            # R√©cup√©rer le dernier run (non annul√© si possible)
            url = f"https://api.github.com/repos/{repo}/actions/workflows/scraping.yml/runs?per_page=10"
        
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            if workflow_id:
                data = response.json()
            else:
                # R√©cup√©rer tous les runs et trouver le dernier non-annul√© (ou le dernier tout court)
                runs = response.json().get('workflow_runs', [])
                if not runs:
                    return None, None, None
                
                # Chercher le dernier run non-annul√©
                for run in runs:
                    if run.get('conclusion') != 'cancelled':
                        data = run
                        break
                else:
                    # Si tous sont annul√©s, prendre le dernier
                    data = runs[0]
            
            status = data.get('status')  # queued, in_progress, completed
            conclusion = data.get('conclusion')  # success, failure, cancelled, etc.
            run_id = data.get('id')
            return status, conclusion, run_id
        else:
            return None, None, None
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration statut: {e}")
        return None, None, None

def get_github_workflow_logs(token, repo, run_id):
    """R√©cup√®re les logs du workflow GitHub Actions"""
    try:
        # R√©cup√©rer les jobs du workflow
        url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id}/jobs"
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            jobs = response.json().get('jobs', [])
            logs = []
            for job in jobs:
                if job.get('status') == 'completed':
                    # R√©cup√©rer les logs du job
                    logs_url = job.get('logs_url')
                    if logs_url:
                        logs_response = requests.get(logs_url, headers=headers)
                        if logs_response.status_code == 200:
                            # Les logs sont dans un format sp√©cifique GitHub
                            logs.append({
                                'job_name': job.get('name'),
                                'status': job.get('conclusion'),
                                'logs_url': logs_url
                            })
            return logs
        return []
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration logs: {e}")
        return []

# ‚úÖ NOTE : Les fonctions list_github_workflows, cancel_github_workflow et cancel_all_github_workflows 
# sont maintenant d√©finies plus haut (ligne ~206) pour √©viter NameError

# ‚úÖ Cette section a √©t√© d√©plac√©e en haut pour √™tre visible d√®s le d√©marrage (voir ligne ~200)

# ‚úÖ Boutons de contr√¥le SIMPLIFI√âS (comme demand√©)
col_btn1, col_btn2 = st.columns(2)

with col_btn1:
    # ‚úÖ GitHub Actions uniquement - pas de mode local
    # ‚úÖ Le bouton LANCER est toujours activ√© - on peut lancer plusieurs workflows en parall√®le
    button_text = "‚òÅÔ∏è LANCER"
    
    if st.button(button_text, disabled=False):
        if not metiers or not departements:
            st.error("‚ö†Ô∏è Veuillez s√©lectionner au moins un m√©tier et un d√©partement")
        elif not github_token or not github_repo:
            st.error("‚ö†Ô∏è Veuillez renseigner le token GitHub et le repository")
        else:
            # ‚úÖ V√©rifier les villes d√©j√† scrap√©es
            villes_deja_scrapees = []
            villes_a_scraper = []
            
            # Pr√©parer la liste des villes √† scraper
            for dept in departements:
                if use_api_communes:
                    communes = get_communes_from_api(dept, min_pop if use_api_communes else 0, max_pop if use_api_communes else 50000)
                    villes_dept = [c['nom'] for c in communes]
                else:
                    villes_dept = villes_par_dept.get(dept, [])
                
                for metier in metiers:
                    for ville in villes_dept:
                        if is_already_scraped(metier, dept, ville):
                            villes_deja_scrapees.append(f"{metier} - {dept} - {ville}")
                        else:
                            villes_a_scraper.append(f"{metier} - {dept} - {ville}")
            
            # Afficher un avertissement si certaines villes sont d√©j√† scrap√©es
            if villes_deja_scrapees:
                st.warning(f"‚ö†Ô∏è {len(villes_deja_scrapees)} combinaison(s) m√©tier/d√©partement/ville d√©j√† scrap√©e(s). Elles seront ignor√©es si vous continuez.")
                with st.expander("üìã Voir les villes d√©j√† scrap√©es"):
                    for v in villes_deja_scrapees[:20]:  # Limiter √† 20 pour l'affichage
                        st.text(v)
                    if len(villes_deja_scrapees) > 20:
                        st.caption(f"... et {len(villes_deja_scrapees) - 20} autres")
            
            if not villes_a_scraper:
                st.error("‚ùå Toutes les combinaisons s√©lectionn√©es ont d√©j√† √©t√© scrap√©es. Veuillez s√©lectionner d'autres options.")
            else:
                st.info(f"‚úÖ {len(villes_a_scraper)} combinaison(s) √† scraper")
                
                # D√©clencher le workflow GitHub Actions
                result = trigger_github_workflow(
                    github_token,
                    github_repo,
                    metiers,
                    departements,
                    max_results,
                    num_threads,
                    use_api_communes,
                    min_pop if use_api_communes else 0,
                    max_pop if use_api_communes else 50000
                )
                
                # ‚úÖ G√©rer le retour (peut √™tre (success, message) ou (success, message, run_id))
                if len(result) == 3:
                    success, message, run_id = result
                else:
                    success, message = result
                    run_id = None
                
                if success:
                    st.success(f"‚úÖ {message}")
                    st.info("‚è≥ Le scraping est en cours sur GitHub Actions. Les r√©sultats sont sauvegard√©s directement dans la BDD.")
                    st.session_state.scraping_running = True
                    st.session_state.github_workflow_status = 'in_progress'
                    st.session_state.departements_selected = departements
                    st.session_state.metiers_selected = metiers
                    
                    # ‚úÖ Stocker le run_id si disponible
                    if run_id:
                        st.session_state.github_workflow_id = run_id
                    # ‚úÖ Marquer qu'on vient de lancer un workflow pour ne pas l'annuler
                    st.session_state.workflow_just_launched = True
                    try:
                        st.rerun()
                    except AttributeError:
                        try:
                            st.experimental_rerun()
                        except:
                            pass
                else:
                    st.error(f"‚ùå {message}")

with col_btn2:
    # ‚úÖ Bouton ARR√äTER (simplifi√© - pas de confirmation)
    if github_token and github_repo:
        if st.button("‚èπÔ∏è ARR√äTER", help="Arr√™ter tous les workflows GitHub Actions en cours", key="stop_all_workflows"):
            with st.spinner("‚èπÔ∏è Arr√™t des workflows..."):
                success, message = cancel_all_github_workflows(github_token, github_repo)
                if success:
                    st.success(f"‚úÖ {message}")
                    st.session_state.scraping_running = False
                    st.session_state.github_workflow_status = None
                    st.session_state.github_workflow_id = None
                    st.session_state.scraped_results = []
                    try:
                        st.rerun()
                    except AttributeError:
                        try:
                            st.experimental_rerun()
                        except:
                            pass
                else:
                    st.error(f"‚ùå {message}")

# ‚úÖ NOTE : Le dashboard GitHub Actions a √©t√© supprim√© car les workflows sont maintenant g√©r√©s en haut de la page
# avec le bouton "Rafra√Æchir les workflows" qui affiche les statistiques pour chaque workflow

# Zone de scraping (ancien dashboard supprim√© - tout est g√©r√© en haut maintenant)
# Le code du dashboard GitHub Actions a √©t√© supprim√© car il n'est plus n√©cessaire
# Tous les workflows sont maintenant g√©r√©s en haut avec le bouton "Rafra√Æchir les workflows"

# Ancien code du dashboard compl√®tement supprim√©

# ‚úÖ V√©rifier si on doit annuler les workflows au d√©marrage
# ‚úÖ NE PAS annuler si on vient juste de lancer un workflow
if github_token and github_repo and not st.session_state.get('workflow_just_launched', False):
    # ‚úÖ Tuer automatiquement tous les workflows en cours au d√©marrage (une seule fois)
    if not st.session_state.get('workflows_cancelled_on_start', False):
        with st.spinner("‚èπÔ∏è Annulation des workflows GitHub Actions en cours..."):
            success, message = cancel_all_github_workflows(github_token, github_repo)
            # ‚úÖ TOUJOURS d√©finir le flag pour √©viter les boucles, m√™me en cas d'√©chec
            st.session_state.workflows_cancelled_on_start = True
            if success:
                # Ne pas r√©initialiser scraping_running si on a un workflow_id
                if not st.session_state.get('github_workflow_id'):
                    st.session_state.scraping_running = False
                    st.session_state.github_workflow_status = None
                    st.session_state.github_workflow_id = None
                st.success(f"‚úÖ {message}")
                # ‚úÖ Rerun seulement si on a annul√© avec succ√®s
                try:
                    st.rerun()
                except AttributeError:
                    try:
                        st.experimental_rerun()
                    except:
                        pass
            else:
                st.warning(f"‚ö†Ô∏è {message}")
                # ‚úÖ NE PAS faire de rerun en cas d'√©chec pour √©viter les boucles
# ‚úÖ R√©initialiser le flag apr√®s le premier rerun
if st.session_state.get('workflow_just_launched', False):
    st.session_state.workflow_just_launched = False

# ‚úÖ GitHub Actions uniquement - plus de code local

# ‚úÖ CRITIQUE : Charger automatiquement les r√©sultats depuis le JSON ET la BDD au d√©marrage
# Si on n'a pas de r√©sultats en session_state, essayer de les charger depuis le JSON et la BDD
if not st.session_state.scraped_results:
    results_file = Path(__file__).parent.parent.parent / "data" / "scraping_results_github_actions.json"
    results_list = []
    
    # 1. Charger depuis le fichier JSON (si existe)
    if results_file.exists():
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                results_data = json.load(f)
                if isinstance(results_data, dict) and 'results' in results_data:
                    results_list = results_data['results']
                elif isinstance(results_data, list):
                    results_list = results_data
        except Exception as e:
            logger.error(f"Erreur chargement JSON: {e}")
    
    # 2. ‚úÖ AUSSI charger depuis la BDD (pour voir les r√©sultats sauvegard√©s directement)
    try:
        from whatsapp_database.queries import get_artisans
        artisans_bdd = get_artisans(limit=10000)  # R√©cup√©rer tous les artisans
        if artisans_bdd:
            # Convertir les artisans de la BDD en format compatible
            for artisan in artisans_bdd:
                # √âviter les doublons (par t√©l√©phone)
                if not any(r.get('telephone') == artisan.get('telephone') for r in results_list if r.get('telephone')):
                    # ‚úÖ Extraire d√©partement depuis code_postal si manquant
                    dept = artisan.get('departement')
                    code_postal = artisan.get('code_postal')
                    if not dept and code_postal:
                        code_postal_str = str(code_postal).strip()
                        if len(code_postal_str) >= 2:
                            if code_postal_str.startswith('97') or code_postal_str.startswith('98'):
                                dept = code_postal_str[:3]
                            else:
                                dept = code_postal_str[:2]
                    
                    # ‚úÖ Si toujours pas de d√©partement, essayer depuis ville_recherche via API
                    if not dept and artisan.get('ville_recherche'):
                        try:
                            ville_nom = artisan.get('ville_recherche', '').strip()
                            if ville_nom:
                                url = f"https://geo.api.gouv.fr/communes?nom={ville_nom}&fields=codeDepartement&limit=1"
                                response = requests.get(url, timeout=3)
                                if response.status_code == 200:
                                    communes = response.json()
                                    if communes and len(communes) > 0:
                                        dept = communes[0].get('codeDepartement', '')
                        except:
                            pass  # Si l'API √©choue, on continue sans d√©partement
                    
                    results_list.append({
                        'nom': artisan.get('nom_entreprise') or artisan.get('nom'),
                        'telephone': artisan.get('telephone'),
                        'site_web': artisan.get('site_web'),
                        'google_maps_url': artisan.get('google_maps_url'),
                        'adresse': artisan.get('adresse'),
                        'ville': artisan.get('ville'),
                        'code_postal': code_postal,
                        'departement': dept,
                        'note': artisan.get('note'),
                        'nb_avis': artisan.get('nombre_avis'),
                        'ville_recherche': artisan.get('ville_recherche'),
                        'recherche': artisan.get('type_artisan') or artisan.get('recherche')
                    })
    except Exception as e:
        logger.error(f"Erreur chargement BDD: {e}")
    
    if results_list:
        st.session_state.scraped_results = results_list

# ‚úÖ Section "R√©sultats scrap√©s" supprim√©e pour simplifier la page

# ‚úÖ Expander "Mode d'ex√©cution" tout en bas de la page
with st.expander("‚ÑπÔ∏è Mode d'ex√©cution", expanded=False):
    st.info("‚òÅÔ∏è **Mode GitHub Actions activ√©** - Le scraping s'ex√©cutera sur GitHub Actions (gratuit jusqu'√† 2000 min/mois)")
    st.info("‚ÑπÔ∏è Le scraping s'ex√©cutera sur GitHub Actions. Les r√©sultats sont sauvegard√©s directement dans la BDD en temps r√©el.")
