"""
Page Scraping - Google Maps
Extraction des artisans depuis Google Maps
"""
import streamlit as st
import sys
import json
import time
from pathlib import Path
from datetime import datetime
import pandas as pd
import requests
# ‚úÖ Plus besoin de threading/ThreadPoolExecutor - GitHub Actions uniquement

# Configuration de la page
st.set_page_config(page_title="Scraping Google Maps", page_icon="üîç", layout="wide")

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from whatsapp_database.queries import ajouter_artisan, get_statistiques

# ‚úÖ Import des fonctions de tracking (avec fallback si elles n'existent pas)
try:
    from whatsapp_database.queries import is_already_scraped, get_scraping_history, mark_scraping_done
except ImportError:
    # Si les fonctions n'existent pas encore, cr√©er des stubs
    def is_already_scraped(metier: str, departement: str, ville: str) -> bool:
        return False
    
    def get_scraping_history(metier: str = None, departement: str = None):
        return []
    
    def mark_scraping_done(metier: str, departement: str, ville: str, results_count: int = 0):
        pass
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Charger les villes par d√©partement
try:
    with open(Path(__file__).parent.parent.parent / "data" / "villes_par_departement.json", 'r', encoding='utf-8') as f:
        villes_par_dept = json.load(f)
except:
    villes_par_dept = {}

# ‚úÖ Fonction pour r√©cup√©rer les communes depuis data.gouv.fr
def get_communes_from_api(departement: str, min_population: int = 0, max_population: int = 50000):
    """R√©cup√®re les communes d'un d√©partement depuis l'API data.gouv.fr avec coordonn√©es GPS"""
    try:
        url = f"https://geo.api.gouv.fr/departements/{departement}/communes"
        params = {
            "fields": "nom,code,codesPostaux,population,centre",
            "format": "json"
        }
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            communes = response.json()
            # Filtrer par population
            filtered = []
            for c in communes:
                pop = c.get('population', 0)
                if min_population <= pop <= max_population:
                    centre = c.get('centre', {})
                    filtered.append({
                        'nom': c['nom'],
                        'code': c['code'],
                        'code_postal': c.get('codesPostaux', [c.get('code', '')])[0] if c.get('codesPostaux') else c.get('code', ''),
                        'population': pop,
                        'latitude': centre.get('coordinates', [None, None])[1] if centre else None,
                        'longitude': centre.get('coordinates', [None, None])[0] if centre else None
                    })
            # Trier par population (croissant)
            filtered.sort(key=lambda x: x['population'])
            return filtered
    except Exception as e:
        logger.error(f"Erreur API communes: {e}")
    return []

st.title("üîç Scraping Google Maps - Artisans")

# Stats actuelles
stats = get_statistiques()
col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
with col_stat1:
    st.metric("Total artisans", f"{stats.get('total', 0):,}")
with col_stat2:
    st.metric("Avec t√©l√©phone", f"{stats.get('avec_telephone', 0):,}")
with col_stat3:
    st.metric("Avec site web", f"{stats.get('avec_site_web', 0):,}")
with col_stat4:
    st.metric("Sans site web", f"{stats.get('sans_site_web', 0):,}")

st.markdown("---")

# Configuration du scraping
st.subheader("‚öôÔ∏è Configuration")

col_config1, col_config2 = st.columns(2)

with col_config1:
    # ‚úÖ Multi-select pour les m√©tiers
    metiers_options = ["plombier", "electricien", "chauffagiste", "menuisier", "peintre", "macon", "couvreur", "carreleur"]
    metiers = st.multiselect(
        "Type(s) d'artisan(s)",
        options=metiers_options,
        default=["plombier"],
        help="S√©lectionnez un ou plusieurs types d'artisans √† rechercher"
    )
    if not metiers:
        st.warning("‚ö†Ô∏è Veuillez s√©lectionner au moins un m√©tier")
        metier = "plombier"
    else:
        metier = metiers[0]

with col_config2:
    # Liste des d√©partements fran√ßais
    departements_liste = [
        "01", "02", "03", "04", "05", "06", "07", "08", "09", "10",
        "11", "12", "13", "14", "15", "16", "17", "18", "19", "21",
        "22", "23", "24", "25", "26", "27", "28", "29", "2A", "2B",
        "30", "31", "32", "33", "34", "35", "36", "37", "38", "39",
        "40", "41", "42", "43", "44", "45", "46", "47", "48", "49",
        "50", "51", "52", "53", "54", "55", "56", "57", "58", "59",
        "60", "61", "62", "63", "64", "65", "66", "67", "68", "69",
        "70", "71", "72", "73", "74", "75", "76", "77", "78", "79",
        "80", "81", "82", "83", "84", "85", "86", "87", "88", "89",
        "90", "91", "92", "93", "94", "95"
    ]
    # ‚úÖ Multi-select pour les d√©partements
    departements = st.multiselect(
        "D√©partement(s)",
        options=departements_liste,
        default=["77"],
        help="S√©lectionnez un ou plusieurs d√©partements √† scraper"
    )
    if not departements:
        st.warning("‚ö†Ô∏è Veuillez s√©lectionner au moins un d√©partement")
        departement = "77"
    else:
        departement = departements[0]

col_config3, col_config4 = st.columns(2)

with col_config3:
    max_results = st.slider(
        "Nombre max de r√©sultats",
        min_value=10,
        max_value=200,
        value=50,
        step=10,
        help="Nombre maximum d'√©tablissements √† scraper par ville"
    )

with col_config4:
    headless = st.checkbox(
        "Mode headless (navigateur invisible)",
        value=True,
        help="Mode headless activ√© par d√©faut (plus rapide). D√©cochez pour voir le navigateur."
    )

# ‚úÖ Charger la configuration GitHub si elle existe
github_config_file = Path(__file__).parent.parent.parent / "config" / "github_config.json"
github_token_default = ""
github_repo_default = ""

try:
    if github_config_file.exists():
        with open(github_config_file, 'r', encoding='utf-8') as f:
            github_config = json.load(f)
            github_token_default = github_config.get('github_token', '')
            github_repo_default = github_config.get('github_repo', '')
except:
    pass

# ‚úÖ Toggle Local vs GitHub Actions
st.markdown("### üöÄ Mode d'ex√©cution")
# ‚úÖ GitHub Actions est maintenant la SEULE option disponible
# Forcer GitHub Actions √† True
use_github_actions = True
st.session_state.use_github_actions = True

# Afficher un message informatif
st.info("‚òÅÔ∏è **Mode GitHub Actions activ√©** - Le scraping s'ex√©cutera sur GitHub Actions (gratuit jusqu'√† 2000 min/mois)")

if use_github_actions:
    st.info("‚ÑπÔ∏è Le scraping s'ex√©cutera sur GitHub Actions. Les r√©sultats sont sauvegard√©s directement dans la BDD en temps r√©el.")
    
    # ‚úÖ Utiliser les valeurs du fichier de config automatiquement (pas de champs visibles)
    github_token = github_token_default
    github_repo = github_repo_default
    
    if not github_token or not github_repo:
        st.error("‚ö†Ô∏è Configuration GitHub manquante. V√©rifiez que config/github_config.json existe avec token et repo.")
else:
    # Si GitHub Actions n'est pas activ√©, utiliser des valeurs vides
    github_token = ""
    github_repo = ""

# ‚úÖ Section : Gestion des workflows GitHub Actions (VISIBLE EN HAUT, D√àS LE D√âMARRAGE)
# ‚úÖ TOUJOURS AFFICH√âE - m√™me si pas de token (pour montrer qu'il faut configurer)
st.markdown("---")
st.subheader("‚öôÔ∏è Gestion des Workflows GitHub Actions")

if github_token and github_repo:
    # Lister les workflows en cours
    try:
        workflows_en_cours = list_github_workflows(github_token, github_repo)
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration workflows: {e}")
        workflows_en_cours = []
    
    if workflows_en_cours:
        # ‚úÖ AFFICHER LE NOMBRE DE WORKFLOWS EN COURS (comme demand√©)
        col_count1, col_count2 = st.columns([1, 4])
        with col_count1:
            st.metric("Workflows actifs", len(workflows_en_cours))
        with col_count2:
            if st.button("‚èπÔ∏è Arr√™ter tous", key="cancel_all_workflows_top", help="Arr√™ter tous les workflows en cours"):
                with st.spinner("‚èπÔ∏è Annulation de tous les workflows..."):
                    success, message = cancel_all_github_workflows(github_token, github_repo)
                    if success:
                        st.success(message)
                    else:
                        st.warning(message)
                    st.experimental_rerun()
        
        # Afficher chaque workflow avec possibilit√© de le tuer individuellement
        st.markdown("**D√©tails des workflows :**")
        for workflow in workflows_en_cours:
            col_wf1, col_wf2, col_wf3 = st.columns([3, 1, 1])
            with col_wf1:
                status_emoji = "üü¢" if workflow['status'] == 'in_progress' else "üü°"
                status_text = "En cours" if workflow['status'] == 'in_progress' else "En attente"
                created_time = workflow['created_at'][:19].replace('T', ' ')
                st.markdown(f"{status_emoji} **Run #{workflow['run_number']}** - {status_text} - {created_time}")
            with col_wf2:
                github_url = workflow.get('html_url', f"https://github.com/{github_repo}/actions/runs/{workflow['id']}")
                st.markdown(f"[üîó Voir]({github_url})")
            with col_wf3:
                if st.button(f"‚èπÔ∏è Arr√™ter", key=f"cancel_{workflow['id']}"):
                    with st.spinner(f"‚èπÔ∏è Annulation du workflow #{workflow['run_number']}..."):
                        if cancel_github_workflow(github_token, github_repo, workflow['id']):
                            st.success(f"‚úÖ Workflow #{workflow['run_number']} annul√©")
                            st.experimental_rerun()
                        else:
                            st.error(f"‚ùå Erreur lors de l'annulation du workflow #{workflow['run_number']}")
    else:
        st.success("‚úÖ Aucun workflow en cours")
else:
    st.warning("‚ö†Ô∏è Configuration GitHub manquante. La gestion des workflows n√©cessite un token et un repository configur√©s.")
    
st.markdown("---")

# ‚úÖ Initialiser les variables GitHub Actions dans session_state AVANT de les utiliser
if 'github_workflow_id' not in st.session_state:
    st.session_state.github_workflow_id = None
if 'github_workflow_status' not in st.session_state:
    st.session_state.github_workflow_status = None
if 'github_workflow_conclusion' not in st.session_state:
    st.session_state.github_workflow_conclusion = None
# Initialiser aussi scraping_running si n√©cessaire (pour la v√©rification ci-dessous)
if 'scraping_running' not in st.session_state:
    st.session_state.scraping_running = False

# ‚úÖ CRITIQUE : V√©rifier si on a un workflow GitHub Actions actif au d√©marrage
# Si oui, maintenir scraping_running = True pour garder le dashboard visible
# Note: get_github_workflow_status est d√©fini plus tard, donc on ne peut pas l'appeler ici
# Cette v√©rification sera faite dans la section du dashboard GitHub Actions

# ‚úÖ Options avanc√©es
with st.expander("‚öôÔ∏è Options avanc√©es"):
    col_adv1, col_adv2 = st.columns(2)
    with col_adv1:
        use_api_communes = st.checkbox(
            "Utiliser API data.gouv.fr pour les communes",
            value=False,
            help="R√©cup√®re automatiquement les communes depuis l'API officielle"
        )
        if use_api_communes:
            min_pop = st.number_input("Population minimum", min_value=0, value=0, step=100)
            max_pop = st.number_input("Population maximum", min_value=0, value=50000, step=1000)
            
            # ‚úÖ Bouton pour afficher les communes trouv√©es
            if st.button("üìã Afficher les communes trouv√©es"):
                st.session_state.show_communes = True
    with col_adv2:
        enable_resume = st.checkbox(
            "Activer resume/checkpoint",
            value=True,
            disabled=use_github_actions,  # D√©sactiver pour GitHub Actions
            help="Permet de reprendre le scraping o√π il s'est arr√™t√© (non disponible avec GitHub Actions)"
        )
        num_threads = st.slider(
            "Nombre de threads",
            min_value=1,
            max_value=20,
            value=3,
            help="Nombre de navigateurs en parall√®le (attention: plus de threads = plus rapide mais plus de ressources)"
        )

# ‚úÖ Afficher les communes si demand√©
if st.session_state.get('show_communes', False) and use_api_communes and departements:
    st.markdown("---")
    st.subheader("üìç Communes trouv√©es via API data.gouv.fr")
    
    communes_trouvees = {}
    with st.spinner("üîÑ R√©cup√©ration des communes depuis l'API..."):
        for dept in departements:
            communes = get_communes_from_api(dept, min_pop if use_api_communes else 0, max_pop if use_api_communes else 50000)
            communes_trouvees[dept] = communes
    
    # Afficher un tableau avec toutes les communes
    all_communes = []
    for dept, communes in communes_trouvees.items():
        for commune in communes:
            all_communes.append({
                'D√©partement': dept,
                'Commune': commune['nom'],
                'Code postal': commune['code_postal'],
                'Population': f"{commune['population']:,}" if commune['population'] > 0 else "N/A"
            })
    
    if all_communes:
        st.info(f"üìä Total: {len(all_communes)} communes trouv√©es")
        
        # ‚úÖ Mise en page c√¥te √† c√¥te : tableau et carte
        col_table, col_map = st.columns([1, 1])
        
        with col_table:
            st.subheader("üìã Liste des communes")
            df_communes = pd.DataFrame(all_communes)
            
            # CSS pour autofit toutes les colonnes et √©viter la colonne vide
            st.markdown("""
            <style>
            /* Cibler le conteneur du DataFrame */
            div[data-testid="stDataFrame"] {
                width: 100% !important;
            }
            div[data-testid="stDataFrame"] > div {
                width: 100% !important;
                overflow-x: visible !important;
            }
            /* Tableau avec ajustement automatique - largeur 100% mais colonnes auto */
            div[data-testid="stDataFrame"] table {
                width: 100% !important;
                table-layout: auto !important;
                border-collapse: collapse !important;
            }
            /* Colonnes avec ajustement automatique selon le contenu */
            div[data-testid="stDataFrame"] th,
            div[data-testid="stDataFrame"] td {
                white-space: nowrap !important;
                padding: 8px 12px !important;
                width: auto !important;
                max-width: none !important;
            }
            /* Masquer compl√®tement la derni√®re colonne si elle est vide */
            div[data-testid="stDataFrame"] table thead tr th:last-child:empty,
            div[data-testid="stDataFrame"] table tbody tr td:last-child:empty {
                display: none !important;
                width: 0 !important;
                padding: 0 !important;
                border: none !important;
            }
            </style>
            """, unsafe_allow_html=True)
            
            # Utiliser dataframe sans hide_index (non support√© dans cette version)
            st.dataframe(df_communes, height=600)
            
            # JavaScript pour masquer la colonne vide apr√®s le rendu
            st.markdown("""
            <script>
            function hideEmptyColumn() {
                const tables = document.querySelectorAll('div[data-testid="stDataFrame"] table');
                tables.forEach(function(table) {
                    const rows = table.querySelectorAll('tr');
                    if (rows.length > 0) {
                        // V√©rifier si la derni√®re colonne est vide dans toutes les lignes
                        let allEmpty = true;
                        rows.forEach(function(row) {
                            const lastCell = row.querySelector('th:last-child, td:last-child');
                            if (lastCell && lastCell.textContent && lastCell.textContent.trim() !== '') {
                                allEmpty = false;
                            }
                        });
                        
                        // Si toutes les derni√®res colonnes sont vides, les masquer
                        if (allEmpty) {
                            rows.forEach(function(row) {
                                const lastCell = row.querySelector('th:last-child, td:last-child');
                                if (lastCell) {
                                    lastCell.style.display = 'none';
                                    lastCell.style.width = '0';
                                    lastCell.style.padding = '0';
                                    lastCell.style.border = 'none';
                                }
                            });
                        }
                    }
                });
            }
            // Ex√©cuter apr√®s le chargement
            setTimeout(hideEmptyColumn, 100);
            setTimeout(hideEmptyColumn, 500);
            setTimeout(hideEmptyColumn, 1000);
            </script>
            """, unsafe_allow_html=True)
        
        with col_map:
            st.subheader("üó∫Ô∏è Carte interactive")
            
            # ‚úÖ Carte interactive avec folium
            try:
                import folium
                from streamlit_folium import folium_static
                
                # Filtrer les communes avec coordonn√©es GPS directement depuis communes_trouvees
                communes_avec_gps = []
                for dept, communes_list in communes_trouvees.items():
                    for comm in communes_list:
                        if comm.get('latitude') and comm.get('longitude'):
                            communes_avec_gps.append({
                                'nom': comm['nom'],
                                'departement': dept,
                                'code_postal': comm.get('code_postal', ''),
                                'population': comm.get('population', 0),
                                'latitude': comm['latitude'],
                                'longitude': comm['longitude']
                            })
                
                if communes_avec_gps:
                    # Calculer le centre de la carte (moyenne des coordonn√©es)
                    avg_lat = sum(c['latitude'] for c in communes_avec_gps) / len(communes_avec_gps)
                    avg_lon = sum(c['longitude'] for c in communes_avec_gps) / len(communes_avec_gps)
                    
                    # Cr√©er une carte centr√©e sur les communes
                    m = folium.Map(location=[avg_lat, avg_lon], zoom_start=8)
                    
                    # ‚úÖ Calculer les min/max de population POUR LES COMMUNES AFFICH√âES (limit√©es √† 200)
                    sample_communes = communes_avec_gps[:200] if len(communes_avec_gps) > 200 else communes_avec_gps
                    populations = [c['population'] for c in sample_communes if c['population'] > 0]
                    
                    if populations:
                        min_pop_displayed = min(populations)
                        max_pop_displayed = max(populations)
                        pop_range_displayed = max_pop_displayed - min_pop_displayed if max_pop_displayed > min_pop_displayed else 1
                    else:
                        min_pop_displayed = 0
                        max_pop_displayed = 1
                        pop_range_displayed = 1
                    
                    for commune in sample_communes:
                        pop = commune['population']
                        pop_str = f"{pop:,}" if pop > 0 else "N/A"
                        popup_text = f"<b>{commune['nom']}</b><br>D√©partement: {commune['departement']}<br>Code postal: {commune['code_postal']}<br>Population: {pop_str}"
                        
                        # ‚úÖ Taille du marqueur proportionnelle √† la population RELATIVE au min/max affich√©s
                        if pop > 0 and pop_range_displayed > 0:
                            # Normaliser entre 3 et 15 pixels de radius selon le min/max des communes affich√©es
                            normalized = (pop - min_pop_displayed) / pop_range_displayed
                            radius = 3 + (normalized * 12)  # Entre 3 et 15 pixels
                        else:
                            radius = 3
                        
                        # Couleur selon la population (seuils fixes pour la couleur)
                        if pop > 10000:
                            icon_color = 'red'
                        elif pop > 5000:
                            icon_color = 'orange'
                        elif pop > 2000:
                            icon_color = 'blue'
                        else:
                            icon_color = 'green'
                        
                        folium.CircleMarker(
                            location=[commune['latitude'], commune['longitude']],
                            radius=radius,
                            popup=folium.Popup(popup_text, max_width=200),
                            tooltip=f"{commune['nom']} ({pop:,} hab.)" if pop > 0 else commune['nom'],
                            color=icon_color,
                            fill=True,
                            fillColor=icon_color,
                            fillOpacity=0.6,
                            weight=1
                        ).add_to(m)
                    
                    # Afficher la carte
                    folium_static(m, width=700, height=600)
                    
                    if len(communes_avec_gps) > 200:
                        st.caption(f"üó∫Ô∏è Affichage de 200 communes sur {len(communes_avec_gps)} avec coordonn√©es GPS")
                    else:
                        st.caption(f"üó∫Ô∏è {len(communes_avec_gps)} communes avec coordonn√©es GPS")
                    
                    # L√©gende
                    st.markdown("""
                    <div style='background: #f0f0f0; padding: 10px; border-radius: 5px; margin-top: 10px;'>
                        <strong>L√©gende :</strong><br>
                        üî¥ Rouge : > 10 000 hab. | üü† Orange : 5 000 - 10 000 hab. | 
                        üîµ Bleu : 2 000 - 5 000 hab. | üü¢ Vert : < 2 000 hab.<br>
                        <small>La taille des points est proportionnelle √† la population</small>
                    </div>
                    """, unsafe_allow_html=True)
                else:
                    st.warning("‚ö†Ô∏è Aucune commune avec coordonn√©es GPS trouv√©e")
            except ImportError:
                st.info("üí° Pour afficher une carte interactive, installez: `pip install folium streamlit-folium`")
            except Exception as e:
                st.error(f"Erreur lors de la cr√©ation de la carte: {e}")
    else:
        st.warning("Aucune commune trouv√©e avec les crit√®res s√©lectionn√©s")
    
    if st.button("‚ùå Fermer l'affichage des communes"):
        st.session_state.show_communes = False
        st.experimental_rerun()

st.markdown("---")

# √âtat du scraping
if 'scraper' not in st.session_state:
    st.session_state.scraper = None
if 'scraping_running' not in st.session_state:
    st.session_state.scraping_running = False
if 'scraped_results' not in st.session_state:
    st.session_state.scraped_results = []
if 'scraping_thread' not in st.session_state:
    st.session_state.scraping_thread = None
if 'scraping_started' not in st.session_state:
    st.session_state.scraping_started = False
if 'saved_count' not in st.session_state:
    st.session_state.saved_count = 0
if 'logs_buffer' not in st.session_state:
    st.session_state.logs_buffer = []
# Note: github_workflow_id et github_workflow_status sont initialis√©s plus t√¥t dans le code

# ‚úÖ CRITIQUE : V√©rifier si on a un workflow GitHub Actions actif au d√©marrage
# Si oui, maintenir scraping_running = True pour garder le dashboard visible
# Cette v√©rification se fera plus tard, apr√®s le chargement de la config GitHub

# ‚úÖ Fonctions pour GitHub Actions
def trigger_github_workflow(token, repo, metiers, departements, max_results, num_threads, use_api_communes, min_pop, max_pop):
    """D√©clenche le workflow GitHub Actions"""
    try:
        # ‚úÖ D'abord, r√©cup√©rer la liste des workflows pour trouver le bon ID
        workflows_url = f"https://api.github.com/repos/{repo}/actions/workflows"
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        
        # R√©cup√©rer les workflows
        workflows_response = requests.get(workflows_url, headers=headers)
        if workflows_response.status_code != 200:
            return False, f"Erreur r√©cup√©ration workflows: {workflows_response.status_code} - {workflows_response.text}"
        
        workflows_data = workflows_response.json()
        workflow_id = None
        
        # Chercher le workflow "Google Maps Scraping" ou "scraping.yml"
        for workflow in workflows_data.get('workflows', []):
            if workflow.get('name') == 'Google Maps Scraping' or workflow.get('path', '').endswith('scraping.yml'):
                workflow_id = workflow.get('id')
                break
        
        if not workflow_id:
            # Essayer avec le nom du fichier directement
            url = f"https://api.github.com/repos/{repo}/actions/workflows/scraping.yml/dispatches"
        else:
            # Utiliser l'ID du workflow (plus fiable)
            url = f"https://api.github.com/repos/{repo}/actions/workflows/{workflow_id}/dispatches"
        
        data = {
            "ref": "main",  # Essayer "main" d'abord
            "inputs": {
                "metiers": json.dumps(metiers),
                "departements": json.dumps(departements),
                "max_results": str(max_results),
                "num_threads": str(num_threads),
                "use_api_communes": str(use_api_communes).lower(),
                "min_pop": str(min_pop),
                "max_pop": str(max_pop)
            }
        }
        
        response = requests.post(url, headers=headers, json=data)
        
        # Si 404 avec "main", essayer "master"
        if response.status_code == 404 and data["ref"] == "main":
            data["ref"] = "master"
            response = requests.post(url, headers=headers, json=data)
        
        if response.status_code == 204:
            # ‚úÖ R√©cup√©rer le run_id du workflow qui vient d'√™tre lanc√©
            # Attendre un peu pour que GitHub cr√©e le run
            import time
            time.sleep(2)
            
            # R√©cup√©rer le dernier run du workflow
            runs_url = f"https://api.github.com/repos/{repo}/actions/workflows/scraping.yml/runs?per_page=1"
            runs_response = requests.get(runs_url, headers=headers)
            if runs_response.status_code == 200:
                runs_data = runs_response.json().get('workflow_runs', [])
                if runs_data:
                    run_id = runs_data[0].get('id')
                    return True, f"Workflow d√©clench√© avec succ√®s (Run ID: {run_id})", run_id
            
            return True, "Workflow d√©clench√© avec succ√®s"
        else:
            error_msg = response.text
            if response.status_code == 404:
                error_msg += f"\nüí° V√©rifiez que le workflow existe dans .github/workflows/scraping.yml et qu'il est commit√© sur GitHub"
            return False, f"Erreur: {response.status_code} - {error_msg}", None
    except Exception as e:
        return False, f"Erreur: {str(e)}"

def get_github_workflow_status(token, repo, workflow_id=None):
    """R√©cup√®re le statut du workflow GitHub Actions"""
    try:
        if workflow_id:
            url = f"https://api.github.com/repos/{repo}/actions/runs/{workflow_id}"
        else:
            # R√©cup√©rer le dernier run (non annul√© si possible)
            url = f"https://api.github.com/repos/{repo}/actions/workflows/scraping.yml/runs?per_page=10"
        
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            if workflow_id:
                data = response.json()
            else:
                # R√©cup√©rer tous les runs et trouver le dernier non-annul√© (ou le dernier tout court)
                runs = response.json().get('workflow_runs', [])
                if not runs:
                    return None, None, None
                
                # Chercher le dernier run non-annul√©
                for run in runs:
                    if run.get('conclusion') != 'cancelled':
                        data = run
                        break
                else:
                    # Si tous sont annul√©s, prendre le dernier
                    data = runs[0]
            
            status = data.get('status')  # queued, in_progress, completed
            conclusion = data.get('conclusion')  # success, failure, cancelled, etc.
            run_id = data.get('id')
            return status, conclusion, run_id
        else:
            return None, None, None
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration statut: {e}")
        return None, None, None

def download_github_artifact(token, repo, run_id):
    """T√©l√©charge l'artifact depuis GitHub Actions"""
    try:
        # R√©cup√©rer la liste des artifacts
        url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id}/artifacts"
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            artifacts = response.json().get('artifacts', [])
            for artifact in artifacts:
                if artifact.get('name') == 'scraping-results':
                    # T√©l√©charger l'artifact
                    download_url = artifact.get('archive_download_url')
                    if download_url:
                        download_response = requests.get(download_url, headers=headers)
                        if download_response.status_code == 200:
                            # Sauvegarder le zip
                            import zipfile
                            import io
                            data_dir = Path(__file__).parent.parent.parent / "data"
                            zip_path = data_dir / "github_artifact.zip"
                            with open(zip_path, 'wb') as f:
                                f.write(download_response.content)
                            
                            # Extraire le JSON
                            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                                zip_ref.extractall(data_dir)
                            
                            # Lire les fichiers
                            results_file = data_dir / "scraping_results_github_actions.json"
                            status_file = data_dir / "github_actions_status.json"
                            
                            result_data = None
                            status_data = None
                            
                            if results_file.exists():
                                with open(results_file, 'r', encoding='utf-8') as f:
                                    result_data = json.load(f)
                            
                            if status_file.exists():
                                with open(status_file, 'r', encoding='utf-8') as f:
                                    status_data = json.load(f)
                            
                            # Nettoyer
                            zip_path.unlink()
                            
                            # ‚úÖ Retourner dans le format attendu (compatibilit√©)
                            if result_data and isinstance(result_data, dict) and 'results' in result_data:
                                return result_data  # Format: {'results': [...], 'total_results': ...}
                            elif result_data and isinstance(result_data, list):
                                return {'results': result_data, 'total_results': len(result_data)}
                            else:
                                return {'results': [], 'total_results': 0, 'status': status_data}
            return None
        return None
    except Exception as e:
        logger.error(f"Erreur t√©l√©chargement artifact: {e}")
        return None

def get_github_workflow_logs(token, repo, run_id):
    """R√©cup√®re les logs du workflow GitHub Actions"""
    try:
        # R√©cup√©rer les jobs du workflow
        url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id}/jobs"
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            jobs = response.json().get('jobs', [])
            logs = []
            for job in jobs:
                if job.get('status') == 'completed':
                    # R√©cup√©rer les logs du job
                    logs_url = job.get('logs_url')
                    if logs_url:
                        logs_response = requests.get(logs_url, headers=headers)
                        if logs_response.status_code == 200:
                            # Les logs sont dans un format sp√©cifique GitHub
                            logs.append({
                                'job_name': job.get('name'),
                                'status': job.get('conclusion'),
                                'logs_url': logs_url
                            })
            return logs
        return []
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration logs: {e}")
        return []

def list_github_workflows(token, repo):
    """Liste tous les workflows GitHub Actions en cours"""
    try:
        # R√©cup√©rer les runs du workflow scraping
        runs_url = f"https://api.github.com/repos/{repo}/actions/runs"
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        
        params = {
            "status": "in_progress,queued",
            "per_page": 100
        }
        
        response = requests.get(runs_url, headers=headers, params=params)
        if response.status_code != 200:
            return []
        
        runs_data = response.json()
        workflows = []
        
        for run in runs_data.get('workflow_runs', []):
            workflows.append({
                'id': run.get('id'),
                'run_number': run.get('run_number'),
                'status': run.get('status'),
                'conclusion': run.get('conclusion'),
                'created_at': run.get('created_at'),
                'updated_at': run.get('updated_at'),
                'head_branch': run.get('head_branch'),
                'workflow_id': run.get('workflow_id'),
                'html_url': run.get('html_url')
            })
        
        return workflows
    except Exception as e:
        logger.error(f"Erreur liste workflows: {e}")
        return []

def cancel_github_workflow(token, repo, run_id):
    """Annule un workflow GitHub Actions sp√©cifique"""
    try:
        cancel_url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id}/cancel"
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        
        response = requests.post(cancel_url, headers=headers)
        return response.status_code == 202
    except Exception as e:
        logger.error(f"Erreur annulation workflow {run_id}: {e}")
        return False

def cancel_all_github_workflows(token, repo):
    """Annule tous les workflows GitHub Actions en cours (in_progress et queued)"""
    try:
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        
        cancelled_count = 0
        total_runs = 0
        
        # R√©cup√©rer les runs "in_progress"
        url_in_progress = f"https://api.github.com/repos/{repo}/actions/runs?status=in_progress&per_page=100"
        response = requests.get(url_in_progress, headers=headers)
        if response.status_code == 200:
            runs = response.json().get('workflow_runs', [])
            total_runs += len(runs)
            for run in runs:
                run_id = run.get('id')
                if run_id:
                    cancel_url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id}/cancel"
                    cancel_response = requests.post(cancel_url, headers=headers)
                    if cancel_response.status_code == 202:
                        cancelled_count += 1
        elif response.status_code != 200:
            logger.warning(f"Erreur r√©cup√©ration runs in_progress: {response.status_code}")
        
        # R√©cup√©rer les runs "queued"
        url_queued = f"https://api.github.com/repos/{repo}/actions/runs?status=queued&per_page=100"
        response = requests.get(url_queued, headers=headers)
        if response.status_code == 200:
            runs = response.json().get('workflow_runs', [])
            total_runs += len(runs)
            for run in runs:
                run_id = run.get('id')
                if run_id:
                    cancel_url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id}/cancel"
                    cancel_response = requests.post(cancel_url, headers=headers)
                    if cancel_response.status_code == 202:
                        cancelled_count += 1
        elif response.status_code != 200:
            logger.warning(f"Erreur r√©cup√©ration runs queued: {response.status_code}")
        
        if total_runs == 0:
            return True, "Aucun workflow en cours √† annuler"
        elif cancelled_count == total_runs:
            return True, f"‚úÖ {cancelled_count} workflow(s) annul√©(s) sur {total_runs} trouv√©(s)"
        else:
            return False, f"‚ö†Ô∏è {cancelled_count} workflow(s) annul√©(s) sur {total_runs} trouv√©(s) (certains n'ont peut-√™tre pas pu √™tre annul√©s)"
    except Exception as e:
        logger.error(f"Erreur annulation workflows: {e}")
        return False, f"Erreur: {str(e)}"

# ‚úÖ Cette section a √©t√© d√©plac√©e en haut pour √™tre visible d√®s le d√©marrage (voir ligne ~200)

# ‚úÖ Boutons de contr√¥le SIMPLIFI√âS (comme demand√©)
col_btn1, col_btn2, col_btn3 = st.columns(3)

with col_btn1:
    # ‚úÖ GitHub Actions uniquement - pas de mode local
    button_disabled = st.session_state.scraping_running or (st.session_state.github_workflow_status == 'in_progress')
    button_text = "‚òÅÔ∏è LANCER"
    
    if st.button(button_text, disabled=button_disabled):
        if not metiers or not departements:
            st.error("‚ö†Ô∏è Veuillez s√©lectionner au moins un m√©tier et un d√©partement")
        elif not github_token or not github_repo:
            st.error("‚ö†Ô∏è Veuillez renseigner le token GitHub et le repository")
        else:
            # ‚úÖ V√©rifier les villes d√©j√† scrap√©es
            villes_deja_scrapees = []
            villes_a_scraper = []
            
            # Pr√©parer la liste des villes √† scraper
            for dept in departements:
                if use_api_communes:
                    communes = get_communes_from_api(dept, min_pop if use_api_communes else 0, max_pop if use_api_communes else 50000)
                    villes_dept = [c['nom'] for c in communes]
                else:
                    villes_dept = villes_par_dept.get(dept, [])
                
                for metier in metiers:
                    for ville in villes_dept:
                        if is_already_scraped(metier, dept, ville):
                            villes_deja_scrapees.append(f"{metier} - {dept} - {ville}")
                        else:
                            villes_a_scraper.append(f"{metier} - {dept} - {ville}")
            
            # Afficher un avertissement si certaines villes sont d√©j√† scrap√©es
            if villes_deja_scrapees:
                st.warning(f"‚ö†Ô∏è {len(villes_deja_scrapees)} combinaison(s) m√©tier/d√©partement/ville d√©j√† scrap√©e(s). Elles seront ignor√©es si vous continuez.")
                with st.expander("üìã Voir les villes d√©j√† scrap√©es"):
                    for v in villes_deja_scrapees[:20]:  # Limiter √† 20 pour l'affichage
                        st.text(v)
                    if len(villes_deja_scrapees) > 20:
                        st.caption(f"... et {len(villes_deja_scrapees) - 20} autres")
            
            if not villes_a_scraper:
                st.error("‚ùå Toutes les combinaisons s√©lectionn√©es ont d√©j√† √©t√© scrap√©es. Veuillez s√©lectionner d'autres options.")
            else:
                st.info(f"‚úÖ {len(villes_a_scraper)} combinaison(s) √† scraper")
                
                # D√©clencher le workflow GitHub Actions
                result = trigger_github_workflow(
                    github_token,
                    github_repo,
                    metiers,
                    departements,
                    max_results,
                    num_threads,
                    use_api_communes,
                    min_pop if use_api_communes else 0,
                    max_pop if use_api_communes else 50000
                )
                
                # ‚úÖ G√©rer le retour (peut √™tre (success, message) ou (success, message, run_id))
                if len(result) == 3:
                    success, message, run_id = result
                else:
                    success, message = result
                    run_id = None
                
                if success:
                    st.success(f"‚úÖ {message}")
                    st.info("‚è≥ Le scraping est en cours sur GitHub Actions. Les r√©sultats sont sauvegard√©s directement dans la BDD.")
                    st.session_state.scraping_running = True
                    st.session_state.github_workflow_status = 'in_progress'
                    st.session_state.departements_selected = departements
                    st.session_state.metiers_selected = metiers
                    
                    # ‚úÖ Stocker le run_id si disponible
                    if run_id:
                        st.session_state.github_workflow_id = run_id
                    # ‚úÖ Marquer qu'on vient de lancer un workflow pour ne pas l'annuler
                    st.session_state.workflow_just_launched = True
                    st.experimental_rerun()
                else:
                    st.error(f"‚ùå {message}")

with col_btn2:
    # ‚úÖ Bouton ARR√äTER (simplifi√© - pas de confirmation)
    if github_token and github_repo:
        if st.button("‚èπÔ∏è ARR√äTER", help="Arr√™ter tous les workflows GitHub Actions en cours", key="stop_all_workflows"):
            with st.spinner("‚èπÔ∏è Arr√™t des workflows..."):
                success, message = cancel_all_github_workflows(github_token, github_repo)
                if success:
                    st.success(f"‚úÖ {message}")
                    st.session_state.scraping_running = False
                    st.session_state.github_workflow_status = None
                    st.session_state.github_workflow_id = None
                    st.session_state.scraped_results = []
                    st.experimental_rerun()
                else:
                    st.error(f"‚ùå {message}")

with col_btn3:
    # ‚úÖ Bouton RAFRA√éCHIR
    if st.button("üîÑ RAFRA√éCHIR", help="Rafra√Æchir le statut des workflows et les r√©sultats", key="refresh_workflows"):
        st.experimental_rerun()

st.markdown("---")

# Zone de scraping
# ‚úÖ IMPORTANT: Afficher le dashboard si on a un workflow_id OU si scraping_running est True
# Cela permet de garder le dashboard visible m√™me apr√®s un refresh
current_use_github = st.session_state.get('use_github_actions', True)  # Toujours True maintenant
has_workflow_id = st.session_state.get('github_workflow_id') is not None
should_show_dashboard = st.session_state.scraping_running or has_workflow_id

if should_show_dashboard:
    # ‚úÖ V√©rifier si on utilise GitHub Actions
    if current_use_github and github_token and github_repo:
        st.subheader("‚òÅÔ∏è Dashboard GitHub Actions")
        
        # ‚úÖ R√©cup√©rer le statut frais depuis GitHub API (ne pas utiliser l'ancien statut)
        # Si le workflow_id stock√© est annul√©, r√©cup√©rer le dernier non-annul√©
        current_workflow_id = st.session_state.github_workflow_id
        status, conclusion, run_id = get_github_workflow_status(github_token, github_repo, current_workflow_id)
        
        # ‚úÖ Si le workflow r√©cup√©r√© est annul√© mais qu'on a un workflow_id diff√©rent, essayer de r√©cup√©rer le bon
        if conclusion == 'cancelled' and current_workflow_id and run_id == current_workflow_id:
            # Le workflow_id stock√© a √©t√© annul√©, r√©cup√©rer le dernier non-annul√©
            status, conclusion, run_id = get_github_workflow_status(github_token, github_repo, None)
        
        # ‚úÖ Mettre √† jour le statut dans session_state avec les donn√©es fra√Æches
        if status:
            st.session_state.github_workflow_status = status
        if conclusion:
            # Stocker aussi la conclusion pour l'affichage
            st.session_state.github_workflow_conclusion = conclusion
        if run_id:
            st.session_state.github_workflow_id = run_id
            # Maintenir scraping_running pour garder le dashboard visible
            # M√™me si termin√© (success, failure, cancelled), on garde le dashboard pour voir les r√©sultats
            st.session_state.scraping_running = True
        
        # ‚úÖ Si le workflow est annul√©, mettre √† jour le statut mais garder le dashboard visible
        if conclusion == 'cancelled':
            st.session_state.github_workflow_status = 'completed'
            # Ne PAS mettre scraping_running = False pour garder le dashboard visible
            # L'utilisateur peut voir que le workflow a √©t√© annul√©
        
        # ‚úÖ Si on a un workflow_id mais pas de statut, essayer de le r√©cup√©rer
        if st.session_state.github_workflow_id and not status:
            status, conclusion, run_id = get_github_workflow_status(github_token, github_repo, st.session_state.github_workflow_id)
            if status:
                st.session_state.github_workflow_status = status
                if conclusion:
                    st.session_state.github_workflow_conclusion = conclusion
                if run_id:
                    st.session_state.github_workflow_id = run_id
                st.session_state.scraping_running = True
        
        # ‚úÖ Dashboard visuel avec colonnes
        col_status, col_progress, col_actions = st.columns([2, 3, 1])
        
        with col_status:
            # Statut avec badge color√©
            if status == 'completed':
                if conclusion == 'success':
                    st.success("‚úÖ **Termin√© avec succ√®s**")
                elif conclusion == 'failure':
                    st.error("‚ùå **√âchec**")
                elif conclusion == 'cancelled':
                    st.warning("‚èπÔ∏è **Annul√©**")
                else:
                    st.warning(f"‚ö†Ô∏è **{conclusion or 'Termin√©'}**")
            elif status == 'in_progress':
                st.info("üîÑ **En cours...**")
            elif status == 'queued':
                st.info("‚è≥ **En attente...**")
            else:
                st.info(f"üìä **{status}**")
        
        with col_progress:
            # ‚úÖ Essayer de charger le statut depuis le fichier local (si t√©l√©charg√©)
            status_file = Path(__file__).parent.parent.parent / "data" / "github_actions_status.json"
            if status_file.exists():
                try:
                    with open(status_file, 'r', encoding='utf-8') as f:
                        status_data = json.load(f)
                        total_tasks = status_data.get('total_tasks', 0)
                        completed_tasks = status_data.get('completed_tasks', 0)
                        total_results = status_data.get('total_results', 0)
                        
                        if total_tasks > 0:
                            progress_pct = (completed_tasks / total_tasks) * 100
                            st.progress(progress_pct / 100)
                            st.caption(f"üìä {completed_tasks}/{total_tasks} villes scrap√©es | {total_results} r√©sultats trouv√©s")
                        else:
                            st.caption("‚è≥ Initialisation...")
                except:
                    pass
            else:
                if status == 'in_progress' or status == 'queued':
                    st.caption("‚è≥ En attente des premi√®res donn√©es...")
        
        with col_actions:
            if run_id:
                github_url = f"https://github.com/{github_repo}/actions/runs/{run_id}"
                st.markdown(f"[üîó Voir logs]({github_url})")
        
        # ‚úÖ Section d√©taill√©e
        with st.expander("üìã D√©tails du workflow", expanded=True):
            # Informations du workflow
            if run_id:
                st.write(f"**Run ID:** `{run_id}`")
                st.write(f"**Statut:** {status}")
                if conclusion:
                    st.write(f"**Conclusion:** {conclusion}")
            
            # ‚úÖ Charger et afficher le statut d√©taill√©
            status_file = Path(__file__).parent.parent.parent / "data" / "github_actions_status.json"
            if status_file.exists():
                try:
                    with open(status_file, 'r', encoding='utf-8') as f:
                        status_data = json.load(f)
                        
                        col_info1, col_info2, col_info3 = st.columns(3)
                        with col_info1:
                            st.metric("Villes totales", status_data.get('total_tasks', 0))
                        with col_info2:
                            st.metric("Villes compl√©t√©es", status_data.get('completed_tasks', 0))
                        with col_info3:
                            st.metric("R√©sultats trouv√©s", status_data.get('total_results', 0))
                        
                        # ‚úÖ Afficher les r√©sultats progressifs si disponibles (charg√©s automatiquement au refresh)
                        # Les r√©sultats sont d√©j√† charg√©s dans session_state par le bouton "Rafra√Æchir"
                        if st.session_state.get('scraped_results'):
                            results_count = len(st.session_state.scraped_results)
                            st.success(f"üì• {results_count} r√©sultats disponibles")
                            
                            # Afficher un aper√ßu
                            if results_count > 0:
                                preview_df = pd.DataFrame(st.session_state.scraped_results[:10])
                                if not preview_df.empty:
                                    st.caption("üëÄ Aper√ßu des r√©sultats (10 premiers):")
                                    # S√©lectionner les colonnes disponibles
                                    available_cols = ['nom', 'telephone', 'site_web', 'ville_recherche']
                                    cols_to_show = [col for col in available_cols if col in preview_df.columns]
                                    if cols_to_show:
                                        st.dataframe(preview_df[cols_to_show].head(10), use_container_width=True)
                        else:
                            # Essayer de charger depuis le fichier si pas encore charg√©
                            results_file = Path(__file__).parent.parent.parent / "data" / "scraping_results_github_actions.json"
                            if results_file.exists():
                                try:
                                    with open(results_file, 'r', encoding='utf-8') as f:
                                        results_data = json.load(f)
                                        if isinstance(results_data, dict) and 'results' in results_data:
                                            results_list = results_data['results']
                                        elif isinstance(results_data, list):
                                            results_list = results_data
                                        else:
                                            results_list = []
                                        
                                        if results_list:
                                            st.session_state.scraped_results = results_list
                                            st.success(f"üì• {len(results_list)} r√©sultats disponibles")
                                            
                                            # Afficher un aper√ßu
                                            preview_df = pd.DataFrame(results_list[:10])
                                            if not preview_df.empty:
                                                st.caption("üëÄ Aper√ßu des r√©sultats (10 premiers):")
                                                available_cols = ['nom', 'telephone', 'site_web', 'ville_recherche']
                                                cols_to_show = [col for col in available_cols if col in preview_df.columns]
                                                if cols_to_show:
                                                    st.dataframe(preview_df[cols_to_show].head(10), use_container_width=True)
                                except Exception as e:
                                    logger.error(f"Erreur lecture r√©sultats: {e}")
                except Exception as e:
                    st.error(f"Erreur lecture statut: {e}")
        
        # ‚úÖ Boutons de contr√¥le simplifi√©s
        col_btn1, col_btn2, col_btn3 = st.columns(3)
        
        with col_btn1:
            # Bouton pour rafra√Æchir le statut (remplace auto-refresh)
            if st.button("üîÑ Rafra√Æchir", key="refresh_github"):
                # ‚úÖ CRITIQUE : Maintenir TOUJOURS le workflow_id et scraping_running pour garder le dashboard visible
                saved_workflow_id = st.session_state.get('github_workflow_id')
                
                # Si on a un workflow_id, on le maintient ABSOLUMENT pour garder le dashboard visible
                if saved_workflow_id:
                    st.session_state.github_workflow_id = saved_workflow_id
                    st.session_state.scraping_running = True  # TOUJOURS True si on a un workflow_id
                
                # ‚úÖ Charger automatiquement les r√©sultats progressifs depuis le fichier local
                results_file = Path(__file__).parent.parent.parent / "data" / "scraping_results_github_actions.json"
                if results_file.exists():
                    try:
                        with open(results_file, 'r', encoding='utf-8') as f:
                            results_data = json.load(f)
                            if isinstance(results_data, dict) and 'results' in results_data:
                                results_list = results_data['results']
                            elif isinstance(results_data, list):
                                results_list = results_data
                            else:
                                results_list = []
                            
                            if results_list:
                                # Mettre √† jour les r√©sultats affich√©s
                                st.session_state.scraped_results = results_list
                                st.session_state.saved_count = len(results_list)
                    except Exception as e:
                        logger.error(f"Erreur chargement r√©sultats: {e}")
                
                # Le statut sera r√©cup√©r√© frais depuis GitHub API lors du rerun
                # Ne PAS restaurer l'ancien statut - laisser get_github_workflow_status le r√©cup√©rer frais
                
                # Forcer le rerun pour rafra√Æchir les donn√©es depuis GitHub
                st.experimental_rerun()

        with col_btn2:
            # ‚úÖ Bouton pour t√©l√©charger les r√©sultats - TOUJOURS visible si on a un workflow_id
            # M√™me si le workflow est cancelled ou failed, il peut y avoir des r√©sultats partiels
            if run_id:
                if st.button("üì• T√©l√©charger les r√©sultats", key="download_progress"):
                    # ‚úÖ T√©l√©charger depuis l'artifact ou le fichier local
                    results_file = Path(__file__).parent.parent.parent / "data" / "scraping_results_github_actions.json"
                    results_list = []
                    
                    # Essayer d'abord le fichier local (si d√©j√† t√©l√©charg√©)
                    if results_file.exists():
                        try:
                            with open(results_file, 'r', encoding='utf-8') as f:
                                results_data = json.load(f)
                                if isinstance(results_data, dict) and 'results' in results_data:
                                    results_list = results_data['results']
                                elif isinstance(results_data, list):
                                    results_list = results_data
                        except:
                            pass
                    
                    # Si pas de fichier local et workflow termin√©, t√©l√©charger l'artifact
                    if not results_list and status == 'completed' and run_id:
                        with st.spinner("üì• T√©l√©chargement depuis GitHub..."):
                            artifact_data = download_github_artifact(github_token, github_repo, run_id)
                            if artifact_data:
                                if isinstance(artifact_data, dict) and 'results' in artifact_data:
                                    results_data = artifact_data['results']
                                    if isinstance(results_data, dict) and 'results' in results_data:
                                        results_list = results_data['results']
                                    elif isinstance(results_data, list):
                                        results_list = results_data
                    
                    if results_list:
                        # Sauvegarder automatiquement en BDD avec TOUTES les donn√©es
                        saved_count = 0
                        for info in results_list:
                            try:
                                artisan_data = {
                                    'nom_entreprise': info.get('nom', 'N/A'),
                                    'telephone': info.get('telephone', '').replace(' ', '') if info.get('telephone') else None,
                                    'adresse': info.get('adresse', ''),
                                    'code_postal': info.get('code_postal', ''),
                                    'ville': info.get('ville', ''),
                                    'ville_recherche': info.get('ville_recherche', ''),
                                    'type_artisan': info.get('recherche', metiers[0] if metiers else 'plombier'),
                                    'source': 'google_maps_github_actions'
                                }
                                
                                if info.get('site_web'):
                                    artisan_data['site_web'] = info.get('site_web')
                                
                                # ‚úÖ Ajouter note et nombre_avis
                                if info.get('note'):
                                    artisan_data['note'] = float(info.get('note'))
                                if info.get('nb_avis') or info.get('nombre_avis'):
                                    artisan_data['nombre_avis'] = int(info.get('nb_avis') or info.get('nombre_avis', 0))
                                
                                ajouter_artisan(artisan_data)
                                saved_count += 1
                            except Exception as e:
                                if "UNIQUE constraint" not in str(e) and "duplicate" not in str(e).lower():
                                    logger.error(f"Erreur sauvegarde: {e}")
                        
                        # Mettre √† jour les r√©sultats affich√©s
                        st.session_state.scraped_results = results_list
                        st.session_state.saved_count = saved_count
                        
                        st.success(f"‚úÖ {len(results_list)} r√©sultats t√©l√©charg√©s et {saved_count} sauvegard√©s en BDD !")
                        st.experimental_rerun()
                    else:
                        st.warning("‚ö†Ô∏è Aucun r√©sultat disponible pour le moment. Le scraping est peut-√™tre encore en cours.")
        
        with col_btn3:
            # Bouton pour arr√™ter le workflow
            if status == 'in_progress' or status == 'queued':
                if st.button("‚èπÔ∏è Arr√™ter", key="stop_github"):
                    try:
                        url = f"https://api.github.com/repos/{github_repo}/actions/runs/{run_id}/cancel"
                        headers = {
                            "Accept": "application/vnd.github+json",
                            "Authorization": f"Bearer {github_token}",
                            "X-GitHub-Api-Version": "2022-11-28"
                        }
                        response = requests.post(url, headers=headers)
                        if response.status_code == 202:
                            st.success("‚èπÔ∏è Annulation demand√©e. Le workflow sera arr√™t√© dans quelques instants.")
                            # Mettre √† jour le statut localement
                            st.session_state.github_workflow_status = 'cancelled'
                            # Ne pas r√©initialiser scraping_running imm√©diatement pour permettre de voir le statut final
                            time.sleep(1)  # Petite pause pour que l'utilisateur voie le message
                        else:
                            st.warning(f"‚ö†Ô∏è Erreur lors de l'annulation: {response.status_code}")
                            if response.text:
                                logger.error(f"R√©ponse API: {response.text}")
                    except Exception as e:
                        st.error(f"‚ùå Erreur: {e}")
                        logger.error(f"Erreur annulation workflow: {e}")
                    st.experimental_rerun()
        
        # ‚úÖ Gestion des workflows termin√©s (success, failure, ou cancelled)
        if status == 'completed':
            if conclusion == 'failure':
                st.error("‚ùå Le scraping a √©chou√© sur GitHub Actions. V√©rifiez les logs sur GitHub.")
                st.info("üí° Vous pouvez quand m√™me essayer de t√©l√©charger les r√©sultats partiels avec le bouton 'üì• T√©l√©charger les r√©sultats' ci-dessus.")
            elif conclusion == 'success':
                st.success("‚úÖ Le scraping est termin√© avec succ√®s !")
                st.info("üí° Utilisez le bouton 'üì• T√©l√©charger les r√©sultats' ci-dessus pour r√©cup√©rer les donn√©es.")
            elif conclusion == 'cancelled':
                st.warning("‚èπÔ∏è Le scraping a √©t√© annul√©.")
                st.info("üí° Si le scraping avait commenc√©, vous pouvez essayer de t√©l√©charger les r√©sultats partiels avec le bouton 'üì• T√©l√©charger les r√©sultats' ci-dessus.")
            
            # Bouton pour r√©initialiser et permettre un nouveau lancement
            if st.button("üîÑ R√©initialiser et permettre nouveau lancement", key="reset_completed"):
                st.session_state.scraping_running = False
                st.session_state.github_workflow_status = None
                st.session_state.github_workflow_id = None
                st.session_state.github_workflow_conclusion = None
                st.success("‚úÖ √âtat r√©initialis√©. Vous pouvez lancer un nouveau scraping.")
                st.experimental_rerun()
        elif not status:
            # ‚úÖ Si on a un workflow_id mais pas de statut, c'est qu'on attend encore ou erreur API
            if st.session_state.github_workflow_id:
                st.warning("‚è≥ En attente du statut du workflow...")
                st.info("üí° Cliquez sur 'Rafra√Æchir' pour v√©rifier √† nouveau")
                # ‚úÖ CRITIQUE : Maintenir scraping_running pour garder le dashboard visible
                st.session_state.scraping_running = True
            else:
                st.warning("‚è≥ En attente du d√©marrage du workflow...")
                # Si on n'a pas encore de workflow_id, on peut r√©initialiser
                # Mais seulement si l'utilisateur le demande explicitement

elif not should_show_dashboard:
    # ‚úÖ Pas de dashboard √† afficher - le formulaire de lancement sera affich√© plus haut
    # Mais d'abord, v√©rifier si on doit annuler les workflows au d√©marrage
    # ‚úÖ NE PAS annuler si on vient juste de lancer un workflow
    if github_token and github_repo and not st.session_state.get('workflow_just_launched', False):
        # ‚úÖ Tuer automatiquement tous les workflows en cours au d√©marrage (une seule fois)
        if not st.session_state.get('workflows_cancelled_on_start', False):
            with st.spinner("‚èπÔ∏è Annulation des workflows GitHub Actions en cours..."):
                success, message = cancel_all_github_workflows(github_token, github_repo)
                if success:
                    st.session_state.workflows_cancelled_on_start = True
                    # Ne pas r√©initialiser scraping_running si on a un workflow_id
                    if not st.session_state.get('github_workflow_id'):
                        st.session_state.scraping_running = False
                        st.session_state.github_workflow_status = None
                        st.session_state.github_workflow_id = None
                    st.success(f"‚úÖ {message}")
                else:
                    st.warning(f"‚ö†Ô∏è {message}")
            st.experimental_rerun()
    # ‚úÖ R√©initialiser le flag apr√®s le premier rerun
    if st.session_state.get('workflow_just_launched', False):
        st.session_state.workflow_just_launched = False

# ‚úÖ GitHub Actions uniquement - plus de code local

# Afficher les r√©sultats scrap√©s
if st.session_state.scraped_results:
    st.markdown("---")
    st.subheader("üìä R√©sultats scrap√©s")
    
    df = pd.DataFrame(st.session_state.scraped_results)
    
    # Stats
    avec_tel = len(df[df['telephone'].notna()])
    avec_site = len(df[df['site_web'].notna()])
    sans_site = len(df[df['site_web'].isna()])
    
    col_res1, col_res2, col_res3, col_res4 = st.columns(4)
    with col_res1:
        st.metric("Total scrap√©s", len(df))
    with col_res2:
        st.metric("Avec t√©l√©phone", f"{avec_tel} ({avec_tel/len(df)*100:.1f}%)")
    with col_res3:
        st.metric("Avec site web", f"{avec_site} ({avec_site/len(df)*100:.1f}%)")
    with col_res4:
        st.metric("‚≠ê SANS site web", f"{sans_site} ({sans_site/len(df)*100:.1f}%)")
    
    # Filtrer les r√©sultats
    st.markdown("### üîç Filtres")
    col_filt1, col_filt2 = st.columns(2)
    
    with col_filt1:
        filtre_tel = st.checkbox("Avec t√©l√©phone uniquement", value=False)
    with col_filt2:
        filtre_sans_site = st.checkbox("Sans site web uniquement (prospects)", value=False)
    
    df_filtre = df.copy()
    if filtre_tel:
        df_filtre = df_filtre[df_filtre['telephone'].notna()]
    if filtre_sans_site:
        df_filtre = df_filtre[df_filtre['site_web'].isna()]
    
    # ‚úÖ Afficher le tableau avec colonne ville_recherche et meilleur affichage
    colonnes_afficher = ['nom', 'telephone', 'site_web', 'adresse', 'ville_recherche', 'ville', 'note', 'nb_avis']
    colonnes_disponibles = [col for col in colonnes_afficher if col in df_filtre.columns]
    
    # CSS am√©lior√© pour le tableau
    st.markdown("""
    <style>
    .stDataFrame {
        width: 100% !important;
    }
    .stDataFrame > div {
        width: 100% !important;
    }
    .stDataFrame table {
        width: 100% !important;
        table-layout: auto !important;
    }
    .stDataFrame th, .stDataFrame td {
        padding: 8px !important;
        word-wrap: break-word !important;
        overflow-wrap: break-word !important;
        white-space: normal !important;
    }
    .stDataFrame th:nth-child(1) { width: 15% !important; } /* Nom */
    .stDataFrame th:nth-child(2) { width: 10% !important; } /* T√©l√©phone */
    .stDataFrame th:nth-child(3) { width: 20% !important; } /* Site web */
    .stDataFrame th:nth-child(4) { width: 20% !important; } /* Adresse */
    .stDataFrame th:nth-child(5) { width: 12% !important; } /* Ville recherch√©e */
    .stDataFrame th:nth-child(6) { width: 10% !important; } /* Ville */
    .stDataFrame th:nth-child(7) { width: 6% !important; } /* Note */
    .stDataFrame th:nth-child(8) { width: 7% !important; } /* Nb avis */
    </style>
    """, unsafe_allow_html=True)
    
    # ‚úÖ Utiliser width au lieu de use_container_width (compatibilit√© Streamlit)
    st.dataframe(
        df_filtre[colonnes_disponibles],
        height=600
    )
    
    # ‚úÖ Boutons d'export (gard√©s car utiles)
    col_exp1, col_exp2, col_exp3 = st.columns(3)
    
    with col_exp1:
        csv_all = df.to_csv(index=False, encoding='utf-8-sig')
        dept = st.session_state.get('departements_selected', departements)[0] if st.session_state.get('departements_selected') else (departements[0] if departements else '77')
        metier_export = st.session_state.get('metiers_selected', metiers)[0] if st.session_state.get('metiers_selected') else (metiers[0] if metiers else 'plombier')
        st.download_button(
            "üì• T√©l√©charger CSV complet",
            csv_all,
            f"{metier_export}_{dept}_complet_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            "text/csv"
        )
    
    with col_exp2:
        df_avec_site = df[df['site_web'].notna()]
        if len(df_avec_site) > 0:
            csv_avec = df_avec_site.to_csv(index=False, encoding='utf-8-sig')
            dept = st.session_state.get('departements_selected', departements)[0] if st.session_state.get('departements_selected') else (departements[0] if departements else '77')
            metier_export = st.session_state.get('metiers_selected', metiers)[0] if st.session_state.get('metiers_selected') else (metiers[0] if metiers else 'plombier')
            st.download_button(
                "üì• CSV avec site web",
                csv_avec,
                f"{metier_export}_{dept}_AVEC_site_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                "text/csv"
            )
    
    with col_exp3:
        df_sans_site = df[df['site_web'].isna()]
        if len(df_sans_site) > 0:
            csv_sans = df_sans_site.to_csv(index=False, encoding='utf-8-sig')
            dept = st.session_state.get('departements_selected', departements)[0] if st.session_state.get('departements_selected') else (departements[0] if departements else '77')
            metier_export = st.session_state.get('metiers_selected', metiers)[0] if st.session_state.get('metiers_selected') else (metiers[0] if metiers else 'plombier')
            st.download_button(
                "‚≠ê CSV SANS site web (PROSPECTS)",
                csv_sans,
                f"{metier_export}_{dept}_SANS_site_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                "text/csv"
            )
