"""
Page Scraping - Google Maps
Extraction des artisans depuis Google Maps
"""
import streamlit as st
import sys
import json
import time
from pathlib import Path
from datetime import datetime
import pandas as pd
import requests
from io import BytesIO
try:
    from openpyxl import Workbook
    from openpyxl.styles import Font, Alignment, PatternFill
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False
# ‚úÖ Plus besoin de threading/ThreadPoolExecutor - GitHub Actions uniquement

# Configuration de la page
st.set_page_config(page_title="Scraping Google Maps", page_icon="üîç", layout="wide")

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from whatsapp_database.queries import ajouter_artisan, get_statistiques
from whatsapp_database.models import init_database

# ‚úÖ Initialiser la base de donn√©es au d√©marrage de la page (ajoute les nouvelles colonnes si n√©cessaire)
try:
    init_database()
except Exception as e:
    # Ne pas bloquer si erreur, mais logger
    import logging
    logging.warning(f"Erreur initialisation BDD: {e}")

# ‚úÖ Import des fonctions de tracking (avec fallback si elles n'existent pas)
try:
    from whatsapp_database.queries import is_already_scraped, get_scraping_history, mark_scraping_done
except ImportError:
    # Si les fonctions n'existent pas encore, cr√©er des stubs
    def is_already_scraped(metier: str, departement: str, ville: str) -> bool:
        return False
    
    def get_scraping_history(metier: str = None, departement: str = None):
        return []
    
    def mark_scraping_done(metier: str, departement: str, ville: str, results_count: int = 0):
        pass
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Charger les villes par d√©partement
try:
    with open(Path(__file__).parent.parent.parent / "data" / "villes_par_departement.json", 'r', encoding='utf-8') as f:
        villes_par_dept = json.load(f)
except:
    villes_par_dept = {}

# ‚úÖ Fonction pour r√©cup√©rer les communes depuis data.gouv.fr
def get_communes_from_api(departement: str, min_population: int = 0, max_population: int = 50000):
    """R√©cup√®re les communes d'un d√©partement depuis l'API data.gouv.fr avec coordonn√©es GPS"""
    try:
        url = f"https://geo.api.gouv.fr/departements/{departement}/communes"
        params = {
            "fields": "nom,code,codesPostaux,population,centre",
            "format": "json"
        }
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            communes = response.json()
            # Filtrer par population
            filtered = []
            for c in communes:
                pop = c.get('population', 0)
                if min_population <= pop <= max_population:
                    centre = c.get('centre', {})
                    filtered.append({
                        'nom': c['nom'],
                        'code': c['code'],
                        'code_postal': c.get('codesPostaux', [c.get('code', '')])[0] if c.get('codesPostaux') else c.get('code', ''),
                        'population': pop,
                        'latitude': centre.get('coordinates', [None, None])[1] if centre else None,
                        'longitude': centre.get('coordinates', [None, None])[0] if centre else None
                    })
            # Trier par population (croissant)
            filtered.sort(key=lambda x: x['population'])
            return filtered
    except Exception as e:
        logger.error(f"Erreur API communes: {e}")
    return []

st.title("üîç Scraping Google Maps - Artisans")

# Stats actuelles
stats = get_statistiques()
col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
with col_stat1:
    st.metric("Total artisans", f"{stats.get('total', 0):,}")
with col_stat2:
    st.metric("Avec t√©l√©phone", f"{stats.get('avec_telephone', 0):,}")
with col_stat3:
    st.metric("Avec site web", f"{stats.get('avec_site_web', 0):,}")
with col_stat4:
    st.metric("Sans site web", f"{stats.get('sans_site_web', 0):,}")

st.markdown("---")

# Configuration du scraping
st.subheader("‚öôÔ∏è Configuration")

col_config1, col_config2 = st.columns(2)

with col_config1:
    # ‚úÖ Multi-select pour les m√©tiers
    metiers_options = ["plombier", "√©lectricien", "chauffagiste", "menuisier", "peintre", "ma√ßon", "couvreur", "carreleur"]
    metiers = st.multiselect(
        "Type(s) d'artisan(s)",
        options=metiers_options,
        default=["plombier"],
        help="S√©lectionnez un ou plusieurs types d'artisans √† rechercher"
    )
    if not metiers:
        st.warning("‚ö†Ô∏è Veuillez s√©lectionner au moins un m√©tier")
        metier = "plombier"
    else:
        metier = metiers[0]

with col_config2:
    # Liste des d√©partements fran√ßais
    departements_liste = [
        "01", "02", "03", "04", "05", "06", "07", "08", "09", "10",
        "11", "12", "13", "14", "15", "16", "17", "18", "19", "21",
        "22", "23", "24", "25", "26", "27", "28", "29", "2A", "2B",
        "30", "31", "32", "33", "34", "35", "36", "37", "38", "39",
        "40", "41", "42", "43", "44", "45", "46", "47", "48", "49",
        "50", "51", "52", "53", "54", "55", "56", "57", "58", "59",
        "60", "61", "62", "63", "64", "65", "66", "67", "68", "69",
        "70", "71", "72", "73", "74", "75", "76", "77", "78", "79",
        "80", "81", "82", "83", "84", "85", "86", "87", "88", "89",
        "90", "91", "92", "93", "94", "95"
    ]
    # ‚úÖ Multi-select pour les d√©partements
    departements = st.multiselect(
        "D√©partement(s)",
        options=departements_liste,
        default=["77"],
        help="S√©lectionnez un ou plusieurs d√©partements √† scraper"
    )
    if not departements:
        st.warning("‚ö†Ô∏è Veuillez s√©lectionner au moins un d√©partement")
        departement = "77"
    else:
        departement = departements[0]

col_config3, col_config4 = st.columns(2)

with col_config3:
    max_results = st.slider(
        "Nombre max de r√©sultats",
        min_value=10,
        max_value=200,
        value=50,
        step=10,
        help="Nombre maximum d'√©tablissements √† scraper par ville"
    )

with col_config4:
    headless = st.checkbox(
        "Mode headless (navigateur invisible)",
        value=True,
        help="Mode headless activ√© par d√©faut (plus rapide). D√©cochez pour voir le navigateur."
    )

# ‚úÖ Charger la configuration GitHub si elle existe
github_config_file = Path(__file__).parent.parent.parent / "config" / "github_config.json"
github_token_default = ""
github_repo_default = ""

try:
    if github_config_file.exists():
        with open(github_config_file, 'r', encoding='utf-8') as f:
            github_config = json.load(f)
            github_token_default = github_config.get('github_token', '')
            github_repo_default = github_config.get('github_repo', '')
except:
    pass

# ‚úÖ GitHub Actions est maintenant la SEULE option disponible
# Forcer GitHub Actions √† True
use_github_actions = True
st.session_state.use_github_actions = True

# ‚úÖ Utiliser les valeurs du fichier de config automatiquement (pas de champs visibles)
github_token = github_token_default
github_repo = github_repo_default

if not github_token or not github_repo:
    st.error("‚ö†Ô∏è Configuration GitHub manquante. V√©rifiez que config/github_config.json existe avec token et repo.")

# ‚úÖ Section : Gestion des workflows GitHub Actions (VISIBLE EN HAUT, D√àS LE D√âMARRAGE)
# ‚úÖ TOUJOURS AFFICH√âE - m√™me si pas de token (pour montrer qu'il faut configurer)
st.markdown("---")
st.subheader("‚öôÔ∏è Gestion des Workflows GitHub Actions")

# ‚úÖ FIX CRITIQUE : D√©finir les fonctions AVANT leur utilisation
def list_github_workflows(token, repo):
    """Liste tous les workflows GitHub Actions en cours"""
    try:
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        
        workflows = []
        
        # ‚úÖ FIX : Faire deux appels s√©par√©s car l'API ne supporte pas "in_progress,queued" dans un seul param√®tre
        # R√©cup√©rer les runs "in_progress"
        runs_url_in_progress = f"https://api.github.com/repos/{repo}/actions/runs?status=in_progress&per_page=100"
        response = requests.get(runs_url_in_progress, headers=headers)
        if response.status_code == 200:
            runs_data = response.json()
            for run in runs_data.get('workflow_runs', []):
                workflows.append({
                    'id': run.get('id'),
                    'run_number': run.get('run_number'),
                    'status': run.get('status'),
                    'conclusion': run.get('conclusion'),
                    'created_at': run.get('created_at'),
                    'updated_at': run.get('updated_at'),
                    'head_branch': run.get('head_branch'),
                    'workflow_id': run.get('workflow_id'),
                    'html_url': run.get('html_url')
                })
        
        # R√©cup√©rer les runs "queued"
        runs_url_queued = f"https://api.github.com/repos/{repo}/actions/runs?status=queued&per_page=100"
        response = requests.get(runs_url_queued, headers=headers)
        if response.status_code == 200:
            runs_data = response.json()
            for run in runs_data.get('workflow_runs', []):
                # √âviter les doublons
                if not any(w['id'] == run.get('id') for w in workflows):
                    workflows.append({
                        'id': run.get('id'),
                        'run_number': run.get('run_number'),
                        'status': run.get('status'),
                        'conclusion': run.get('conclusion'),
                        'created_at': run.get('created_at'),
                        'updated_at': run.get('updated_at'),
                        'head_branch': run.get('head_branch'),
                        'workflow_id': run.get('workflow_id'),
                        'html_url': run.get('html_url')
                    })
        
        # Trier par date de cr√©ation (plus r√©cent en premier)
        workflows.sort(key=lambda x: x.get('created_at', ''), reverse=True)
        
        return workflows
    except Exception as e:
        logger.error(f"Erreur liste workflows: {e}")
        return []

def cancel_github_workflow(token, repo, run_id):
    """Annule un workflow GitHub Actions sp√©cifique"""
    try:
        cancel_url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id}/cancel"
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        
        response = requests.post(cancel_url, headers=headers)
        return response.status_code == 202
    except Exception as e:
        logger.error(f"Erreur annulation workflow {run_id}: {e}")
        return False

def cancel_all_github_workflows(token, repo):
    """Annule tous les workflows GitHub Actions en cours (in_progress et queued)"""
    try:
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        
        cancelled_count = 0
        total_runs = 0
        
        # R√©cup√©rer les runs "in_progress"
        url_in_progress = f"https://api.github.com/repos/{repo}/actions/runs?status=in_progress&per_page=100"
        response = requests.get(url_in_progress, headers=headers)
        if response.status_code == 200:
            runs_data = response.json()
            for run in runs_data.get('workflow_runs', []):
                total_runs += 1
                if cancel_github_workflow(token, repo, run.get('id')):
                    cancelled_count += 1
        
        # R√©cup√©rer les runs "queued"
        url_queued = f"https://api.github.com/repos/{repo}/actions/runs?status=queued&per_page=100"
        response = requests.get(url_queued, headers=headers)
        if response.status_code == 200:
            runs_data = response.json()
            for run in runs_data.get('workflow_runs', []):
                total_runs += 1
                if cancel_github_workflow(token, repo, run.get('id')):
                    cancelled_count += 1
        
        if total_runs == 0:
            return True, "Aucun workflow en cours √† annuler"
        elif cancelled_count == total_runs:
            return True, f"‚úÖ {cancelled_count} workflow(s) annul√©(s) avec succ√®s"
        else:
            return False, f"‚ö†Ô∏è {cancelled_count}/{total_runs} workflow(s) annul√©(s)"
    except Exception as e:
        logger.error(f"Erreur annulation workflows: {e}")
        return False, f"Erreur: {str(e)}"

def download_github_artifact(token, repo, run_id):
    """T√©l√©charge l'artifact depuis GitHub Actions"""
    try:
        # R√©cup√©rer la liste des artifacts
        url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id}/artifacts"
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            artifacts = response.json().get('artifacts', [])
            for artifact in artifacts:
                if artifact.get('name') == 'scraping-results':
                    # T√©l√©charger l'artifact
                    download_url = artifact.get('archive_download_url')
                    if download_url:
                        download_response = requests.get(download_url, headers=headers)
                        if download_response.status_code == 200:
                            # Sauvegarder le zip
                            import zipfile
                            import io
                            data_dir = Path(__file__).parent.parent.parent / "data"
                            zip_path = data_dir / "github_artifact.zip"
                            with open(zip_path, 'wb') as f:
                                f.write(download_response.content)
                            
                            # Extraire le JSON
                            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                                zip_ref.extractall(data_dir)
                            
                            # Lire les fichiers
                            results_file = data_dir / "scraping_results_github_actions.json"
                            status_file = data_dir / "github_actions_status.json"
                            
                            result_data = None
                            status_data = None
                            
                            if results_file.exists():
                                with open(results_file, 'r', encoding='utf-8') as f:
                                    result_data = json.load(f)
                            
                            if status_file.exists():
                                with open(status_file, 'r', encoding='utf-8') as f:
                                    status_data = json.load(f)
                            
                            # Nettoyer
                            zip_path.unlink()
                            
                            # ‚úÖ Retourner dans le format attendu (compatibilit√©)
                            if result_data and isinstance(result_data, dict) and 'results' in result_data:
                                return result_data  # Format: {'results': [...], 'total_results': ...}
                            elif result_data and isinstance(result_data, list):
                                return {'results': result_data, 'total_results': len(result_data)}
                            else:
                                return {'results': [], 'total_results': 0, 'status': status_data}
            return None
        return None
    except Exception as e:
        logger.error(f"Erreur t√©l√©chargement artifact: {e}")
        return None

if github_token and github_repo:
    # ‚úÖ Bouton Rafra√Æchir TOUJOURS visible EN HAUT pour forcer la mise √† jour
    col_refresh_top1, col_refresh_top2 = st.columns([1, 1])
    with col_refresh_top1:
        if st.button("üîÑ Rafra√Æchir les workflows", key="refresh_workflows_top", help="Afficher tous les workflows et leurs statistiques"):
            # Forcer la mise √† jour en r√©initialisant le cache de session
            if 'workflows_last_refresh' in st.session_state:
                del st.session_state.workflows_last_refresh
            # Forcer le rerun imm√©diatement
            st.experimental_rerun()

    # Lister les workflows en cours
    try:
        workflows_en_cours = list_github_workflows(github_token, github_repo)
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration workflows: {e}")
        workflows_en_cours = []
    
    # ‚úÖ Affichage simplifi√© - pas de d√©tails individuels qui apparaissent/disparaissent
    if workflows_en_cours:
        st.info(f"üü¢ **{len(workflows_en_cours)} workflow(s) en cours** - Utilisez le bouton 'Rafra√Æchir les workflows' ci-dessus pour voir les d√©tails")
    
    # ‚úÖ Bouton supprim√© - d√©j√† pr√©sent dans la page Base de donn√©es
    
    # ‚úÖ Afficher les statistiques pour chaque workflow (m√™me termin√©s)
    # R√©cup√©rer TOUS les workflows (pas seulement in_progress/queued)
    try:
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {github_token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        # ‚úÖ R√©cup√©rer les 4 derniers workflows (tous statuts)
        all_workflows_url = f"https://api.github.com/repos/{github_repo}/actions/runs?per_page=4"
        response = requests.get(all_workflows_url, headers=headers)
        all_workflows = []
        if response.status_code == 200:
            runs_data = response.json()
            for run in runs_data.get('workflow_runs', []):
                all_workflows.append({
                    'id': run.get('id'),
                    'run_number': run.get('run_number'),
                    'status': run.get('status'),
                    'conclusion': run.get('conclusion'),
                    'created_at': run.get('created_at'),
                    'html_url': run.get('html_url')
                })
            all_workflows.sort(key=lambda x: x.get('created_at', ''), reverse=True)
    except:
        all_workflows = workflows_en_cours
    
    if all_workflows:
        st.markdown("### üìä Statistiques par workflow")
        from whatsapp_database.queries import get_artisans
        
        # ‚úÖ Afficher en grille 2 colonnes (4 workflows max)
        workflows_to_show = all_workflows[:4]  # Limiter aux 4 derniers
        
        # Cr√©er des paires de workflows pour affichage en 2 colonnes
        for i in range(0, len(workflows_to_show), 2):
            cols = st.columns(2)
            for j, col in enumerate(cols):
                if i + j < len(workflows_to_show):
                    workflow = workflows_to_show[i + j]
                    status_emoji = "üü¢" if workflow['status'] == 'in_progress' else "üü°" if workflow['status'] == 'queued' else "üîµ"
                    conclusion_emoji = "‚úÖ" if workflow.get('conclusion') == 'success' else "‚ùå" if workflow.get('conclusion') == 'failure' else "‚èπÔ∏è" if workflow.get('conclusion') == 'cancelled' else ""
                    
                    with col:
                        with st.expander(f"{status_emoji} {conclusion_emoji} Workflow #{workflow['run_number']} - {workflow['status']} ({workflow['created_at'][:19].replace('T', ' ')})"):
                            # R√©cup√©rer les artisans scrap√©s depuis le d√©but de ce workflow
                            workflow_start = workflow['created_at']
                            
                            try:
                                # R√©cup√©rer tous les artisans
                                all_artisans = get_artisans(limit=10000)
                                
                                # ‚úÖ Normaliser le format de workflow_start (GitHub API format: "2025-11-28T16:56:17Z")
                                # Convertir en format comparable (sans Z, avec espace au lieu de T)
                                workflow_start_normalized = workflow_start.replace('T', ' ').replace('Z', '').split('.')[0]
                                
                                # Filtrer ceux cr√©√©s apr√®s le d√©but du workflow
                                workflow_artisans = []
                                for a in all_artisans:
                                    created_at = a.get('created_at')
                                    if created_at:
                                        # Normaliser created_at (peut √™tre ISO ou SQLite format)
                                        created_at_normalized = str(created_at).replace('T', ' ').replace('Z', '').split('.')[0]
                                        # Comparaison de cha√Ænes ISO normalis√©es (format: "YYYY-MM-DD HH:MM:SS")
                                        if created_at_normalized >= workflow_start_normalized:
                                            workflow_artisans.append(a)
                                
                                # ‚úÖ Debug: Afficher le nombre total d'artisans et ceux filtr√©s
                                if len(all_artisans) > 0 and len(workflow_artisans) == 0:
                                    # Si on a des artisans mais aucun ne correspond, c'est peut-√™tre un probl√®me de format
                                    # Afficher les 3 derniers artisans pour debug
                                    st.caption(f"üîç Debug: {len(all_artisans)} artisans totaux, workflow_start: {workflow_start_normalized}")
                                
                                if workflow_artisans:
                                    total = len(workflow_artisans)
                                    avec_tel = len([a for a in workflow_artisans if a.get('telephone')])
                                    avec_site = len([a for a in workflow_artisans if a.get('site_web')])
                                    sans_site = total - avec_site
                                    
                                    # ‚úÖ Stats simples sans colonnes imbriqu√©es
                                    st.markdown(f"**üìä Scrap√©s:** {total} | **üìû Avec t√©l√©phone:** {avec_tel} | **üåê Avec site web:** {avec_site} | **‚≠ê SANS site:** {sans_site}")
                                else:
                                    st.info("‚è≥ Aucun r√©sultat encore pour ce workflow")
                            except Exception as e:
                                st.error(f"Erreur calcul stats: {e}")
else:
    st.warning("‚ö†Ô∏è Configuration GitHub manquante. La gestion des workflows n√©cessite un token et un repository configur√©s.")

st.markdown("---")

# ‚úÖ SECTION : Carte de scraping par m√©tier (bas√©e sur la BDD locale)
st.markdown("### üó∫Ô∏è Carte des scrapings par m√©tier")
st.caption("Visualisez les d√©partements scrap√©s pour chaque m√©tier. La taille des points est proportionnelle au nombre d'artisans.")

# R√©cup√©rer la liste des m√©tiers depuis la BDD
from whatsapp_database.queries import get_artisans
all_artisans = get_artisans(limit=10000)
metiers_bdd = sorted(list(set([a.get('type_artisan') for a in all_artisans if a.get('type_artisan')])))

col_map1, col_map2 = st.columns([1, 3])
with col_map1:
    if metiers_bdd:
        metier_carte = st.selectbox("S√©lectionner un m√©tier", ["Tous"] + metiers_bdd, key="map_metier_selection")
    else:
        metier_carte = "Tous"
        st.info("‚ÑπÔ∏è Aucun m√©tier dans la BDD")

# Importer et utiliser la fonction de carte
try:
    from whatsapp_app.utils.map_utils import create_scraping_map_by_job
    from streamlit_folium import st_folium
    
    carte = create_scraping_map_by_job(metier_carte if metier_carte != "Tous" else None)
    
    if carte:
        with col_map2:
            st_folium(carte, width=None, height=500, returned_objects=[])
    else:
        with col_map2:
            st.info("‚ÑπÔ∏è Aucune donn√©e de scraping pour ce m√©tier. Lancez un scraping pour voir la carte se remplir.")
except ImportError as e:
    st.warning(f"‚ö†Ô∏è Erreur import map_utils: {e}")
    st.info("üí° Installez les d√©pendances: `pip install folium streamlit-folium`")
except Exception as e:
    st.error(f"‚ùå Erreur cr√©ation carte: {e}")
    import traceback
    st.code(traceback.format_exc())

# ‚úÖ SECTION : Tableau de bord strat√©gique de recherche
try:
    from whatsapp_database.scraping_analytics import (
        get_metier_statistics, get_departement_statistics,
        get_coverage_metrics, get_session_statistics,
        get_priority_suggestions, generate_research_report
    )
    ANALYTICS_AVAILABLE = True
except ImportError as e:
    ANALYTICS_AVAILABLE = False
    # Debug: afficher l'erreur en mode d√©veloppement
    import traceback
    # st.warning(f"Module analytics non disponible: {e}")  # Comment√© pour ne pas polluer l'UI

# ‚úÖ SECTION : Historique des scrapings par d√©partement (masqu√© si vide)
from whatsapp_database.queries import get_scraping_history
historique = get_scraping_history()

# ‚úÖ Initialiser departements_stats pour √©viter l'erreur
departements_stats = {}

if historique:
    col_title, col_filter = st.columns([2, 1])
    with col_title:
        st.markdown("### üìä Historique des scrapings par d√©partement")
    with col_filter:
        # S√©lection du m√©tier pour filtrer
        metiers_disponibles = list(set([h['metier'] for h in historique if h.get('metier')]))
        if metiers_disponibles:
            metier_selectionne = st.selectbox("M√©tier", ["Tous"] + sorted(metiers_disponibles), key="tracking_metier", label_visibility="visible")
        else:
            metier_selectionne = "Tous"
    
    # Filtrer par m√©tier si sp√©cifi√©
    if metier_selectionne != "Tous":
        historique = [h for h in historique if h.get('metier') == metier_selectionne]
    
    # Calculer les statistiques par d√©partement
    for entry in historique:
        dept = entry.get('departement', '')
        metier = entry.get('metier', '')
        results_count = entry.get('results_count', 0)
        
        if not dept:
            continue
        
        key = f"{metier}_{dept}"
        if key not in departements_stats:
            departements_stats[key] = {
                'departement': dept,
                'metier': metier,
                'total_results': 0,
                'villes_scrapees': 0,
                'max_results': 0
            }
        
        departements_stats[key]['total_results'] += results_count
        departements_stats[key]['villes_scrapees'] += 1
        departements_stats[key]['max_results'] = max(departements_stats[key]['max_results'], results_count)

# Coordonn√©es approximatives des d√©partements fran√ßais (centres)
dept_coords = {
    '01': (46.2, 5.2), '02': (49.4, 3.4), '03': (46.3, 3.1), '04': (44.1, 6.1), '05': (44.7, 6.1),
    '06': (43.7, 7.3), '07': (44.5, 4.4), '08': (49.8, 4.7), '09': (43.0, 1.6), '10': (48.3, 4.1),
    '11': (43.2, 2.4), '12': (44.3, 2.6), '13': (43.3, 5.4), '14': (49.2, -0.4), '15': (45.0, 2.4),
    '16': (45.6, 0.2), '17': (46.2, -1.2), '18': (47.1, 2.4), '19': (45.3, 1.8), '21': (47.3, 5.0),
    '22': (48.5, -2.8), '23': (46.2, 1.9), '24': (45.2, 0.7), '25': (47.2, 6.0), '26': (44.9, 4.9),
    '27': (49.1, 1.1), '28': (48.4, 1.5), '29': (48.4, -4.5), '30': (44.1, 4.1), '31': (43.6, 1.4),
    '32': (43.6, 0.6), '33': (44.8, -0.6), '34': (43.6, 3.9), '35': (48.1, -1.7), '36': (46.8, 1.7),
    '37': (47.4, 0.7), '38': (45.2, 5.7), '39': (46.7, 5.6), '40': (43.9, -0.5), '41': (47.6, 1.3),
    '42': (45.4, 4.4), '43': (45.0, 3.9), '44': (47.2, -1.6), '45': (47.9, 1.9), '46': (44.4, 1.4),
    '47': (44.2, 0.6), '48': (44.5, 3.5), '49': (47.5, -0.6), '50': (49.1, -1.1), '51': (49.3, 4.0),
    '52': (48.1, 5.1), '53': (48.1, -0.8), '54': (48.7, 6.2), '55': (49.1, 5.4), '56': (47.7, -2.8),
    '57': (49.1, 6.2), '58': (47.0, 3.4), '59': (50.6, 3.1), '60': (49.4, 2.8), '61': (48.4, 0.1),
    '62': (50.3, 2.8), '63': (45.8, 3.1), '64': (43.3, -0.4), '65': (43.2, 0.1), '66': (42.7, 2.9),
    '67': (48.6, 7.8), '68': (47.7, 7.3), '69': (45.8, 4.8), '70': (47.6, 6.2), '71': (46.8, 4.9),
    '72': (48.0, 0.2), '73': (45.6, 5.9), '74': (46.0, 6.1), '75': (48.9, 2.3), '76': (49.4, 1.1),
    '77': (48.6, 2.7), '78': (48.8, 2.1), '79': (46.3, -0.5), '80': (49.9, 2.3), '81': (43.6, 2.1),
    '82': (44.0, 1.4), '83': (43.1, 6.0), '84': (44.0, 5.0), '85': (46.7, -1.4), '86': (46.6, 0.3),
    '87': (45.8, 1.3), '88': (48.2, 6.5), '89': (47.8, 3.6), '90': (47.6, 6.9), '91': (48.6, 2.3),
    '92': (48.9, 2.2), '93': (48.9, 2.4), '94': (48.8, 2.4), '95': (49.1, 2.3)
}

# Cr√©er la carte
if departements_stats:
    import folium
    from streamlit_folium import st_folium
    
    # Centre de la France
    m = folium.Map(location=[46.6, 2.2], zoom_start=6)
    
    # Seuils pour d√©terminer le statut
    SEUIL_FAIT = 5  # Au moins 5 r√©sultats = "fait"
    SEUIL_PARTIEL = 1  # Entre 1 et 4 = "partiellement fait"
    
    for key, stats in departements_stats.items():
        dept = stats['departement']
        metier = stats['metier']
        total_results = stats['total_results']
        villes_scrapees = stats['villes_scrapees']
        max_results = stats['max_results']
        
        if dept in dept_coords:
            lat, lon = dept_coords[dept]
            
            # D√©terminer le statut et la couleur
            if max_results >= SEUIL_FAIT:
                couleur = 'green'
                statut = '‚úÖ Fait'
                taille = 15
            elif max_results >= SEUIL_PARTIEL:
                couleur = 'orange'
                statut = '‚ö†Ô∏è Partiellement fait'
                taille = 10
            else:
                couleur = 'gray'
                statut = '‚ùå Non fait'
                taille = 5
            
            # Popup avec informations
            popup_text = f"""
            <b>D√©partement {dept}</b><br>
            <b>M√©tier:</b> {metier}<br>
            <b>Statut:</b> {statut}<br>
            <b>Villes scrap√©es:</b> {villes_scrapees}<br>
            <b>Total r√©sultats:</b> {total_results}<br>
            <b>Max par ville:</b> {max_results}
            """
            
            folium.CircleMarker(
                location=[lat, lon],
                radius=taille,
                popup=folium.Popup(popup_text, max_width=300),
                color='black',
                weight=2,
                fillColor=couleur,
                fillOpacity=0.8,
                tooltip=f"D√©pt {dept}: {statut} ({villes_scrapees} villes, {total_results} r√©sultats)"
            ).add_to(m)
    
    # ‚úÖ AUSSI afficher les villes individuelles scrap√©es (petits points bleus)
    # R√©cup√©rer les coordonn√©es des villes depuis l'API ou depuis les artisans scrap√©s
    from whatsapp_database.queries import get_artisans
    artisans = get_artisans(limit=10000)
    
    # Grouper les villes scrap√©es par m√©tier/d√©partement
    villes_scrapees_coords = {}
    for artisan in artisans:
        if metier_selectionne != "Tous" and artisan.get('type_artisan') != metier_selectionne:
            continue
        dept_art = artisan.get('departement', '')
        ville_art = artisan.get('ville', '')
        if dept_art and ville_art:
            key = f"{dept_art}_{ville_art}"
            if key not in villes_scrapees_coords:
                # Essayer de r√©cup√©rer les coordonn√©es depuis l'API
                try:
                    communes = get_communes_from_api(dept_art, 0, 1000000)  # Toutes les communes
                    for commune in communes:
                        if commune['nom'].lower() == ville_art.lower():
                            villes_scrapees_coords[key] = {
                                'lat': commune.get('latitude'),
                                'lon': commune.get('longitude'),
                                'ville': ville_art,
                                'dept': dept_art
                            }
                            break
                except:
                    pass
    
    # Ajouter les villes sur la carte (petits points bleus)
    for key, coords in villes_scrapees_coords.items():
        if coords.get('lat') and coords.get('lon'):
            folium.CircleMarker(
                location=[coords['lat'], coords['lon']],
                radius=3,
                popup=folium.Popup(f"<b>{coords['ville']}</b><br>D√©partement: {coords['dept']}", max_width=200),
                color='blue',
                weight=1,
                fillColor='blue',
                fillOpacity=0.6,
                tooltip=f"{coords['ville']} ({coords['dept']})"
            ).add_to(m)
    
    # L√©gende
    legend_html = '''
    <div style="position: fixed; 
                bottom: 50px; left: 50px; width: 200px; height: 120px; 
                background-color: white; z-index:9999; font-size:14px;
                border:2px solid grey; border-radius:5px; padding: 10px">
    <b>L√©gende</b><br>
    <span style="color:green">‚óè</span> Fait (‚â•5 r√©sultats)<br>
    <span style="color:orange">‚óè</span> Partiel (1-4 r√©sultats)<br>
    <span style="color:gray">‚óè</span> Non fait (0 r√©sultat)<br>
    <span style="color:blue">‚óè</span> Villes scrap√©es
    </div>
    '''
    m.get_root().html.add_child(folium.Element(legend_html))
    
    # ‚úÖ Afficher la carte EN GRAND
    st_folium(m, width=None, height=600, returned_objects=[])
    
    # Statistiques r√©sum√©es
    col_stat1, col_stat2, col_stat3 = st.columns(3)
    with col_stat1:
        total_depts = len(departements_stats)
        st.metric("D√©partements scrap√©s", total_depts)
    with col_stat2:
        depts_faits = len([s for s in departements_stats.values() if s['max_results'] >= SEUIL_FAIT])
        st.metric("D√©partements complets", depts_faits)
    with col_stat3:
        total_villes = sum([s['villes_scrapees'] for s in departements_stats.values()])
        st.metric("Villes scrap√©es", total_villes)
else:
    st.empty()  # Pas de message si aucun scraping - affichage plus compact

# ‚úÖ NOUVELLE SECTION : Tableau de bord strat√©gique de recherche
st.markdown("---")
st.markdown("### üìä Tableau de bord strat√©gique de recherche")

if not ANALYTICS_AVAILABLE:
    st.warning("‚ö†Ô∏è Le module d'analytics n'est pas disponible. V√©rifiez que le fichier `whatsapp_database/scraping_analytics.py` existe.")
    st.stop()

if not historique or len(historique) == 0:
    st.info("‚ÑπÔ∏è Aucun historique de scraping trouv√© dans la base de donn√©es. Les statistiques appara√Ætront apr√®s vos premiers scrapings qui seront enregistr√©s dans `scraping_history`.")
    st.caption("üí° Note: Les scrapings effectu√©s via GitHub Actions doivent √™tre marqu√©s dans la table `scraping_history` pour appara√Ætre ici.")
    
    # Bouton pour forcer la r√©initialisation de la BDD (utile si les colonnes manquent)
    if st.button("üîÑ R√©initialiser la structure de la base de donn√©es", help="Ajoute les nouvelles colonnes √† la table scraping_history si elles manquent"):
        try:
            from whatsapp_database.models import init_database
            init_database()
            st.success("‚úÖ Base de donn√©es r√©initialis√©e ! Rafra√Æchissez la page.")
            try:
                st.rerun()
            except:
                st.experimental_rerun()
        except Exception as e:
            st.error(f"‚ùå Erreur: {e}")
else:
    # Tabs pour organiser les diff√©rentes vues
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "üéØ Vue d'ensemble", 
        "üìà Par m√©tier", 
        "üó∫Ô∏è Par d√©partement",
        "üìÖ Sessions r√©centes",
        "üí° Suggestions"
    ])
    
    with tab1:
        st.markdown("#### Vue d'ensemble globale")
        
        # Statistiques globales
        try:
            stats_metiers = get_metier_statistics()
            stats_departements = get_departement_statistics()
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("M√©tiers scrap√©s", len(stats_metiers))
            with col2:
                st.metric("D√©partements couverts", len(stats_departements))
            with col3:
                total_artisans = sum(s['artisans_trouves'] for s in stats_metiers)
                st.metric("Artisans trouv√©s", f"{total_artisans:,}")
            with col4:
                total_villes = sum(s['villes_scrapees'] for s in stats_metiers)
                st.metric("Villes scrap√©es", f"{total_villes:,}")
            
            # Graphique des m√©tiers les plus scrap√©s
            if stats_metiers:
                st.markdown("#### Top m√©tiers par nombre d'artisans trouv√©s")
                df_metiers = pd.DataFrame(stats_metiers[:10])
                st.bar_chart(df_metiers.set_index('metier')['artisans_trouves'])
                
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erreur calcul statistiques: {e}")
    
    with tab2:
        st.markdown("#### Statistiques par m√©tier")
        
        try:
            stats_metiers = get_metier_statistics()
            
            if stats_metiers:
                # Tableau d√©taill√©
                df = pd.DataFrame(stats_metiers)
                df['taux_couverture_moyen'] = df['taux_couverture_moyen'].apply(
                    lambda x: f"{x:.1f}%" if x is not None else "N/A"
                )
                
                st.dataframe(
                    df[['metier', 'departements_couverts', 'villes_scrapees', 
                        'artisans_trouves', 'taux_couverture_moyen']],
                    use_container_width=True,
                    hide_index=True
                )
                
                # Export
                csv = df.to_csv(index=False).encode('utf-8')
                st.download_button(
                    "üì• T√©l√©charger en CSV",
                    csv,
                    f"stats_metiers_{datetime.now().strftime('%Y%m%d')}.csv",
                    "text/csv"
                )
            else:
                st.info("Aucune statistique disponible")
                
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erreur: {e}")
    
    with tab3:
        st.markdown("#### Statistiques par d√©partement")
        
        try:
            stats_departements = get_departement_statistics()
            
            if stats_departements:
                # Filtre par m√©tier
                metiers_list = sorted(list(set([h.get('metier', '') for h in historique if h.get('metier')])))
                if metiers_list:
                    metier_filter = st.selectbox(
                        "Filtrer par m√©tier",
                        ["Tous"] + metiers_list,
                        key="dept_metier_filter"
                    )
                    
                    if metier_filter != "Tous":
                        stats_departements = [s for s in stats_departements 
                                            if any(h.get('metier') == metier_filter 
                                                  for h in historique if h.get('departement') == s['departement'])]
                
                # Tableau
                df = pd.DataFrame(stats_departements)
                df['taux_couverture'] = df['taux_couverture'].apply(
                    lambda x: f"{x:.1f}%" if x is not None else "N/A"
                )
                
                st.dataframe(
                    df[['departement', 'metiers_scrapes', 'villes_scrapees', 
                        'villes_disponibles', 'taux_couverture', 'artisans_trouves']],
                    use_container_width=True,
                    hide_index=True
                )
                
                # Export
                csv = df.to_csv(index=False).encode('utf-8')
                st.download_button(
                    "üì• T√©l√©charger en CSV",
                    csv,
                    f"stats_departements_{datetime.now().strftime('%Y%m%d')}.csv",
                    "text/csv"
                )
            else:
                st.info("Aucune statistique disponible")
                
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erreur: {e}")
    
    with tab4:
        st.markdown("#### Sessions de scraping r√©centes (30 derniers jours)")
        
        try:
            sessions = get_session_statistics(days=30)
            
            if sessions:
                df_sessions = pd.DataFrame(sessions)
                df_sessions['duree_moyenne'] = df_sessions['duree_moyenne'].apply(
                    lambda x: f"{x:.0f}s" if x and not pd.isna(x) else "N/A"
                )
                
                st.dataframe(
                    df_sessions,
                    use_container_width=True,
                    hide_index=True
                )
                
                # Graphique d'√©volution
                if len(df_sessions) > 1:
                    st.markdown("#### √âvolution du nombre d'artisans trouv√©s")
                    st.line_chart(df_sessions.set_index('date')['artisans_trouves'])
            else:
                st.info("Aucune session r√©cente")
                
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erreur: {e}")
    
    with tab5:
        st.markdown("#### Suggestions de d√©partements prioritaires")
        st.caption("D√©partements partiellement couverts n√©cessitant un compl√©ment de scraping")
        
        try:
            suggestions = get_priority_suggestions(limit=10)
            
            if suggestions:
                df_suggestions = pd.DataFrame(suggestions)
                df_suggestions['taux_couverture_actuel'] = df_suggestions['taux_couverture_actuel'].apply(
                    lambda x: f"{x:.1f}%" if x is not None else "N/A"
                )
                df_suggestions['priorite_score'] = df_suggestions['priorite_score'].apply(
                    lambda x: f"{x:.0f}"
                )
                
                st.dataframe(
                    df_suggestions[['departement', 'metier', 'taux_couverture_actuel', 
                                   'villes_manquantes', 'raison', 'priorite_score']],
                    use_container_width=True,
                    hide_index=True
                )
            else:
                st.success("‚úÖ Tous les d√©partements sont bien couverts !")
                
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erreur: {e}")
    
    # Section Rapport complet
    st.markdown("---")
    st.markdown("#### üìÑ G√©n√©rer un rapport complet")
    
    col_report1, col_report2 = st.columns(2)
    with col_report1:
        metier_report = st.selectbox(
            "M√©tier (optionnel)",
            ["Tous"] + sorted(list(set([h.get('metier', '') for h in historique if h.get('metier')]))),
            key="report_metier"
        )
    with col_report2:
        dept_report = st.selectbox(
            "D√©partement (optionnel)",
            ["Tous"] + sorted(list(set([h.get('departement', '') for h in historique if h.get('departement')]))),
            key="report_dept"
        )
    
    if st.button("üìä G√©n√©rer rapport", key="generate_report"):
        try:
            with st.spinner("G√©n√©ration du rapport..."):
                report = generate_research_report(
                    metier=metier_report if metier_report != "Tous" else None,
                    departement=dept_report if dept_report != "Tous" else None
                )
                
                st.success("‚úÖ Rapport g√©n√©r√© avec succ√®s !")
                
                # Afficher le r√©sum√©
                st.markdown("##### R√©sum√©")
                stats_gen = report['statistiques_generales']
                col_r1, col_r2, col_r3, col_r4 = st.columns(4)
                with col_r1:
                    st.metric("Scrapings", stats_gen['total_scrapings'])
                with col_r2:
                    st.metric("Artisans", f"{stats_gen['total_artisans_trouves']:,}")
                with col_r3:
                    st.metric("Villes", stats_gen['villes_scrapees'])
                with col_r4:
                    st.metric("D√©partements", stats_gen['departements_couverts'])
                
                # Exports
                col_exp1, col_exp2 = st.columns(2)
                
                with col_exp1:
                    # Export JSON
                    import json
                    report_json = json.dumps(report, ensure_ascii=False, indent=2)
                    st.download_button(
                        "üì• T√©l√©charger rapport JSON",
                        report_json.encode('utf-8'),
                        f"rapport_recherche_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        "application/json"
                    )
                
                with col_exp2:
                    # Export Excel si disponible
                    if EXCEL_AVAILABLE:
                        try:
                            wb = Workbook()
                            ws = wb.active
                            ws.title = "R√©sum√©"
                            
                            # Style header
                            header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                            header_font = Font(bold=True, color="FFFFFF")
                            
                            # Feuille 1: R√©sum√©
                            ws.append(["Rapport de recherche"])
                            ws.append(["G√©n√©r√© le", report['periode']['generated_at']])
                            ws.append([])
                            
                            stats = report['statistiques_generales']
                            ws.append(["Statistiques g√©n√©rales"])
                            ws.append(["Total scrapings", stats['total_scrapings']])
                            ws.append(["Artisans trouv√©s", stats['total_artisans_trouves']])
                            ws.append(["Villes scrap√©es", stats['villes_scrapees']])
                            ws.append(["D√©partements couverts", stats['departements_couverts']])
                            ws.append(["M√©tiers scrap√©s", stats['metiers_scrapes']])
                            ws.append(["Moyenne artisans/scraping", f"{stats['artisans_moyen_par_scraping']:.2f}"])
                            
                            # Feuille 2: Par m√©tier
                            if report['statistiques_par_metier']:
                                ws2 = wb.create_sheet("Par m√©tier")
                                df_metiers = pd.DataFrame(report['statistiques_par_metier'])
                                for r_idx, row in enumerate(df_metiers.itertuples(), start=1):
                                    for c_idx, value in enumerate(row[1:], start=1):
                                        cell = ws2.cell(row=r_idx, column=c_idx, value=value)
                                        if r_idx == 1:
                                            cell.fill = header_fill
                                            cell.font = header_font
                            
                            # Feuille 3: Par d√©partement
                            if report['statistiques_par_departement']:
                                ws3 = wb.create_sheet("Par d√©partement")
                                df_depts = pd.DataFrame(report['statistiques_par_departement'])
                                for r_idx, row in enumerate(df_depts.itertuples(), start=1):
                                    for c_idx, value in enumerate(row[1:], start=1):
                                        cell = ws3.cell(row=r_idx, column=c_idx, value=value)
                                        if r_idx == 1:
                                            cell.fill = header_fill
                                            cell.font = header_font
                            
                            # Sauvegarder en m√©moire
                            excel_buffer = BytesIO()
                            wb.save(excel_buffer)
                            excel_buffer.seek(0)
                            
                            st.download_button(
                                "üìä T√©l√©charger rapport Excel",
                                excel_buffer.getvalue(),
                                f"rapport_recherche_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )
                        except Exception as e:
                            st.warning(f"‚ö†Ô∏è Export Excel non disponible: {e}")
                    else:
                        st.info("üí° Installez openpyxl pour l'export Excel: `pip install openpyxl`")
                
        except Exception as e:
            st.error(f"‚ùå Erreur g√©n√©ration rapport: {e}")
            import traceback
            st.code(traceback.format_exc())

# ‚úÖ Initialiser les variables GitHub Actions dans session_state AVANT de les utiliser
if 'github_workflow_id' not in st.session_state:
    st.session_state.github_workflow_id = None
if 'github_workflow_status' not in st.session_state:
    st.session_state.github_workflow_status = None
if 'github_workflow_conclusion' not in st.session_state:
    st.session_state.github_workflow_conclusion = None
# Initialiser aussi scraping_running si n√©cessaire (pour la v√©rification ci-dessous)
if 'scraping_running' not in st.session_state:
    st.session_state.scraping_running = False

# ‚úÖ CRITIQUE : V√©rifier si on a un workflow GitHub Actions actif au d√©marrage
# Si oui, maintenir scraping_running = True pour garder le dashboard visible
# Note: get_github_workflow_status est d√©fini plus tard, donc on ne peut pas l'appeler ici
# Cette v√©rification sera faite dans la section du dashboard GitHub Actions

# ‚úÖ Options avanc√©es
with st.expander("‚öôÔ∏è Options avanc√©es"):
    col_adv1, col_adv2 = st.columns(2)
    with col_adv1:
        # ‚úÖ API activ√©e par d√©faut avec filtre <100k habitants
        use_api_communes = st.checkbox(
            "Utiliser API data.gouv.fr pour les communes",
            value=True,  # ‚úÖ Activ√© par d√©faut
            help="‚úÖ RECOMMAND√â : R√©cup√®re automatiquement toutes les communes depuis l'API officielle avec coordonn√©es GPS. Si d√©sactiv√©, utilise le fichier data/villes_par_departement.json (liste limit√©e)"
        )
        if use_api_communes:
            # ‚úÖ Valeurs par d√©faut : 0 √† 100k habitants
            min_pop = st.number_input("Population minimum", min_value=0, value=0, step=100, help="Villes avec au moins cette population")
            max_pop = st.number_input("Population maximum", min_value=0, value=100000, step=1000, help="Villes avec au maximum cette population (d√©faut: 100k)")
            
            # ‚úÖ Bouton pour afficher les communes trouv√©es (uniquement pour l'affichage, pas n√©cessaire pour le scraping)
            if st.button("üìã Afficher les communes trouv√©es", help="Affiche la liste des communes qui seront scrap√©es. Le scraping fonctionne m√™me sans cliquer sur ce bouton."):
                st.session_state.show_communes = True
    with col_adv2:
        enable_resume = st.checkbox(
            "Activer resume/checkpoint",
            value=True,
            disabled=use_github_actions,  # D√©sactiver pour GitHub Actions
            help="Permet de reprendre le scraping o√π il s'est arr√™t√© (non disponible avec GitHub Actions)"
        )
        num_threads = st.slider(
            "Nombre de threads",
            min_value=1,
            max_value=20,
            value=10,  # ‚úÖ D√©faut √† 10
            help="Nombre de navigateurs en parall√®le (attention: plus de threads = plus rapide mais plus de ressources)"
        )

# ‚úÖ Afficher les communes si demand√©
if st.session_state.get('show_communes', False) and use_api_communes and departements:
    st.markdown("---")
    st.subheader("üìç Communes trouv√©es via API data.gouv.fr")
    
    communes_trouvees = {}
    with st.spinner("üîÑ R√©cup√©ration des communes depuis l'API..."):
        for dept in departements:
            communes = get_communes_from_api(dept, min_pop if use_api_communes else 0, max_pop if use_api_communes else 50000)
            communes_trouvees[dept] = communes
    
    # Afficher un tableau avec toutes les communes
    all_communes = []
    for dept, communes in communes_trouvees.items():
        for commune in communes:
            all_communes.append({
                'D√©partement': dept,
                'Commune': commune['nom'],
                'Code postal': commune['code_postal'],
                'Population': commune['population'] if commune['population'] > 0 else None
            })
    
    if all_communes:
        st.info(f"üìä Total: {len(all_communes)} communes trouv√©es")
        
        # ‚úÖ Mise en page c√¥te √† c√¥te : tableau et carte
        col_table, col_map = st.columns([1, 1])
        
        with col_table:
            st.subheader("üìã Liste des communes")
            df_communes = pd.DataFrame(all_communes)
            # ‚úÖ Convertir la colonne 'Population' en num√©rique pour le tri
            if 'Population' in df_communes.columns:
                df_communes['Population'] = pd.to_numeric(df_communes['Population'], errors='coerce').fillna(0).astype(int)
                # Cr√©er une colonne format√©e pour l'affichage avec espaces (ex: 56 659)
                # On garde la colonne num√©rique pour le tri, et on cr√©e une colonne format√©e pour l'affichage
                df_communes['Population (format√©e)'] = df_communes['Population'].apply(
                    lambda x: f"{x:,}".replace(',', ' ') if x > 0 else "N/A"
                )
                # Pour le tri num√©rique, on garde la colonne Population comme nombre
                # Pour l'affichage, on utilise la version format√©e
                df_communes_display = df_communes[['D√©partement', 'Commune', 'Code postal', 'Population']].copy()
                # On remplace la colonne Population par la version format√©e pour l'affichage
                df_communes_display['Population'] = df_communes['Population (format√©e)']
            else:
                df_communes_display = df_communes
            
            # CSS pour autofit toutes les colonnes et √©viter la colonne vide
            st.markdown("""
            <style>
            /* Cibler le conteneur du DataFrame */
            div[data-testid="stDataFrame"] {
                width: 100% !important;
            }
            div[data-testid="stDataFrame"] > div {
                width: 100% !important;
                overflow-x: visible !important;
            }
            /* Tableau avec ajustement automatique - largeur 100% mais colonnes auto */
            div[data-testid="stDataFrame"] table {
                width: 100% !important;
                table-layout: auto !important;
                border-collapse: collapse !important;
            }
            /* Colonnes avec ajustement automatique selon le contenu */
            div[data-testid="stDataFrame"] th,
            div[data-testid="stDataFrame"] td {
                white-space: nowrap !important;
                padding: 8px 12px !important;
                width: auto !important;
                max-width: none !important;
            }
            /* Masquer compl√®tement la derni√®re colonne si elle est vide */
            div[data-testid="stDataFrame"] table thead tr th:last-child:empty,
            div[data-testid="stDataFrame"] table tbody tr td:last-child:empty {
                display: none !important;
                width: 0 !important;
                padding: 0 !important;
                border: none !important;
            }
            </style>
            """, unsafe_allow_html=True)
            
            # Utiliser dataframe avec formatage de la population (espaces pour s√©parer les milliers)
            st.dataframe(df_communes_display, height=600)
            
            # JavaScript pour masquer la colonne vide apr√®s le rendu
            st.markdown("""
            <script>
            function hideEmptyColumn() {
                const tables = document.querySelectorAll('div[data-testid="stDataFrame"] table');
                tables.forEach(function(table) {
                    const rows = table.querySelectorAll('tr');
                    if (rows.length > 0) {
                        // V√©rifier si la derni√®re colonne est vide dans toutes les lignes
                        let allEmpty = true;
                        rows.forEach(function(row) {
                            const lastCell = row.querySelector('th:last-child, td:last-child');
                            if (lastCell && lastCell.textContent && lastCell.textContent.trim() !== '') {
                                allEmpty = false;
                            }
                        });
                        
                        // Si toutes les derni√®res colonnes sont vides, les masquer
                        if (allEmpty) {
                            rows.forEach(function(row) {
                                const lastCell = row.querySelector('th:last-child, td:last-child');
                                if (lastCell) {
                                    lastCell.style.display = 'none';
                                    lastCell.style.width = '0';
                                    lastCell.style.padding = '0';
                                    lastCell.style.border = 'none';
                                }
                            });
                        }
                    }
                });
            }
            // Ex√©cuter apr√®s le chargement
            setTimeout(hideEmptyColumn, 100);
            setTimeout(hideEmptyColumn, 500);
            setTimeout(hideEmptyColumn, 1000);
            </script>
            """, unsafe_allow_html=True)
        
        with col_map:
            st.subheader("üó∫Ô∏è Carte interactive")
            
            # ‚úÖ Carte interactive avec folium
            try:
                import folium
                from streamlit_folium import folium_static
                
                # Filtrer les communes avec coordonn√©es GPS directement depuis communes_trouvees
                communes_avec_gps = []
                for dept, communes_list in communes_trouvees.items():
                    for comm in communes_list:
                        if comm.get('latitude') and comm.get('longitude'):
                            communes_avec_gps.append({
                                'nom': comm['nom'],
                                'departement': dept,
                                'code_postal': comm.get('code_postal', ''),
                                'population': comm.get('population', 0),
                                'latitude': comm['latitude'],
                                'longitude': comm['longitude']
                            })
                
                if communes_avec_gps:
                    # Calculer le centre de la carte (moyenne des coordonn√©es)
                    avg_lat = sum(c['latitude'] for c in communes_avec_gps) / len(communes_avec_gps)
                    avg_lon = sum(c['longitude'] for c in communes_avec_gps) / len(communes_avec_gps)
                    
                    # Cr√©er une carte centr√©e sur les communes avec un meilleur style
                    m = folium.Map(
                        location=[avg_lat, avg_lon], 
                        zoom_start=8,
                        tiles='OpenStreetMap'  # Style plus propre
                    )
                    
                    # ‚úÖ AFFICHER TOUTES LES COMMUNES (pas de limite de 200)
                    populations = [c['population'] for c in communes_avec_gps if c['population'] > 0]
                    
                    if populations:
                        min_pop_displayed = min(populations)
                        max_pop_displayed = max(populations)
                        pop_range_displayed = max_pop_displayed - min_pop_displayed if max_pop_displayed > min_pop_displayed else 1
                    else:
                        min_pop_displayed = 0
                        max_pop_displayed = 1
                        pop_range_displayed = 1
                    
                    # ‚úÖ Am√©liorer le design : utiliser des clusters pour les grandes quantit√©s
                    from folium.plugins import MarkerCluster
                    marker_cluster = MarkerCluster().add_to(m)
                    
                    for commune in communes_avec_gps:
                        pop = commune['population']
                        pop_str = f"{pop:,}" if pop > 0 else "N/A"
                        popup_text = f"""
                        <div style='font-family: Arial, sans-serif;'>
                            <h4 style='margin: 0 0 10px 0; color: #2c3e50;'>{commune['nom']}</h4>
                            <p style='margin: 5px 0;'><strong>D√©partement:</strong> {commune['departement']}</p>
                            <p style='margin: 5px 0;'><strong>Code postal:</strong> {commune['code_postal']}</p>
                            <p style='margin: 5px 0;'><strong>Population:</strong> {pop_str}</p>
                        </div>
                        """
                        
                        # ‚úÖ Taille du marqueur proportionnelle √† la population (plus petite pour un meilleur design)
                        if pop > 0 and pop_range_displayed > 0:
                            # Normaliser entre 3 et 10 pixels de radius
                            normalized = (pop - min_pop_displayed) / pop_range_displayed
                            radius = 3 + (normalized * 7)  # Entre 3 et 10 pixels
                        else:
                            radius = 3
                        
                        # Couleur selon la population (seuils fixes pour la couleur)
                        if pop > 10000:
                            icon_color = '#e74c3c'  # Rouge
                        elif pop > 5000:
                            icon_color = '#f39c12'  # Orange
                        elif pop > 2000:
                            icon_color = '#3498db'  # Bleu
                        else:
                            icon_color = '#27ae60'  # Vert
                        
                        folium.CircleMarker(
                            location=[commune['latitude'], commune['longitude']],
                            radius=radius,
                            popup=folium.Popup(popup_text, max_width=250),
                            tooltip=f"{commune['nom']} ({pop:,} hab.)" if pop > 0 else commune['nom'],
                            color='white',
                            weight=1.5,
                            fillColor=icon_color,
                            fillOpacity=0.7,
                        ).add_to(marker_cluster)
                    
                    # Afficher la carte
                    folium_static(m, width=700, height=600)
                    
                    st.success(f"üó∫Ô∏è {len(communes_avec_gps)} communes affich√©es avec coordonn√©es GPS")
                    
                    # L√©gende am√©lior√©e
                    st.markdown("""
                    <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 15px; border-radius: 8px; margin-top: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);'>
                        <strong style='font-size: 16px;'>üìä L√©gende :</strong><br>
                        <div style='margin-top: 10px; line-height: 1.8;'>
                            <span style='color: #e74c3c; font-weight: bold;'>‚óè</span> Rouge : > 10 000 hab. | 
                            <span style='color: #f39c12; font-weight: bold;'>‚óè</span> Orange : 5 000 - 10 000 hab. | 
                            <span style='color: #3498db; font-weight: bold;'>‚óè</span> Bleu : 2 000 - 5 000 hab. | 
                            <span style='color: #27ae60; font-weight: bold;'>‚óè</span> Vert : < 2 000 hab.<br>
                            <small style='opacity: 0.9;'>La taille des points est proportionnelle √† la population. Les points proches sont regroup√©s automatiquement.</small>
                        </div>
                    </div>
                    """, unsafe_allow_html=True)
                else:
                    st.warning("‚ö†Ô∏è Aucune commune avec coordonn√©es GPS trouv√©e")
            except ImportError:
                st.info("üí° Pour afficher une carte interactive, installez: `pip install folium streamlit-folium`")
            except Exception as e:
                st.error(f"Erreur lors de la cr√©ation de la carte: {e}")
    else:
        st.warning("Aucune commune trouv√©e avec les crit√®res s√©lectionn√©s")
    
    if st.button("‚ùå Fermer l'affichage des communes"):
        st.session_state.show_communes = False
        st.experimental_rerun()

st.markdown("---")

# √âtat du scraping
if 'scraper' not in st.session_state:
    st.session_state.scraper = None
if 'scraping_running' not in st.session_state:
    st.session_state.scraping_running = False
if 'scraped_results' not in st.session_state:
    st.session_state.scraped_results = []
if 'scraping_thread' not in st.session_state:
    st.session_state.scraping_thread = None
if 'scraping_started' not in st.session_state:
    st.session_state.scraping_started = False
if 'saved_count' not in st.session_state:
    st.session_state.saved_count = 0
if 'logs_buffer' not in st.session_state:
    st.session_state.logs_buffer = []
# Note: github_workflow_id et github_workflow_status sont initialis√©s plus t√¥t dans le code

# ‚úÖ CRITIQUE : V√©rifier si on a un workflow GitHub Actions actif au d√©marrage
# Si oui, maintenir scraping_running = True pour garder le dashboard visible
# Cette v√©rification se fera plus tard, apr√®s le chargement de la config GitHub

# ‚úÖ Fonctions pour GitHub Actions
def trigger_github_workflow(token, repo, metiers, departements, max_results, num_threads, use_api_communes, min_pop, max_pop):
    """D√©clenche le workflow GitHub Actions"""
    try:
        # ‚úÖ D'abord, r√©cup√©rer la liste des workflows pour trouver le bon ID
        workflows_url = f"https://api.github.com/repos/{repo}/actions/workflows"
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        
        # R√©cup√©rer les workflows
        workflows_response = requests.get(workflows_url, headers=headers)
        if workflows_response.status_code != 200:
            return False, f"Erreur r√©cup√©ration workflows: {workflows_response.status_code} - {workflows_response.text}"
        
        workflows_data = workflows_response.json()
        workflow_id = None
        
        # Chercher le workflow "Google Maps Scraping" ou "scraping.yml"
        for workflow in workflows_data.get('workflows', []):
            if workflow.get('name') == 'Google Maps Scraping' or workflow.get('path', '').endswith('scraping.yml'):
                workflow_id = workflow.get('id')
                break
        
        if not workflow_id:
            # Essayer avec le nom du fichier directement
            url = f"https://api.github.com/repos/{repo}/actions/workflows/scraping.yml/dispatches"
        else:
            # Utiliser l'ID du workflow (plus fiable)
            url = f"https://api.github.com/repos/{repo}/actions/workflows/{workflow_id}/dispatches"
        
        data = {
            "ref": "main",  # Essayer "main" d'abord
            "inputs": {
                "metiers": json.dumps(metiers),
                "departements": json.dumps(departements),
                "max_results": str(max_results),
                "num_threads": str(num_threads),
                "use_api_communes": str(use_api_communes).lower(),
                "min_pop": str(min_pop),
                "max_pop": str(max_pop)
            }
        }
        
        response = requests.post(url, headers=headers, json=data)
        
        # Si 404 avec "main", essayer "master"
        if response.status_code == 404 and data["ref"] == "main":
            data["ref"] = "master"
            response = requests.post(url, headers=headers, json=data)
        
        if response.status_code == 204:
            # ‚úÖ R√©cup√©rer le run_id du workflow qui vient d'√™tre lanc√©
            # Attendre un peu pour que GitHub cr√©e le run
            import time
            time.sleep(2)
            
            # R√©cup√©rer le dernier run du workflow
            runs_url = f"https://api.github.com/repos/{repo}/actions/workflows/scraping.yml/runs?per_page=1"
            runs_response = requests.get(runs_url, headers=headers)
            if runs_response.status_code == 200:
                runs_data = runs_response.json().get('workflow_runs', [])
                if runs_data:
                    run_id = runs_data[0].get('id')
                    return True, f"Workflow d√©clench√© avec succ√®s (Run ID: {run_id})", run_id
            
            return True, "Workflow d√©clench√© avec succ√®s"
        else:
            error_msg = response.text
            if response.status_code == 404:
                error_msg += f"\nüí° V√©rifiez que le workflow existe dans .github/workflows/scraping.yml et qu'il est commit√© sur GitHub"
            return False, f"Erreur: {response.status_code} - {error_msg}", None
    except Exception as e:
        return False, f"Erreur: {str(e)}", None

def get_github_workflow_status(token, repo, workflow_id=None):
    """R√©cup√®re le statut du workflow GitHub Actions"""
    try:
        if workflow_id:
            url = f"https://api.github.com/repos/{repo}/actions/runs/{workflow_id}"
        else:
            # R√©cup√©rer le dernier run (non annul√© si possible)
            url = f"https://api.github.com/repos/{repo}/actions/workflows/scraping.yml/runs?per_page=10"
        
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            if workflow_id:
                data = response.json()
            else:
                # R√©cup√©rer tous les runs et trouver le dernier non-annul√© (ou le dernier tout court)
                runs = response.json().get('workflow_runs', [])
                if not runs:
                    return None, None, None
                
                # Chercher le dernier run non-annul√©
                for run in runs:
                    if run.get('conclusion') != 'cancelled':
                        data = run
                        break
                else:
                    # Si tous sont annul√©s, prendre le dernier
                    data = runs[0]
            
            status = data.get('status')  # queued, in_progress, completed
            conclusion = data.get('conclusion')  # success, failure, cancelled, etc.
            run_id = data.get('id')
            return status, conclusion, run_id
        else:
            return None, None, None
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration statut: {e}")
        return None, None, None

def get_github_workflow_logs(token, repo, run_id):
    """R√©cup√®re les logs du workflow GitHub Actions"""
    try:
        # R√©cup√©rer les jobs du workflow
        url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id}/jobs"
        headers = {
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            jobs = response.json().get('jobs', [])
            logs = []
            for job in jobs:
                if job.get('status') == 'completed':
                    # R√©cup√©rer les logs du job
                    logs_url = job.get('logs_url')
                    if logs_url:
                        logs_response = requests.get(logs_url, headers=headers)
                        if logs_response.status_code == 200:
                            # Les logs sont dans un format sp√©cifique GitHub
                            logs.append({
                                'job_name': job.get('name'),
                                'status': job.get('conclusion'),
                                'logs_url': logs_url
                            })
            return logs
        return []
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration logs: {e}")
        return []

# ‚úÖ NOTE : Les fonctions list_github_workflows, cancel_github_workflow et cancel_all_github_workflows 
# sont maintenant d√©finies plus haut (ligne ~206) pour √©viter NameError

# ‚úÖ Cette section a √©t√© d√©plac√©e en haut pour √™tre visible d√®s le d√©marrage (voir ligne ~200)

# ‚úÖ Boutons de contr√¥le SIMPLIFI√âS (comme demand√©)
col_btn1, col_btn2 = st.columns(2)

with col_btn1:
    # ‚úÖ GitHub Actions uniquement - pas de mode local
    # ‚úÖ Le bouton LANCER est toujours activ√© - on peut lancer plusieurs workflows en parall√®le
    button_text = "‚òÅÔ∏è LANCER"
    
    if st.button(button_text, disabled=False):
        if not metiers or not departements:
            st.error("‚ö†Ô∏è Veuillez s√©lectionner au moins un m√©tier et un d√©partement")
        elif not github_token or not github_repo:
            st.error("‚ö†Ô∏è Veuillez renseigner le token GitHub et le repository")
        else:
            # ‚úÖ V√©rifier les villes d√©j√† scrap√©es
            villes_deja_scrapees = []
            villes_a_scraper = []
            
            # Pr√©parer la liste des villes √† scraper
            for dept in departements:
                if use_api_communes:
                    communes = get_communes_from_api(dept, min_pop if use_api_communes else 0, max_pop if use_api_communes else 50000)
                    villes_dept = [c['nom'] for c in communes]
                else:
                    villes_dept = villes_par_dept.get(dept, [])
                
                for metier in metiers:
                    for ville in villes_dept:
                        if is_already_scraped(metier, dept, ville):
                            villes_deja_scrapees.append(f"{metier} - {dept} - {ville}")
                        else:
                            villes_a_scraper.append(f"{metier} - {dept} - {ville}")
            
            # Afficher un avertissement si certaines villes sont d√©j√† scrap√©es
            if villes_deja_scrapees:
                st.warning(f"‚ö†Ô∏è {len(villes_deja_scrapees)} combinaison(s) m√©tier/d√©partement/ville d√©j√† scrap√©e(s). Elles seront ignor√©es si vous continuez.")
                with st.expander("üìã Voir les villes d√©j√† scrap√©es"):
                    for v in villes_deja_scrapees[:20]:  # Limiter √† 20 pour l'affichage
                        st.text(v)
                    if len(villes_deja_scrapees) > 20:
                        st.caption(f"... et {len(villes_deja_scrapees) - 20} autres")
            
            if not villes_a_scraper:
                st.error("‚ùå Toutes les combinaisons s√©lectionn√©es ont d√©j√† √©t√© scrap√©es. Veuillez s√©lectionner d'autres options.")
            else:
                st.info(f"‚úÖ {len(villes_a_scraper)} combinaison(s) √† scraper")
                
                # D√©clencher le workflow GitHub Actions
                result = trigger_github_workflow(
                    github_token,
                    github_repo,
                    metiers,
                    departements,
                    max_results,
                    num_threads,
                    use_api_communes,
                    min_pop if use_api_communes else 0,
                    max_pop if use_api_communes else 50000
                )
                
                # ‚úÖ G√©rer le retour (peut √™tre (success, message) ou (success, message, run_id))
                if len(result) == 3:
                    success, message, run_id = result
                else:
                    success, message = result
                    run_id = None
                
                if success:
                    st.success(f"‚úÖ {message}")
                    st.info("‚è≥ Le scraping est en cours sur GitHub Actions. Les r√©sultats sont sauvegard√©s directement dans la BDD.")
                    st.session_state.scraping_running = True
                    st.session_state.github_workflow_status = 'in_progress'
                    st.session_state.departements_selected = departements
                    st.session_state.metiers_selected = metiers
                    
                    # ‚úÖ Stocker le run_id si disponible
                    if run_id:
                        st.session_state.github_workflow_id = run_id
                    # ‚úÖ Marquer qu'on vient de lancer un workflow pour ne pas l'annuler
                    st.session_state.workflow_just_launched = True
                    st.experimental_rerun()
                else:
                    st.error(f"‚ùå {message}")

with col_btn2:
    # ‚úÖ Bouton ARR√äTER (simplifi√© - pas de confirmation)
    if github_token and github_repo:
        if st.button("‚èπÔ∏è ARR√äTER", help="Arr√™ter tous les workflows GitHub Actions en cours", key="stop_all_workflows"):
            with st.spinner("‚èπÔ∏è Arr√™t des workflows..."):
                success, message = cancel_all_github_workflows(github_token, github_repo)
                if success:
                    st.success(f"‚úÖ {message}")
                    st.session_state.scraping_running = False
                    st.session_state.github_workflow_status = None
                    st.session_state.github_workflow_id = None
                    st.session_state.scraped_results = []
                    st.experimental_rerun()
                else:
                    st.error(f"‚ùå {message}")

# ‚úÖ NOTE : Le dashboard GitHub Actions a √©t√© supprim√© car les workflows sont maintenant g√©r√©s en haut de la page
# avec le bouton "Rafra√Æchir les workflows" qui affiche les statistiques pour chaque workflow

# Zone de scraping (ancien dashboard supprim√© - tout est g√©r√© en haut maintenant)
# Le code du dashboard GitHub Actions a √©t√© supprim√© car il n'est plus n√©cessaire
# Tous les workflows sont maintenant g√©r√©s en haut avec le bouton "Rafra√Æchir les workflows"

# Ancien code du dashboard compl√®tement supprim√©

# ‚úÖ V√©rifier si on doit annuler les workflows au d√©marrage
# ‚úÖ NE PAS annuler si on vient juste de lancer un workflow
if github_token and github_repo and not st.session_state.get('workflow_just_launched', False):
    # ‚úÖ Tuer automatiquement tous les workflows en cours au d√©marrage (une seule fois)
    if not st.session_state.get('workflows_cancelled_on_start', False):
        with st.spinner("‚èπÔ∏è Annulation des workflows GitHub Actions en cours..."):
            success, message = cancel_all_github_workflows(github_token, github_repo)
            # ‚úÖ TOUJOURS d√©finir le flag pour √©viter les boucles, m√™me en cas d'√©chec
            st.session_state.workflows_cancelled_on_start = True
            if success:
                # Ne pas r√©initialiser scraping_running si on a un workflow_id
                if not st.session_state.get('github_workflow_id'):
                    st.session_state.scraping_running = False
                    st.session_state.github_workflow_status = None
                    st.session_state.github_workflow_id = None
                st.success(f"‚úÖ {message}")
                # ‚úÖ Rerun seulement si on a annul√© avec succ√®s
                st.experimental_rerun()
            else:
                st.warning(f"‚ö†Ô∏è {message}")
                # ‚úÖ NE PAS faire de rerun en cas d'√©chec pour √©viter les boucles
# ‚úÖ R√©initialiser le flag apr√®s le premier rerun
if st.session_state.get('workflow_just_launched', False):
    st.session_state.workflow_just_launched = False

# ‚úÖ GitHub Actions uniquement - plus de code local

# ‚úÖ CRITIQUE : Charger automatiquement les r√©sultats depuis le JSON ET la BDD au d√©marrage
# Si on n'a pas de r√©sultats en session_state, essayer de les charger depuis le JSON et la BDD
if not st.session_state.scraped_results:
    results_file = Path(__file__).parent.parent.parent / "data" / "scraping_results_github_actions.json"
    results_list = []
    
    # 1. Charger depuis le fichier JSON (si existe)
    if results_file.exists():
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                results_data = json.load(f)
                if isinstance(results_data, dict) and 'results' in results_data:
                    results_list = results_data['results']
                elif isinstance(results_data, list):
                    results_list = results_data
        except Exception as e:
            logger.error(f"Erreur chargement JSON: {e}")
    
    # 2. ‚úÖ AUSSI charger depuis la BDD (pour voir les r√©sultats sauvegard√©s directement)
    try:
        from whatsapp_database.queries import get_artisans
        artisans_bdd = get_artisans(limit=10000)  # R√©cup√©rer tous les artisans
        if artisans_bdd:
            # Convertir les artisans de la BDD en format compatible
            for artisan in artisans_bdd:
                # √âviter les doublons (par t√©l√©phone)
                if not any(r.get('telephone') == artisan.get('telephone') for r in results_list if r.get('telephone')):
                    # ‚úÖ Extraire d√©partement depuis code_postal si manquant
                    dept = artisan.get('departement')
                    code_postal = artisan.get('code_postal')
                    if not dept and code_postal:
                        code_postal_str = str(code_postal).strip()
                        if len(code_postal_str) >= 2:
                            if code_postal_str.startswith('97') or code_postal_str.startswith('98'):
                                dept = code_postal_str[:3]
                            else:
                                dept = code_postal_str[:2]
                    
                    # ‚úÖ Si toujours pas de d√©partement, essayer depuis ville_recherche via API
                    if not dept and artisan.get('ville_recherche'):
                        try:
                            ville_nom = artisan.get('ville_recherche', '').strip()
                            if ville_nom:
                                url = f"https://geo.api.gouv.fr/communes?nom={ville_nom}&fields=codeDepartement&limit=1"
                                response = requests.get(url, timeout=3)
                                if response.status_code == 200:
                                    communes = response.json()
                                    if communes and len(communes) > 0:
                                        dept = communes[0].get('codeDepartement', '')
                        except:
                            pass  # Si l'API √©choue, on continue sans d√©partement
                    
                    results_list.append({
                        'nom': artisan.get('nom_entreprise') or artisan.get('nom'),
                        'telephone': artisan.get('telephone'),
                        'site_web': artisan.get('site_web'),
                        'google_maps_url': artisan.get('google_maps_url'),
                        'adresse': artisan.get('adresse'),
                        'ville': artisan.get('ville'),
                        'code_postal': code_postal,
                        'departement': dept,
                        'note': artisan.get('note'),
                        'nb_avis': artisan.get('nombre_avis'),
                        'ville_recherche': artisan.get('ville_recherche'),
                        'recherche': artisan.get('type_artisan') or artisan.get('recherche')
                    })
    except Exception as e:
        logger.error(f"Erreur chargement BDD: {e}")
    
    if results_list:
        st.session_state.scraped_results = results_list

# Afficher les r√©sultats scrap√©s
if st.session_state.scraped_results:
    st.markdown("---")
    st.subheader("üìä R√©sultats scrap√©s")
    
    df = pd.DataFrame(st.session_state.scraped_results)
    
    # Stats
    avec_tel = len(df[df['telephone'].notna()])
    avec_site = len(df[df['site_web'].notna()])
    sans_site = len(df[df['site_web'].isna()])
    
    col_res1, col_res2, col_res3, col_res4 = st.columns(4)
    with col_res1:
        st.metric("Total scrap√©s", len(df))
    with col_res2:
        st.metric("Avec t√©l√©phone", f"{avec_tel} ({avec_tel/len(df)*100:.1f}%)")
    with col_res3:
        st.metric("Avec site web", f"{avec_site} ({avec_site/len(df)*100:.1f}%)")
    with col_res4:
        st.metric("‚≠ê SANS site web", f"{sans_site} ({sans_site/len(df)*100:.1f}%)")
    
    # ‚úÖ Filtres supprim√©s comme demand√©
    df_filtre = df.copy()
    
    # ‚úÖ Afficher le tableau avec toutes les colonnes importantes
    colonnes_afficher = ['nom', 'telephone', 'site_web', 'google_maps_url', 'adresse', 'ville', 'code_postal', 'departement', 'ville_recherche', 'note', 'nb_avis']
    colonnes_disponibles = [col for col in colonnes_afficher if col in df_filtre.columns]
    
    # CSS am√©lior√© pour le tableau
    css_table = (
        "<style>"
        ".stDataFrame { width: 100% !important; }"
        ".stDataFrame > div { width: 100% !important; }"
        ".stDataFrame table { width: 100% !important; table-layout: auto !important; }"
        ".stDataFrame th, .stDataFrame td { padding: 8px !important; word-wrap: break-word !important; overflow-wrap: break-word !important; white-space: normal !important; }"
        ".stDataFrame th:nth-child(1) { width: 15% !important; }"
        ".stDataFrame th:nth-child(2) { width: 10% !important; }"
        ".stDataFrame th:nth-child(3) { width: 20% !important; }"
        ".stDataFrame th:nth-child(4) { width: 20% !important; }"
        ".stDataFrame th:nth-child(5) { width: 12% !important; }"
        ".stDataFrame th:nth-child(6) { width: 10% !important; }"
        ".stDataFrame th:nth-child(7) { width: 6% !important; }"
        ".stDataFrame th:nth-child(8) { width: 7% !important; }"
        "</style>"
    )
    st.markdown(css_table, unsafe_allow_html=True)
    
    # ‚úÖ Utiliser width au lieu de use_container_width (compatibilit√© Streamlit)
    st.dataframe(
        df_filtre[colonnes_disponibles],
        height=600
    )
    
    # ‚úÖ Bouton d'export CSV complet
    csv_all = df.to_csv(index=False, encoding='utf-8-sig')
    dept = st.session_state.get('departements_selected', departements)[0] if st.session_state.get('departements_selected') else (departements[0] if departements else '77')
    metier_export = st.session_state.get('metiers_selected', metiers)[0] if st.session_state.get('metiers_selected') else (metiers[0] if metiers else 'plombier')
    st.download_button(
        "üì• T√©l√©charger CSV complet",
        csv_all,
        f"{metier_export}_{dept}_complet_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        "text/csv"
    )
    
# ‚úÖ Expander "Mode d'ex√©cution" tout en bas de la page
with st.expander("‚ÑπÔ∏è Mode d'ex√©cution", expanded=False):
    st.info("‚òÅÔ∏è **Mode GitHub Actions activ√©** - Le scraping s'ex√©cutera sur GitHub Actions (gratuit jusqu'√† 2000 min/mois)")
    st.info("‚ÑπÔ∏è Le scraping s'ex√©cutera sur GitHub Actions. Les r√©sultats sont sauvegard√©s directement dans la BDD en temps r√©el.")
