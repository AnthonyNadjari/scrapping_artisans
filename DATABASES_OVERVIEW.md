# ğŸ“Š Vue d'ensemble des bases de donnÃ©es et fichiers de stockage

Ce document liste tous les fichiers de base de donnÃ©es et de stockage utilisÃ©s dans le projet.

## ğŸ—„ï¸ Base de donnÃ©es SQLite

### `data/whatsapp_artisans.db`

**Type** : Base de donnÃ©es SQLite principale  
**Chemin** : `data/whatsapp_artisans.db`  
**DÃ©finition** : `config/whatsapp_settings.py` â†’ `DB_PATH`

**Tables contenues** :

1. **`artisans`** (table principale)
   - DonnÃ©es des artisans scrapÃ©s (nom, tÃ©lÃ©phone, adresse, etc.)
   - Colonnes principales :
     - `id` : Identifiant unique
     - `nom_entreprise`, `nom`, `prenom`
     - `type_artisan` : MÃ©tier (plombier, Ã©lectricien, etc.)
     - `adresse`, `code_postal`, `ville`, `departement`
     - `telephone` : UNIQUE (Ã©vite les doublons)
     - `site_web` : URL du site web
     - `note` : Note Google Maps (0-5)
     - `nombre_avis` : Nombre d'avis Google Maps
     - `ville_recherche` : Ville utilisÃ©e pour la recherche
     - `source` : Source des donnÃ©es ('google_maps_github_actions', etc.)
     - `phone_type` : Type de tÃ©lÃ©phone ('mobile', 'landline', etc.)
     - `site_type` : Type de site ('facebook', 'instagram', 'classic', 'none')
     - `last_message_date` : Date du dernier message WhatsApp
     - `last_template_used` : Template de message utilisÃ©
     - `message_envoye`, `a_repondu` : Statuts de campagne
     - `created_at` : Date de crÃ©ation

2. **`scraping_history`**
   - Historique des scrapings effectuÃ©s
   - Ã‰vite les doublons (mÃ©tier + dÃ©partement + ville)
   - Colonnes : `metier`, `departement`, `ville`, `scraped_at`, `results_count`

3. **`messages_log`**
   - Log des messages WhatsApp envoyÃ©s
   - Colonnes : `artisan_id`, `date_envoi`, `message_id`, `statut`, `erreur`

4. **`reponses`**
   - RÃ©ponses reÃ§ues des artisans
   - Colonnes : `artisan_id`, `date_reception`, `contenu`, `message_id`

**Initialisation** : `whatsapp_database/models.py` â†’ `init_database()`

**RÃ©initialisation** : `scripts/reset_all_databases.py`

---

## ğŸ“„ Fichiers JSON de donnÃ©es

### 1. `data/scraping_results_github_actions.json`

**Type** : RÃ©sultats de scraping depuis GitHub Actions  
**Format** : Objet JSON `{"timestamp": "...", "total_results": N, "results": [...]}`  
**Utilisation** :
- **Sur GitHub Actions (runner)** : Mis Ã  jour progressivement pendant le scraping (Ã  chaque Ã©tablissement trouvÃ©)
- **Upload comme artifact** : Le fichier est uploadÃ© comme artifact GitHub Actions Ã  la fin du workflow (ligne 106-114 de `.github/workflows/scraping.yml`)
- **Local (Streamlit)** : TÃ©lÃ©chargÃ© depuis l'artifact GitHub Actions uniquement quand on clique sur "ğŸ“¥ Importer depuis GitHub Actions"
- **Import dans SQLite** : Les donnÃ©es sont importÃ©es dans la base SQLite locale uniquement lors de l'import manuel
- **RÃ©initialisÃ©** : Liste vide `[]`

**âš ï¸ IMPORTANT** :
- âŒ **PAS de mise Ã  jour en continu automatique** : La base SQLite locale n'est PAS mise Ã  jour automatiquement pendant que le workflow tourne
- âŒ **PAS de transfert automatique** : Il faut cliquer manuellement sur "Importer depuis GitHub Actions" pour tÃ©lÃ©charger l'artifact et importer dans la base locale
- âœ… **Mise Ã  jour progressive sur GitHub** : Le fichier JSON est mis Ã  jour Ã  chaque Ã©tablissement trouvÃ© dans le runner GitHub Actions
- âœ… **Base SQLite sur GitHub Actions** : Une base SQLite est aussi crÃ©Ã©e dans le runner GitHub Actions, mais elle n'est PAS accessible depuis Streamlit (elle est dÃ©truite Ã  la fin du workflow)

**Fichiers utilisant ce fichier** :
- `scripts/run_scraping_github_actions.py` : Sauvegarde progressive pendant le scraping (ligne 222)
- `whatsapp_app/pages/2_ğŸ“Š_Base_de_DonnÃ©es.py` : TÃ©lÃ©chargement et import depuis l'artifact GitHub Actions

---

### 2. `data/github_actions_status.json`

**Type** : Statut des workflows GitHub Actions  
**Format** : Objet JSON `{}`  
**Utilisation** :
- Stocke le statut des workflows GitHub Actions (en cours, terminÃ©s, etc.)
- Suivi des workflows actifs
- **RÃ©initialisÃ©** : Objet vide `{}`

**Fichiers utilisant ce fichier** :
- `whatsapp_app/pages/1_ğŸ”_Scraping.py` : Affichage du statut des workflows

---

### 3. `data/ville_dept_cache.json`

**Type** : Cache pour mapping ville â†’ dÃ©partement  
**Format** : Objet JSON `{"ville": "departement", ...}`  
**Utilisation** :
- Cache les rÃ©sultats des appels API `data.gouv.fr` pour Ã©viter les appels rÃ©pÃ©tÃ©s
- AccÃ©lÃ¨re l'affichage des cartes
- **Non rÃ©initialisÃ©** : Cache persistant (peut Ãªtre supprimÃ© manuellement)

**Fichiers utilisant ce fichier** :
- `whatsapp_app/utils/map_utils.py` : Cache pour les cartes

---

### 4. `data/villes_par_departement.json`

**Type** : Liste des villes par dÃ©partement (fallback)  
**Format** : Objet JSON `{"77": ["ville1", "ville2", ...], ...}`  
**Utilisation** :
- Liste de secours si l'API `data.gouv.fr` n'est pas utilisÃ©e
- UtilisÃ© uniquement si `use_api_communes = False`
- **Non rÃ©initialisÃ©** : DonnÃ©es statiques

**Fichiers utilisant ce fichier** :
- `whatsapp_app/pages/1_ğŸ”_Scraping.py` : Liste des villes (mode non-API)
- `scripts/run_scraping_github_actions.py` : Liste des villes (mode non-API)

---

### 5. `data/codes_naf.json`

**Type** : Codes NAF (activitÃ©s Ã©conomiques)  
**Format** : Objet JSON  
**Utilisation** :
- Codes NAF pour catÃ©goriser les artisans
- **Non rÃ©initialisÃ©** : DonnÃ©es statiques

---

### 6. `config/github_config.json`

**Type** : Configuration GitHub Actions  
**Format** : Objet JSON  
**Utilisation** :
- Configuration des workflows GitHub Actions
- Token GitHub, repository, etc.
- **Non rÃ©initialisÃ©** : Configuration

---

### 7. `config/api_config.json`

**Type** : Configuration API  
**Format** : Objet JSON  
**Utilisation** :
- Configuration des APIs externes (data.gouv.fr, etc.)
- **Non rÃ©initialisÃ©** : Configuration

---

## ğŸ—‘ï¸ Fichiers JSON obsolÃ¨tes (supprimÃ©s automatiquement)

Ces fichiers ne sont **plus utilisÃ©s** (vestiges du mode scraping local) et sont supprimÃ©s par `scripts/reset_all_databases.py` :

- `data/scraping_results_temp.json` âŒ
- `data/scraping_status.json` âŒ
- `data/scraping_checkpoint.json` âŒ
- `data/scraping_logs.json` âŒ
- `data/saved_count.json` âŒ

---

## ğŸ“‹ RÃ©sumÃ©

| Fichier | Type | RÃ©initialisÃ© ? | Utilisation principale |
|---------|------|----------------|------------------------|
| `data/whatsapp_artisans.db` | SQLite | âœ… Oui | Base de donnÃ©es principale (artisans, messages, etc.) |
| `data/scraping_results_github_actions.json` | JSON | âœ… Oui | RÃ©sultats temporaires GitHub Actions |
| `data/github_actions_status.json` | JSON | âœ… Oui | Statut workflows GitHub Actions |
| `data/ville_dept_cache.json` | JSON | âŒ Non | Cache villeâ†’dÃ©partement (performance) |
| `data/villes_par_departement.json` | JSON | âŒ Non | Liste villes fallback (si API dÃ©sactivÃ©e) |
| `data/codes_naf.json` | JSON | âŒ Non | Codes NAF (donnÃ©es statiques) |
| `config/github_config.json` | JSON | âŒ Non | Configuration GitHub |
| `config/api_config.json` | JSON | âŒ Non | Configuration API |

---

## ğŸ”„ Flux de donnÃ©es GitHub Actions â†’ Streamlit

### Pendant le scraping (sur GitHub Actions)

1. **Scraping en cours** :
   - Chaque Ã©tablissement trouvÃ© est sauvegardÃ© dans :
     - âœ… **Base SQLite locale du runner** : `data/whatsapp_artisans.db` (via `ajouter_artisan()`)
     - âœ… **Fichier JSON** : `data/scraping_results_github_actions.json` (via `save_progress()`)
   - Les deux sont mis Ã  jour **progressivement** Ã  chaque Ã©tablissement

2. **Fin du workflow** :
   - Le fichier JSON est **uploadÃ© comme artifact** GitHub Actions (ligne 106-114 de `.github/workflows/scraping.yml`)
   - La base SQLite du runner est **dÃ©truite** (le runner est supprimÃ© aprÃ¨s le workflow)
   - âš ï¸ **La base SQLite sur GitHub Actions n'est PAS accessible depuis Streamlit**

### Import dans Streamlit (local)

1. **Clic sur "ğŸ“¥ Importer depuis GitHub Actions"** :
   - TÃ©lÃ©charge l'artifact `scraping-results` depuis GitHub Actions
   - Extrait `scraping_results_github_actions.json` depuis le ZIP
   - Parse le JSON et importe chaque artisan dans la **base SQLite locale** via `ajouter_artisan()`

2. **âš ï¸ IMPORTANT** :
   - âŒ **PAS de mise Ã  jour automatique** : La base SQLite locale n'est PAS mise Ã  jour pendant que le workflow tourne
   - âŒ **PAS de transfert en continu** : Il faut cliquer manuellement sur "Importer" pour rÃ©cupÃ©rer les rÃ©sultats
   - âœ… **Import manuel uniquement** : Les donnÃ©es sont importÃ©es uniquement quand vous cliquez sur le bouton d'import

### RÃ©sumÃ© du flux

```
GitHub Actions Runner:
  â””â”€ Scraping â†’ save_callback() â†’ Base SQLite (runner) + JSON (runner)
  â””â”€ Fin workflow â†’ Upload artifact (JSON uniquement)
  
Streamlit Local:
  â””â”€ Clic "Importer" â†’ TÃ©lÃ©charge artifact â†’ Parse JSON â†’ Base SQLite (local)
```

---

## ğŸ”„ RÃ©initialisation

Pour rÃ©initialiser **toutes** les bases de donnÃ©es :

```bash
python scripts/reset_all_databases.py
```

Ou en mode non-interactif :

```bash
python scripts/reset_all_databases.py --force
```

**Ce qui est rÃ©initialisÃ©** :
- âœ… Base SQLite locale : Toutes les tables vidÃ©es
- âœ… `scraping_results_github_actions.json` local : Liste vide
- âœ… `github_actions_status.json` local : Objet vide
- âŒ Cache et fichiers de configuration : **Non rÃ©initialisÃ©s**
- âš ï¸ **Les artifacts GitHub Actions ne sont PAS supprimÃ©s** (ils expirent aprÃ¨s 7 jours automatiquement)

---

## ğŸ“ Emplacements

- **Base SQLite** : `data/whatsapp_artisans.db`
- **Fichiers JSON de donnÃ©es** : `data/*.json`
- **Fichiers de configuration** : `config/*.json`

---

## ğŸ” VÃ©rification

Pour voir le contenu de la base SQLite :

```bash
python scripts/analyze_database.py
```

Pour voir les fichiers JSON :

```bash
# Lister tous les fichiers JSON
ls -la data/*.json
ls -la config/*.json
```

