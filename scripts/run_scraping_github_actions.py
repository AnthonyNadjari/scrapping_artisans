#!/usr/bin/env python3
"""
Script pour exÃ©cuter le scraping depuis GitHub Actions
Avec commits pÃ©riodiques pour sauvegarder les rÃ©sultats mÃªme en cas de timeout
"""
import json
import sys
import os
import subprocess
import threading
import time
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# Ajouter le rÃ©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from scraping.google_maps_scraper import GoogleMapsScraper
import requests
from whatsapp_database.queries import ajouter_artisan, mark_scraping_done
from whatsapp_database.models import init_database

# Variable globale pour contrÃ´ler le thread de commit pÃ©riodique
stop_periodic_commit = threading.Event()
last_commit_count = 0


def git_commit_and_push(message: str) -> bool:
    """Commit et push les rÃ©sultats vers GitHub"""
    try:
        # VÃ©rifier si on est dans un environnement GitHub Actions
        if not os.environ.get('GITHUB_TOKEN'):
            print("âš ï¸ Pas de GITHUB_TOKEN, commit ignorÃ©")
            return False

        results_file = Path('data/scraping_results_github_actions.json')
        status_file = Path('data/github_actions_status.json')

        if not results_file.exists():
            print("âš ï¸ Pas de fichier de rÃ©sultats Ã  commiter")
            return False

        # âœ… VÃ©rifier que le fichier contient des donnÃ©es
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                total = data.get('total_results', 0)
                print(f"ğŸ“„ Fichier de rÃ©sultats: {total} rÃ©sultats")
        except Exception as e:
            print(f"âš ï¸ Erreur lecture fichier: {e}")

        # âœ… Pull avant push pour Ã©viter les conflits
        print("ğŸ”„ git pull --rebase...")
        pull_result = subprocess.run(
            ['git', 'pull', '--rebase', 'origin', 'main'],
            capture_output=True, text=True, check=False
        )
        if pull_result.returncode != 0:
            print(f"âš ï¸ git pull: {pull_result.stderr}")
            # Continuer quand mÃªme

        # Ajouter les fichiers
        print("ğŸ“ git add...")
        add_result = subprocess.run(['git', 'add', str(results_file), str(status_file)],
                      capture_output=True, text=True, check=False)
        if add_result.returncode != 0:
            print(f"âš ï¸ git add stderr: {add_result.stderr}")

        # VÃ©rifier s'il y a des changements Ã  commiter
        result = subprocess.run(['git', 'status', '--porcelain'],
                               capture_output=True, text=True, check=False)
        print(f"ğŸ“‹ git status: '{result.stdout.strip()}'")
        if not result.stdout.strip():
            print("â„¹ï¸ Aucun changement Ã  commiter")
            return True

        # Commit
        print(f"ğŸ’¾ git commit -m '{message[:50]}...'")
        commit_result = subprocess.run(
            ['git', 'commit', '-m', message],
            capture_output=True, text=True, check=False
        )

        if commit_result.returncode != 0:
            print(f"âš ï¸ Erreur commit (code {commit_result.returncode})")
            print(f"   stdout: {commit_result.stdout}")
            print(f"   stderr: {commit_result.stderr}")
            return False
        else:
            print(f"âœ… Commit OK: {commit_result.stdout.strip()}")

        # Push
        print("ğŸš€ git push...")
        push_result = subprocess.run(
            ['git', 'push', 'origin', 'main'],
            capture_output=True, text=True, check=False
        )

        if push_result.returncode != 0:
            print(f"âš ï¸ Erreur push (code {push_result.returncode})")
            print(f"   stdout: {push_result.stdout}")
            print(f"   stderr: {push_result.stderr}")
            return False
        else:
            print(f"âœ… Push OK: {push_result.stdout.strip()}")

        print(f"âœ… Commit et push rÃ©ussis: {message}")
        return True

    except Exception as e:
        print(f"âŒ Erreur git: {e}")
        import traceback
        traceback.print_exc()
        return False


def periodic_commit_thread(interval_minutes: int = 10):
    """Thread qui fait des commits pÃ©riodiques des rÃ©sultats"""
    global last_commit_count

    interval_seconds = interval_minutes * 60
    print(f"ğŸ”„ Thread de commit pÃ©riodique dÃ©marrÃ© (intervalle: {interval_minutes} min)")

    # âœ… FIX: Attendre un peu avant le premier check (30 secondes) pour laisser le scraping dÃ©marrer
    # puis vÃ©rifier rÃ©guliÃ¨rement
    first_check_delay = 30  # PremiÃ¨re vÃ©rification aprÃ¨s 30 secondes

    # Premier dÃ©lai court
    if stop_periodic_commit.wait(first_check_delay):
        print("ğŸ”„ Thread de commit pÃ©riodique arrÃªtÃ© (avant premier check)")
        return

    while True:
        try:
            # Lire le nombre actuel de rÃ©sultats
            results_file = Path('data/scraping_results_github_actions.json')
            if results_file.exists():
                with open(results_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    current_count = data.get('total_results', 0)

                # Ne commiter que s'il y a de nouveaux rÃ©sultats
                if current_count > last_commit_count:
                    new_results = current_count - last_commit_count
                    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    message = f"ğŸ¤– Auto-save: {current_count} rÃ©sultats (+{new_results}) - {timestamp}"

                    print(f"ğŸ”„ Tentative de commit: {current_count} rÃ©sultats...")
                    if git_commit_and_push(message):
                        last_commit_count = current_count
                        print(f"ğŸ’¾ Commit pÃ©riodique rÃ©ussi: {current_count} rÃ©sultats sauvegardÃ©s")
                    else:
                        print(f"âš ï¸ Ã‰chec du commit pÃ©riodique (voir logs ci-dessus)")
                else:
                    print(f"â„¹ï¸ Pas de nouveaux rÃ©sultats depuis le dernier commit ({current_count} total)")
            else:
                print(f"â³ Fichier de rÃ©sultats pas encore crÃ©Ã©...")
        except Exception as e:
            print(f"âŒ Erreur dans le thread de commit: {e}")
            import traceback
            traceback.print_exc()

        # Attendre l'intervalle ou arrÃªt
        if stop_periodic_commit.wait(interval_seconds):
            break

    print("ğŸ”„ Thread de commit pÃ©riodique arrÃªtÃ©")


def get_communes_from_api(dept, min_pop, max_pop):
    """RÃ©cupÃ¨re les communes d'un dÃ©partement depuis l'API data.gouv.fr"""
    try:
        url = f"https://geo.api.gouv.fr/departements/{dept}/communes"
        params = {
            "fields": "nom,code,codesPostaux,population,centre",
            "format": "json"
        }
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            communes = response.json()
            filtered = []
            for c in communes:
                pop = c.get('population', 0)
                if min_pop <= pop <= max_pop:
                    centre = c.get('centre', {})
                    filtered.append({
                        'nom': c['nom'],
                        'code_postal': c.get('codesPostaux', [c.get('code', '')])[0] if c.get('codesPostaux') else c.get('code', ''),
                        'population': pop,
                        'latitude': centre.get('coordinates', [None, None])[1] if centre else None,
                        'longitude': centre.get('coordinates', [None, None])[0] if centre else None
                    })
            filtered.sort(key=lambda x: x['population'])
            return filtered
    except Exception as e:
        print(f"Erreur API communes: {e}")
    return []

def save_callback(artisan_data):
    """Callback pour sauvegarder directement dans la BDD Ã  chaque Ã©tablissement trouvÃ©"""
    try:
        # VÃ©rifier que artisan_data contient au moins une donnÃ©e valide
        if not artisan_data:
            print("âš ï¸ save_callback: artisan_data est None ou vide")
            return None
        
        # PrÃ©parer les donnÃ©es pour la BDD
        # âœ… FIX : Utiliser 'nom_entreprise' au lieu de 'nom' pour correspondre au schÃ©ma de la BDD
        import re
        
        # âœ… NETTOYER l'adresse : enlever "Closed", "FermÃ©", sauts de ligne, etc.
        adresse_brute = artisan_data.get('adresse', '')
        if adresse_brute:
            adresse_clean = re.sub(r'\s*(Closed|Closes|Closes soon|FermÃ©|FermÃ©e|Ouvert|Open|Opens|Opening|Soon)\s*', '', str(adresse_brute), flags=re.IGNORECASE)
            adresse_clean = re.sub(r'\s*\n\s*', ' ', adresse_clean)  # Remplacer sauts de ligne par espaces
            adresse_clean = re.sub(r'\s+', ' ', adresse_clean).strip()  # Normaliser les espaces
        else:
            adresse_clean = ''
        
        # âœ… Stocker le dÃ©partement de recherche pour rÃ©fÃ©rence (mais ne pas l'utiliser comme dÃ©partement rÃ©el)
        departement_recherche = artisan_data.get('departement_recherche') or artisan_data.get('departement')

        data = {
            'nom_entreprise': artisan_data.get('nom'),  # âœ… FIX : nom_entreprise au lieu de nom
            'telephone': artisan_data.get('telephone'),
            'site_web': artisan_data.get('site_web'),
            'google_maps_url': artisan_data.get('google_maps_url'),  # URL directe vers la fiche Google Maps
            'adresse': adresse_clean,  # âœ… Adresse nettoyÃ©e
            'code_postal': artisan_data.get('code_postal'),
            'ville': artisan_data.get('ville'),
            'departement': None,  # âœ… FIX: Sera dÃ©rivÃ© du code_postal, pas du dÃ©partement recherchÃ©
            'note': artisan_data.get('note'),
            'nombre_avis': artisan_data.get('nb_avis') or artisan_data.get('nombre_avis'),  # âœ… Support des deux formats
            'ville_recherche': artisan_data.get('ville_recherche'),
            'departement_recherche': departement_recherche,  # âœ… Garder trace du dÃ©partement de recherche
            'source': 'google_maps',
            'source_telephone': 'google_maps',
            'type_artisan': artisan_data.get('recherche') or artisan_data.get('type_artisan')  # âœ… Support des deux formats
        }

        # âœ… PRIORITÃ‰ 1: Extraire le code postal depuis l'adresse si manquant
        if not data.get('code_postal') and data.get('adresse'):
            cp_match = re.search(r'\b(\d{5})\b', data['adresse'])
            if cp_match:
                data['code_postal'] = cp_match.group(1)

        # âœ… PRIORITÃ‰ 2: Extraire le dÃ©partement depuis le code postal (la source la plus fiable)
        if data.get('code_postal'):
            code_postal = str(data['code_postal']).strip()
            # Valider que c'est un code postal franÃ§ais valide (5 chiffres)
            if re.match(r'^\d{5}$', code_postal):
                # Pour les dÃ©partements d'outre-mer (97x, 98x), prendre les 3 premiers chiffres
                if code_postal.startswith('97') or code_postal.startswith('98'):
                    data['departement'] = code_postal[:3]
                else:
                    data['departement'] = code_postal[:2]

        # âœ… FALLBACK: Utiliser le dÃ©partement recherchÃ© seulement si on n'a pas pu l'extraire du CP
        if not data.get('departement') and departement_recherche:
            data['departement'] = departement_recherche
        
        # âœ… Extraire la ville depuis l'adresse si manquante (amÃ©liorÃ©)
        if not data.get('ville') and data.get('adresse'):
            adresse = str(data['adresse'])
            # Pattern 1: "code_postal ville" (format franÃ§ais standard)
            ville_match = re.search(r'\b\d{5}\s+([A-Za-zÃ€-Ã¿\s-]+?)(?:\s|$|,|;|France|Closed|FermÃ©)', adresse, re.IGNORECASE)
            if ville_match:
                ville = ville_match.group(1).strip()
                # Nettoyer la ville (enlever "France", "Closed", etc.)
                ville = re.sub(r'\s*(France|FR|FRANCE|Closed|FermÃ©|FermÃ©e)\s*$', '', ville, flags=re.IGNORECASE).strip()
                # âœ… VÃ©rifier que ce n'est pas un nom d'entreprise
                mots_interdits = ['rue', 'avenue', 'boulevard', 'place', 'allÃ©e', 'chemin', 'route', 
                                 'plomberie', 'solution', 'eaux', 'cernoise', 'services', 'entreprise']
                if ville and ville.lower() not in mots_interdits and not any(mot in ville.lower() for mot in ['plombier', 'plomberie', 'solution']):
                    data['ville'] = ville
            # Pattern 2: Si pas trouvÃ©, chercher aprÃ¨s le dernier chiffre
            if not data.get('ville'):
                ville_match2 = re.search(r'\d{5}\s+(.+?)(?:\s*$|,|;|France|Closed|FermÃ©)', adresse)
                if ville_match2:
                    ville = ville_match2.group(1).strip()
                    ville = re.sub(r'\s*(France|FR|FRANCE|Closed|FermÃ©|FermÃ©e)\s*$', '', ville, flags=re.IGNORECASE).strip()
                    # âœ… VÃ©rifier que ce n'est pas un nom d'entreprise
                    mots_interdits = ['rue', 'avenue', 'boulevard', 'place', 'allÃ©e', 'chemin', 'route', 
                                     'plomberie', 'solution', 'eaux', 'cernoise', 'services', 'entreprise']
                    if ville and ville.lower() not in mots_interdits and not any(mot in ville.lower() for mot in ['plombier', 'plomberie', 'solution']):
                        data['ville'] = ville
        
        # âœ… Si toujours pas de ville, utiliser ville_recherche (PRIORITÃ‰)
        if not data.get('ville'):
            ville_recherche = artisan_data.get('ville_recherche') or artisan_data.get('ville')
            if ville_recherche:
                data['ville'] = ville_recherche
                data['ville_recherche'] = ville_recherche
        
        # âœ… FALLBACK : Si on a la ville mais pas le code postal, chercher via l'API data.gouv.fr
        if not data.get('code_postal') and data.get('ville'):
            try:
                ville_nom = data['ville']
                # Chercher le code postal via l'API
                url = f"https://geo.api.gouv.fr/communes?nom={ville_nom}&fields=nom,code,codesPostaux"
                response = requests.get(url, timeout=2)
                if response.status_code == 200:
                    communes = response.json()
                    if communes:
                        # Prendre la premiÃ¨re commune trouvÃ©e
                        codes_postaux = communes[0].get('codesPostaux', [])
                        if codes_postaux:
                            data['code_postal'] = codes_postaux[0]
                            # Extraire le dÃ©partement
                            if len(data['code_postal']) >= 2:
                                if data['code_postal'].startswith('97') or data['code_postal'].startswith('98'):
                                    data['departement'] = data['code_postal'][:3]
                                else:
                                    data['departement'] = data['code_postal'][:2]
            except Exception as e:
                # Silencieux - ne pas bloquer si l'API Ã©choue
                pass
        
        # âœ… VALIDATION : Note et nombre d'avis doivent Ãªtre cohÃ©rents
        if data.get('note') is not None and data.get('nombre_avis') is None:
            data['nombre_avis'] = 0
        
        # âœ… VÃ©rifier qu'on a au moins une donnÃ©e valide avant d'insÃ©rer
        has_valid_data = any([
            data.get('nom_entreprise'),
            data.get('telephone'),
            data.get('site_web'),
            data.get('adresse')
        ])
        
        if not has_valid_data:
            print(f"âš ï¸ save_callback: Aucune donnÃ©e valide pour sauvegarder. DonnÃ©es reÃ§ues: {artisan_data}")
            return None
        
        # Sauvegarder dans la BDD
        artisan_id = ajouter_artisan(data)
        if artisan_id:
            print(f"âœ… Artisan sauvegardÃ© (ID: {artisan_id})")
        else:
            print(f"âš ï¸ save_callback: ajouter_artisan a retournÃ© None pour: {data.get('nom_entreprise', 'N/A')}")
        return artisan_id
    except ValueError as e:
        # Erreur de validation (pas de donnÃ©es valides)
        print(f"âš ï¸ Erreur validation artisan: {e} - DonnÃ©es: {artisan_data}")
        return None
    except Exception as e:
        print(f"âŒ Erreur sauvegarde artisan: {e}")
        import traceback
        print(traceback.format_exc())
        return None

def scrape_ville(task_info, max_results, status_file):
    """Scrape une ville et met Ã  jour le statut"""
    metier_actuel = task_info['metier']
    ville_actuelle = task_info['ville']
    departement_actuel = task_info['departement']
    
    # âœ… VÃ©rifier si dÃ©jÃ  scrapÃ© (optionnel - peut Ãªtre dÃ©sactivÃ© pour re-scraping)
    # if is_already_scraped(metier_actuel, departement_actuel, ville_actuelle):
    #     print(f"â­ï¸ {metier_actuel} - {departement_actuel} - {ville_actuelle} dÃ©jÃ  scrapÃ©, ignorÃ©")
    #     update_status_file(status_file, task_info, 0, 'skipped')
    #     return []
    
    try:
        scraper = GoogleMapsScraper(headless=True)
        scraper.is_running = True
        
        # âœ… Callback pour sauvegarder directement dans la BDD ET dans le fichier JSON
        # DÃ©finir le chemin du fichier une seule fois
        results_file = Path('data') / 'scraping_results_github_actions.json'
        results_file.parent.mkdir(parents=True, exist_ok=True)
        
        def progress_callback(index, total, info):
            if info:
                info['ville_recherche'] = ville_actuelle
                info['recherche'] = metier_actuel
                info['departement_recherche'] = departement_actuel  # âœ… Stocker le dÃ©partement recherchÃ© sÃ©parÃ©ment
                # Ne pas Ã©craser le dÃ©partement extrait depuis le code postal, mais utiliser le dÃ©partement recherchÃ© en prioritÃ©
                if not info.get('departement'):
                    info['departement'] = departement_actuel
                # Sauvegarder dans la BDD
                save_callback(info)
                # âœ… Sauvegarder aussi dans le fichier JSON progressivement (Ã  chaque Ã©tablissement)
                try:
                    save_progress(results_file, [info])
                except Exception as e:
                    print(f"âš ï¸ Erreur sauvegarde JSON progressive: {e}")
        
        resultats = scraper.scraper(
            recherche=metier_actuel,
            ville=ville_actuelle,
            max_results=max_results,
            progress_callback=progress_callback
        )
        scraper.quit()
        
        # âœ… Marquer comme scrapÃ© dans l'historique
        mark_scraping_done(metier_actuel, departement_actuel, ville_actuelle, len(resultats) if resultats else 0)
        
        # âœ… Mettre Ã  jour le statut aprÃ¨s chaque ville
        update_status_file(status_file, task_info, len(resultats) if resultats else 0, 'completed')
        
        return resultats or []
    except Exception as e:
        print(f"âŒ Erreur {ville_actuelle}: {e}")
        update_status_file(status_file, task_info, 0, 'failed', str(e))
        return []

def update_status_file(status_file, task_info, results_count, status, error=None):
    """Met Ã  jour le fichier de statut"""
    try:
        if status_file.exists():
            with open(status_file, 'r', encoding='utf-8') as f:
                status_data = json.load(f)
        else:
            status_data = {
                'started_at': datetime.now().isoformat(),
                'total_tasks': 0,
                'completed_tasks': 0,
                'failed_tasks': 0,
                'total_results': 0,
                'tasks': {}
            }
        
        task_key = f"{task_info['metier']}_{task_info['departement']}_{task_info['ville']}"
        status_data['tasks'][task_key] = {
            'status': status,
            'results_count': results_count,
            'completed_at': datetime.now().isoformat(),
            'error': error
        }
        
        # Mettre Ã  jour les compteurs
        status_data['completed_tasks'] = sum(1 for t in status_data['tasks'].values() if t['status'] == 'completed')
        status_data['failed_tasks'] = sum(1 for t in status_data['tasks'].values() if t['status'] == 'failed')
        status_data['total_results'] = sum(t['results_count'] for t in status_data['tasks'].values())
        status_data['last_updated'] = datetime.now().isoformat()
        
        with open(status_file, 'w', encoding='utf-8') as f:
            json.dump(status_data, f, ensure_ascii=False, indent=2)
        
        # âœ… Logger la progression pour visibilitÃ© dans les logs GitHub
        if status_data['total_tasks'] > 0:
            progress_pct = (status_data['completed_tasks'] / status_data['total_tasks'] * 100)
            print(f"ğŸ“Š Progression: {status_data['completed_tasks']}/{status_data['total_tasks']} villes ({progress_pct:.1f}%) | {status_data['total_results']} rÃ©sultats trouvÃ©s")
    except Exception as e:
        print(f"âš ï¸ Erreur mise Ã  jour statut: {e}")

def save_progress(results_file, new_results):
    """Sauvegarde les rÃ©sultats progressivement"""
    try:
        if results_file.exists():
            with open(results_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            data = {
                'timestamp': datetime.now().isoformat(),
                'total_results': 0,
                'results': []
            }
        
        # Ajouter les nouveaux rÃ©sultats (Ã©viter les doublons)
        existing_ids = {f"{r.get('nom', '')}_{r.get('telephone', '')}_{r.get('ville_recherche', '')}" for r in data['results']}
        for r in new_results:
            r_id = f"{r.get('nom', '')}_{r.get('telephone', '')}_{r.get('ville_recherche', '')}"
            if r_id not in existing_ids:
                data['results'].append(r)
        
        data['total_results'] = len(data['results'])
        data['last_updated'] = datetime.now().isoformat()
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ Erreur sauvegarde progressive: {e}")

if __name__ == "__main__":
    # âœ… Initialiser la base de donnÃ©es
    init_database()

    # RÃ©cupÃ©rer les paramÃ¨tres depuis les variables d'environnement
    metiers = json.loads(os.environ.get('METIERS', '[]'))
    departements = json.loads(os.environ.get('DEPARTEMENTS', '[]'))
    max_results = int(os.environ.get('MAX_RESULTS', '50'))
    num_threads = int(os.environ.get('NUM_THREADS', '3'))
    use_api_communes = os.environ.get('USE_API_COMMUNES', 'false').lower() == 'true'
    min_pop = int(os.environ.get('MIN_POP', '0'))
    max_pop = int(os.environ.get('MAX_POP', '50000'))

    # ParamÃ¨tres de commit pÃ©riodique
    enable_periodic_commits = os.environ.get('ENABLE_PERIODIC_COMMITS', 'false').lower() == 'true'
    commit_interval = int(os.environ.get('COMMIT_INTERVAL_MINUTES', '10'))

    print(f'ğŸš€ DÃ©marrage scraping GitHub Actions')
    print(f'ğŸ“‹ MÃ©tiers: {metiers}')
    print(f'ğŸ“ DÃ©partements: {departements}')
    print(f'ğŸ”¢ Max rÃ©sultats: {max_results}')
    print(f'ğŸ§µ Threads: {num_threads}')
    print(f'ğŸ’¾ Sauvegarde directe dans la BDD activÃ©e')
    if enable_periodic_commits:
        print(f'ğŸ”„ Commits pÃ©riodiques activÃ©s (intervalle: {commit_interval} min)')

    # DÃ©marrer le thread de commit pÃ©riodique si activÃ©
    commit_thread = None
    if enable_periodic_commits:
        commit_thread = threading.Thread(
            target=periodic_commit_thread,
            args=(commit_interval,),
            daemon=True
        )
        commit_thread.start()
    
    # Charger les villes par dÃ©partement
    try:
        with open('data/villes_par_departement.json', 'r', encoding='utf-8') as f:
            villes_par_dept = json.load(f)
    except:
        villes_par_dept = {}
    
    # PrÃ©parer la liste des villes
    toutes_villes = []
    for dept in departements:
        if use_api_communes:
            communes = get_communes_from_api(dept, min_pop, max_pop)
            villes_dept = [c['nom'] for c in communes]
            print(f'ğŸ“¡ API: {len(villes_dept)} communes trouvÃ©es pour {dept}')
        else:
            villes_dept = villes_par_dept.get(dept, [])
            if not villes_dept:
                villes_dept = [f"{metiers[0]} {dept}"]
        
        for metier in metiers:
            for ville in villes_dept:
                toutes_villes.append({
                    'metier': metier,
                    'departement': dept,
                    'ville': ville
                })
    
    print(f'ğŸ“Š Total villes Ã  scraper: {len(toutes_villes)}')
    
    # âœ… Fichiers de statut et rÃ©sultats
    os.makedirs('data', exist_ok=True)
    status_file = Path('data/github_actions_status.json')
    results_file = Path('data/scraping_results_github_actions.json')
    
    # Initialiser le fichier de statut
    initial_status = {
        'started_at': datetime.now().isoformat(),
        'total_tasks': len(toutes_villes),
        'completed_tasks': 0,
        'failed_tasks': 0,
        'total_results': 0,
        'tasks': {}
    }
    with open(status_file, 'w', encoding='utf-8') as f:
        json.dump(initial_status, f, ensure_ascii=False, indent=2)
    
    # Initialiser le fichier de rÃ©sultats
    initial_results = {
        'timestamp': datetime.now().isoformat(),
        'total_results': 0,
        'results': []
    }
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(initial_results, f, ensure_ascii=False, indent=2)
    
    # Multi-threading
    tous_resultats = []
    if num_threads > 1:
        print(f'ğŸš€ Multi-threading activÃ© ({num_threads} threads)')
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = {executor.submit(scrape_ville, task, max_results, status_file): task for task in toutes_villes}
            for future in as_completed(futures):
                try:
                    resultats = future.result()
                    if resultats:
                        tous_resultats.extend(resultats)
                        # âœ… Sauvegarder progressivement
                        save_progress(results_file, resultats)
                        print(f'âœ… {len(resultats)} rÃ©sultats ajoutÃ©s (total: {len(tous_resultats)})')
                except Exception as e:
                    print(f'âŒ Erreur thread: {e}')
    else:
        # Mode sÃ©quentiel
        for i, task in enumerate(toutes_villes, 1):
            print(f'ğŸ” [{i}/{len(toutes_villes)}] {task["metier"]} - {task["departement"]} - {task["ville"]}')
            resultats = scrape_ville(task, max_results, status_file)
            if resultats:
                tous_resultats.extend(resultats)
                # âœ… Sauvegarder progressivement
                save_progress(results_file, resultats)
                print(f'âœ… {len(resultats)} rÃ©sultats (total: {len(tous_resultats)})')
    
    print(f'âœ… Scraping terminÃ©: {len(tous_resultats)} rÃ©sultats au total')

    # ArrÃªter le thread de commit pÃ©riodique
    if enable_periodic_commits:
        print("ğŸ”„ ArrÃªt du thread de commit pÃ©riodique...")
        stop_periodic_commit.set()
        if commit_thread:
            commit_thread.join(timeout=5)

    # âœ… Mettre Ã  jour le statut final
    final_status = {
        'started_at': initial_status['started_at'],
        'completed_at': datetime.now().isoformat(),
        'total_tasks': len(toutes_villes),
        'completed_tasks': len([t for t in initial_status['tasks'].values() if t.get('status') == 'completed']),
        'failed_tasks': len([t for t in initial_status['tasks'].values() if t.get('status') == 'failed']),
        'total_results': len(tous_resultats),
        'status': 'completed'
    }
    with open(status_file, 'w', encoding='utf-8') as f:
        json.dump(final_status, f, ensure_ascii=False, indent=2)

    print(f'ğŸ’¾ RÃ©sultats sauvegardÃ©s: {results_file}')
    print(f'ğŸ’¾ Statut sauvegardÃ©: {status_file}')

    # Commit final des rÃ©sultats
    if enable_periodic_commits:
        final_message = f"ğŸ¤– Scraping terminÃ©: {len(tous_resultats)} rÃ©sultats - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        git_commit_and_push(final_message)

